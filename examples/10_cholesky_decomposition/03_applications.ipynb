{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5187dd0d",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/10_cholesky_decomposition`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/10_cholesky_decomposition/03_applications.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a5db85",
   "metadata": {},
   "source": [
    "# Cholesky Decomposition: Applications\n",
    "\n",
    "In this notebook, we explore practical applications of Cholesky decomposition, including:\n",
    "\n",
    "1. Solving linear systems\n",
    "2. Least squares problems\n",
    "3. Matrix inversion\n",
    "4. Sampling from multivariate Gaussian distributions\n",
    "5. Optimization algorithms\n",
    "6. Kalman filtering\n",
    "\n",
    "These applications demonstrate the versatility and efficiency of Cholesky decomposition in various computational tasks.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c413b86a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set the default style for plots\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "# Helper function to create positive definite matrices\n",
    "def create_positive_definite_matrix(n, method=\"random\", condition_number=None):\n",
    "    \"\"\"Create a positive definite matrix of size n×n.\"\"\"\n",
    "    if method == \"random\":\n",
    "        # Create a random matrix\n",
    "        B = torch.randn(n, n)\n",
    "        # A = B*B^T is guaranteed to be positive definite (if B is full rank)\n",
    "        A = B @ B.T\n",
    "        # Add a small value to the diagonal to ensure positive definiteness\n",
    "        A = A + torch.eye(n) * 1e-5\n",
    "        \n",
    "        # If a specific condition number is requested\n",
    "        if condition_number is not None:\n",
    "            # Get eigendecomposition\n",
    "            eigenvalues, eigenvectors = torch.linalg.eigh(A)\n",
    "            # Adjust eigenvalues to get the desired condition number\n",
    "            min_eig = 1.0\n",
    "            max_eig = condition_number\n",
    "            eigenvalues = torch.linspace(min_eig, max_eig, n)\n",
    "            # Reconstruct the matrix\n",
    "            A = eigenvectors @ torch.diag(eigenvalues) @ eigenvectors.T\n",
    "            \n",
    "        return A\n",
    "    elif method == \"predetermined\":\n",
    "        # A predefined example\n",
    "        A = torch.tensor([[4.0, 1.0, 1.0], \n",
    "                          [1.0, 3.0, 2.0], \n",
    "                          [1.0, 2.0, 6.0]])\n",
    "        return A\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "\n",
    "# Helper function to visualize matrices\n",
    "def plot_matrix(matrix, title):\n",
    "    \"\"\"Plot a matrix as a heatmap with annotations.\"\"\"\n",
    "    plt.figure(figsize=(7, 6))\n",
    "    \n",
    "    # Create heatmap\n",
    "    ax = sns.heatmap(matrix.numpy(), annot=True, fmt=\".2f\", cmap=\"Blues\",\n",
    "                    linewidths=.5, cbar=True)\n",
    "    \n",
    "    # Add column and row indices\n",
    "    ax.set_xticks(np.arange(matrix.shape[1]) + 0.5)\n",
    "    ax.set_yticks(np.arange(matrix.shape[0]) + 0.5)\n",
    "    ax.set_xticklabels(range(matrix.shape[1]))\n",
    "    ax.set_yticklabels(range(matrix.shape[0]))\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel(\"Column Index\")\n",
    "    plt.ylabel(\"Row Index\")\n",
    "    plt.title(title)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return ax\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31745e6c",
   "metadata": {},
   "source": [
    "## 1. Solving Linear Systems\n",
    "\n",
    "One of the most common applications of Cholesky decomposition is solving linear systems of the form $Ax = b$, where $A$ is a symmetric positive definite matrix.\n",
    "\n",
    "The solution involves two triangular solves:\n",
    "1. Decompose $A = LL^T$ using Cholesky\n",
    "2. Solve $Ly = b$ for $y$ using forward substitution\n",
    "3. Solve $L^Tx = y$ for $x$ using backward substitution\n",
    "\n",
    "This approach is numerically stable and efficient, requiring only $O(n^2)$ operations for the triangular solves.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3465a698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_linear_system_cholesky(A, b):\n",
    "    \"\"\"\n",
    "    Solve a linear system Ax = b using Cholesky decomposition.\n",
    "    \n",
    "    Parameters:\n",
    "        A (torch.Tensor): Symmetric positive definite matrix\n",
    "        b (torch.Tensor): Right-hand side vector\n",
    "        \n",
    "    Returns:\n",
    "        x (torch.Tensor): Solution vector\n",
    "    \"\"\"\n",
    "    # Step 1: Compute Cholesky decomposition A = L*L^T\n",
    "    L = torch.linalg.cholesky(A)\n",
    "    \n",
    "    # Step 2: Solve Ly = b using forward substitution\n",
    "    y = torch.zeros_like(b)\n",
    "    for i in range(len(b)):\n",
    "        y[i] = (b[i] - torch.sum(L[i, :i] * y[:i])) / L[i, i]\n",
    "    \n",
    "    # Step 3: Solve L^T x = y using backward substitution\n",
    "    x = torch.zeros_like(y)\n",
    "    for i in range(len(y)-1, -1, -1):\n",
    "        x[i] = (y[i] - torch.sum(L[i+1:, i] * x[i+1:])) / L[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Create a system to solve\n",
    "n = 4\n",
    "A = create_positive_definite_matrix(n, method=\"random\")\n",
    "x_true = torch.randn(n)  # True solution\n",
    "b = A @ x_true  # Right-hand side\n",
    "\n",
    "# Solve using our Cholesky implementation\n",
    "x_cholesky = solve_linear_system_cholesky(A, b)\n",
    "\n",
    "# Solve using PyTorch's linear solver for comparison\n",
    "x_torch = torch.linalg.solve(A, b)\n",
    "\n",
    "print(\"True solution:\", x_true)\n",
    "print(\"\\nCholesky solution:\", x_cholesky)\n",
    "print(\"PyTorch direct solution:\", x_torch)\n",
    "print(\"\\nError in Cholesky solution:\", torch.norm(x_cholesky - x_true).item())\n",
    "print(\"Error in PyTorch solution:\", torch.norm(x_torch - x_true).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5e98007",
   "metadata": {},
   "source": [
    "Let's visualize the solution process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68036525",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_linear_system(A, b, L, y, x):\n",
    "    \"\"\"Visualize the steps in solving a linear system using Cholesky.\"\"\"\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Original system\n",
    "    plt.subplot(1, 3, 1)\n",
    "    system_matrix = torch.zeros(n, n+1)\n",
    "    system_matrix[:, :n] = A\n",
    "    system_matrix[:, n] = b\n",
    "    sns.heatmap(system_matrix.numpy(), annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=.5,\n",
    "               xticklabels=list(range(n)) + ['b'])\n",
    "    plt.title(\"Original System: Ax = b\")\n",
    "    \n",
    "    # Step 1: Forward substitution\n",
    "    plt.subplot(1, 3, 2)\n",
    "    forward_matrix = torch.zeros(n, n+1)\n",
    "    forward_matrix[:, :n] = L\n",
    "    forward_matrix[:, n] = y\n",
    "    mask = torch.triu(torch.ones(n, n), diagonal=1)\n",
    "    mask = torch.cat([mask, torch.zeros(n, 1)], dim=1)\n",
    "    sns.heatmap(forward_matrix.numpy(), annot=True, fmt=\".2f\", cmap=\"Greens\", linewidths=.5,\n",
    "               mask=mask.bool().numpy(), xticklabels=list(range(n)) + ['y'])\n",
    "    plt.title(\"Step 1: Solve Ly = b\")\n",
    "    \n",
    "    # Step 2: Backward substitution\n",
    "    plt.subplot(1, 3, 3)\n",
    "    backward_matrix = torch.zeros(n, n+1)\n",
    "    backward_matrix[:, :n] = L.T\n",
    "    backward_matrix[:, n] = x\n",
    "    mask = torch.tril(torch.ones(n, n), diagonal=-1)\n",
    "    mask = torch.cat([mask, torch.zeros(n, 1)], dim=1)\n",
    "    sns.heatmap(backward_matrix.numpy(), annot=True, fmt=\".2f\", cmap=\"Oranges\", linewidths=.5,\n",
    "               mask=mask.bool().numpy(), xticklabels=list(range(n)) + ['x'])\n",
    "    plt.title(\"Step 2: Solve L^T x = y\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute intermediate results for visualization\n",
    "L = torch.linalg.cholesky(A)\n",
    "y = torch.triangular_solve(b.unsqueeze(1), L, upper=False)[0].squeeze()\n",
    "\n",
    "# Visualize the solution process\n",
    "visualize_linear_system(A, b, L, y, x_cholesky)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4578018",
   "metadata": {},
   "source": [
    "### Comparing Efficiency with Different Methods\n",
    "\n",
    "Let's compare the efficiency of solving linear systems using Cholesky decomposition versus other methods:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936e42bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def benchmark_linear_solvers(sizes):\n",
    "    \"\"\"Benchmark different linear solvers for systems of various sizes.\"\"\"\n",
    "    cholesky_times = []\n",
    "    lu_times = []\n",
    "    direct_times = []\n",
    "    \n",
    "    for n in sizes:\n",
    "        # Create a well-conditioned system\n",
    "        A = create_positive_definite_matrix(n, condition_number=10)\n",
    "        b = torch.randn(n)\n",
    "        \n",
    "        # Method 1: Cholesky decomposition\n",
    "        start_time = time.time()\n",
    "        L = torch.linalg.cholesky(A)\n",
    "        y = torch.triangular_solve(b.unsqueeze(1), L, upper=False)[0].squeeze()\n",
    "        x_cholesky = torch.triangular_solve(y.unsqueeze(1), L.T, upper=True)[0].squeeze()\n",
    "        cholesky_time = time.time() - start_time\n",
    "        cholesky_times.append(cholesky_time)\n",
    "        \n",
    "        # Method 2: LU decomposition\n",
    "        start_time = time.time()\n",
    "        LU, pivots = torch.linalg.lu_factor(A)\n",
    "        x_lu = torch.linalg.lu_solve(LU, pivots, b.unsqueeze(1)).squeeze()\n",
    "        lu_time = time.time() - start_time\n",
    "        lu_times.append(lu_time)\n",
    "        \n",
    "        # Method 3: Direct solve\n",
    "        start_time = time.time()\n",
    "        x_direct = torch.linalg.solve(A, b)\n",
    "        direct_time = time.time() - start_time\n",
    "        direct_times.append(direct_time)\n",
    "        \n",
    "        print(f\"Size {n}×{n}:\")\n",
    "        print(f\"  Cholesky: {cholesky_time:.6f} seconds\")\n",
    "        print(f\"  LU: {lu_time:.6f} seconds\")\n",
    "        print(f\"  Direct: {direct_time:.6f} seconds\")\n",
    "    \n",
    "    return cholesky_times, lu_times, direct_times\n",
    "\n",
    "# Benchmark with different matrix sizes\n",
    "sizes = [10, 50, 100, 500, 1000]\n",
    "cholesky_times, lu_times, direct_times = benchmark_linear_solvers(sizes)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(sizes, cholesky_times, 'o-', label='Cholesky')\n",
    "plt.plot(sizes, lu_times, 's-', label='LU')\n",
    "plt.plot(sizes, direct_times, 'x-', label='Direct')\n",
    "plt.xlabel('Matrix Size (n)')\n",
    "plt.ylabel('Computation Time (seconds)')\n",
    "plt.title('Linear System Solver Performance')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc50016",
   "metadata": {},
   "source": [
    "### Effect of Condition Number on Solver Stability\n",
    "\n",
    "The condition number of a matrix affects the numerical stability of linear system solvers. \n",
    "Let's examine how different methods perform with varying condition numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e73b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_solvers_stability(condition_numbers):\n",
    "    \"\"\"Compare solver stability for matrices with different condition numbers.\"\"\"\n",
    "    cholesky_errors = []\n",
    "    lu_errors = []\n",
    "    direct_errors = []\n",
    "    \n",
    "    n = 100  # Fixed size\n",
    "    \n",
    "    for cond in condition_numbers:\n",
    "        # Create a matrix with specific condition number\n",
    "        A = create_positive_definite_matrix(n, condition_number=cond)\n",
    "        x_true = torch.randn(n)\n",
    "        b = A @ x_true\n",
    "        \n",
    "        # Method 1: Cholesky decomposition\n",
    "        try:\n",
    "            L = torch.linalg.cholesky(A)\n",
    "            y = torch.triangular_solve(b.unsqueeze(1), L, upper=False)[0].squeeze()\n",
    "            x_cholesky = torch.triangular_solve(y.unsqueeze(1), L.T, upper=True)[0].squeeze()\n",
    "            error_cholesky = torch.norm(x_cholesky - x_true) / torch.norm(x_true)\n",
    "        except:\n",
    "            error_cholesky = float('nan')\n",
    "        cholesky_errors.append(error_cholesky)\n",
    "        \n",
    "        # Method 2: LU decomposition\n",
    "        try:\n",
    "            LU, pivots = torch.linalg.lu_factor(A)\n",
    "            x_lu = torch.linalg.lu_solve(LU, pivots, b.unsqueeze(1)).squeeze()\n",
    "            error_lu = torch.norm(x_lu - x_true) / torch.norm(x_true)\n",
    "        except:\n",
    "            error_lu = float('nan')\n",
    "        lu_errors.append(error_lu)\n",
    "        \n",
    "        # Method 3: Direct solve\n",
    "        try:\n",
    "            x_direct = torch.linalg.solve(A, b)\n",
    "            error_direct = torch.norm(x_direct - x_true) / torch.norm(x_true)\n",
    "        except:\n",
    "            error_direct = float('nan')\n",
    "        direct_errors.append(error_direct)\n",
    "        \n",
    "        print(f\"Condition Number: {cond}\")\n",
    "        print(f\"  Cholesky relative error: {error_cholesky:.6e}\")\n",
    "        print(f\"  LU relative error: {error_lu:.6e}\")\n",
    "        print(f\"  Direct relative error: {error_direct:.6e}\")\n",
    "    \n",
    "    return cholesky_errors, lu_errors, direct_errors\n",
    "\n",
    "# Compare with different condition numbers\n",
    "condition_numbers = [1, 10, 100, 1000, 10000, 100000]\n",
    "cholesky_errors, lu_errors, direct_errors = compare_solvers_stability(condition_numbers)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(condition_numbers, cholesky_errors, 'o-', label='Cholesky')\n",
    "plt.semilogy(condition_numbers, lu_errors, 's-', label='LU')\n",
    "plt.semilogy(condition_numbers, direct_errors, 'x-', label='Direct')\n",
    "plt.xlabel('Condition Number')\n",
    "plt.ylabel('Relative Error (log scale)')\n",
    "plt.title('Solver Stability vs. Condition Number')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd88d02",
   "metadata": {},
   "source": [
    "## 2. Least Squares Problems\n",
    "\n",
    "Cholesky decomposition is efficient for solving least squares problems, particularly when the normal equations approach is used.\n",
    "\n",
    "Given an overdetermined system $Ax = b$ where $A$ is an $m \\times n$ matrix with $m > n$, the least squares solution minimizes $\\|Ax - b\\|_2^2$. \n",
    "This is solved by the normal equations: $A^TAx = A^Tb$.\n",
    "\n",
    "Since $A^TA$ is symmetric positive definite (if $A$ has full column rank), we can use Cholesky decomposition to solve this efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc032f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_least_squares_cholesky(A, b):\n",
    "    \"\"\"\n",
    "    Solve a least squares problem using normal equations and Cholesky decomposition.\n",
    "    \n",
    "    Parameters:\n",
    "        A (torch.Tensor): Coefficient matrix (m x n with m > n)\n",
    "        b (torch.Tensor): Right-hand side vector (m)\n",
    "        \n",
    "    Returns:\n",
    "        x (torch.Tensor): Least squares solution (n)\n",
    "    \"\"\"\n",
    "    # Form normal equations: A^T A x = A^T b\n",
    "    ATA = A.T @ A\n",
    "    ATb = A.T @ b\n",
    "    \n",
    "    # Solve using Cholesky decomposition\n",
    "    L = torch.linalg.cholesky(ATA)\n",
    "    \n",
    "    # Forward substitution\n",
    "    y = torch.zeros_like(ATb)\n",
    "    for i in range(len(y)):\n",
    "        y[i] = (ATb[i] - torch.sum(L[i, :i] * y[:i])) / L[i, i]\n",
    "    \n",
    "    # Backward substitution\n",
    "    x = torch.zeros_like(y)\n",
    "    for i in range(len(x)-1, -1, -1):\n",
    "        x[i] = (y[i] - torch.sum(L[i+1:, i] * x[i+1:])) / L[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "# Create an overdetermined system\n",
    "m, n = 10, 4  # More equations than unknowns\n",
    "A = torch.randn(m, n)\n",
    "x_true = torch.randn(n)\n",
    "b = A @ x_true + 0.1 * torch.randn(m)  # Add some noise\n",
    "\n",
    "# Solve using Cholesky\n",
    "x_cholesky = solve_least_squares_cholesky(A, b)\n",
    "\n",
    "# Solve using PyTorch's least squares solver for comparison\n",
    "x_torch, _ = torch.linalg.lstsq(A, b.unsqueeze(1))\n",
    "x_torch = x_torch.squeeze()\n",
    "\n",
    "print(\"True solution:\", x_true)\n",
    "print(\"\\nCholesky least squares solution:\", x_cholesky)\n",
    "print(\"PyTorch least squares solution:\", x_torch)\n",
    "print(\"\\nError in Cholesky solution:\", torch.norm(x_cholesky - x_true).item())\n",
    "print(\"Error in PyTorch solution:\", torch.norm(x_torch - x_true).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b8b19be",
   "metadata": {},
   "source": [
    "Let's visualize the least squares solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc7720a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_least_squares(A, b, x_sol):\n",
    "    \"\"\"Visualize a least squares problem and its solution.\"\"\"\n",
    "    # Only work with 2D cases for visualization\n",
    "    if A.shape[1] != 2:\n",
    "        return\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot the data points\n",
    "    plt.scatter(A[:, 0].numpy(), b.numpy(), color='blue', label='Data Points')\n",
    "    \n",
    "    # Plot the fitted line/plane\n",
    "    x_range = torch.linspace(A[:, 0].min(), A[:, 0].max(), 100)\n",
    "    if A.shape[1] == 2:  # If we have two parameters\n",
    "        y_fit = x_sol[0] * x_range + x_sol[1] * A[:, 1].mean()\n",
    "        plt.plot(x_range.numpy(), y_fit.numpy(), 'r-', linewidth=2, label='Least Squares Fit')\n",
    "    \n",
    "    # Plot the residuals\n",
    "    residuals = b - A @ x_sol\n",
    "    for i in range(len(b)):\n",
    "        plt.plot([A[i, 0], A[i, 0]], [b[i], b[i] - residuals[i]], 'g--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Least Squares Fitting')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the residuals\n",
    "    plt.figure(figsize=(10, 4))\n",
    "    plt.stem(range(len(residuals)), residuals.numpy())\n",
    "    plt.xlabel('Data Point')\n",
    "    plt.ylabel('Residual')\n",
    "    plt.title('Residuals')\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "# Create a 2D problem for visualization\n",
    "m = 20\n",
    "A_2d = torch.zeros(m, 2)\n",
    "A_2d[:, 0] = torch.linspace(-5, 5, m)\n",
    "A_2d[:, 1] = 1.0  # Intercept term\n",
    "x_true_2d = torch.tensor([2.5, 1.5])  # Slope and intercept\n",
    "b_2d = A_2d @ x_true_2d + 0.5 * torch.randn(m)  # Add some noise\n",
    "\n",
    "# Solve using Cholesky\n",
    "x_cholesky_2d = solve_least_squares_cholesky(A_2d, b_2d)\n",
    "\n",
    "# Visualize\n",
    "visualize_least_squares(A_2d, b_2d, x_cholesky_2d)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998acd6b",
   "metadata": {},
   "source": [
    "### Comparing Normal Equations vs QR for Least Squares\n",
    "\n",
    "While Cholesky decomposition provides an efficient way to solve least squares problems via normal equations, the QR decomposition approach is often more numerically stable, especially for ill-conditioned problems. Let's compare both approaches:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909f220",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_least_squares_methods(condition_numbers):\n",
    "    \"\"\"Compare normal equations (Cholesky) vs QR for least squares with different condition numbers.\"\"\"\n",
    "    cholesky_errors = []\n",
    "    qr_errors = []\n",
    "    \n",
    "    m, n = 100, 10  # Fix the problem size\n",
    "    \n",
    "    for cond in condition_numbers:\n",
    "        # Create a matrix with specific condition structure\n",
    "        U, _, V = torch.svd(torch.randn(m, n))\n",
    "        singular_values = torch.logspace(0, -np.log10(cond), n)\n",
    "        A = U[:, :n] @ torch.diag(singular_values) @ V.T\n",
    "        \n",
    "        x_true = torch.randn(n)\n",
    "        b = A @ x_true + 0.01 * torch.randn(m)  # Add some noise\n",
    "        \n",
    "        # Method 1: Normal equations with Cholesky\n",
    "        try:\n",
    "            ATA = A.T @ A\n",
    "            ATb = A.T @ b\n",
    "            L = torch.linalg.cholesky(ATA)\n",
    "            y = torch.triangular_solve(ATb.unsqueeze(1), L, upper=False)[0].squeeze()\n",
    "            x_cholesky = torch.triangular_solve(y.unsqueeze(1), L.T, upper=True)[0].squeeze()\n",
    "            error_cholesky = torch.norm(x_cholesky - x_true) / torch.norm(x_true)\n",
    "        except:\n",
    "            error_cholesky = float('nan')\n",
    "        cholesky_errors.append(error_cholesky)\n",
    "        \n",
    "        # Method 2: QR decomposition\n",
    "        try:\n",
    "            Q, R = torch.linalg.qr(A)\n",
    "            x_qr = torch.triangular_solve((Q.T @ b).unsqueeze(1), R, upper=True)[0].squeeze()\n",
    "            error_qr = torch.norm(x_qr - x_true) / torch.norm(x_true)\n",
    "        except:\n",
    "            error_qr = float('nan')\n",
    "        qr_errors.append(error_qr)\n",
    "        \n",
    "        print(f\"Condition Number: {cond}\")\n",
    "        print(f\"  Cholesky relative error: {error_cholesky:.6e}\")\n",
    "        print(f\"  QR relative error: {error_qr:.6e}\")\n",
    "    \n",
    "    return cholesky_errors, qr_errors\n",
    "\n",
    "# Compare with different condition numbers\n",
    "condition_numbers = [1, 10, 100, 1000, 10000, 100000]\n",
    "cholesky_errors, qr_errors = compare_least_squares_methods(condition_numbers)\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.semilogy(condition_numbers, cholesky_errors, 'o-', label='Normal Equations (Cholesky)')\n",
    "plt.semilogy(condition_numbers, qr_errors, 's-', label='QR Decomposition')\n",
    "plt.xlabel('Condition Number')\n",
    "plt.ylabel('Relative Error (log scale)')\n",
    "plt.title('Least Squares Methods: Stability vs. Condition Number')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "404f1950",
   "metadata": {},
   "source": [
    "## 3. Matrix Inversion\n",
    "\n",
    "Cholesky decomposition can be used for efficient matrix inversion when the matrix is symmetric positive definite. This involves:\n",
    "1. Computing the Cholesky decomposition $A = LL^T$\n",
    "2. Inverting the triangular factor $L$ to get $L^{-1}$\n",
    "3. Computing $A^{-1} = (L^{-1})^T L^{-1}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10597a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invert_matrix_cholesky(A):\n",
    "    \"\"\"\n",
    "    Compute the inverse of a symmetric positive definite matrix using Cholesky decomposition.\n",
    "    \n",
    "    Parameters:\n",
    "        A (torch.Tensor): Symmetric positive definite matrix\n",
    "        \n",
    "    Returns:\n",
    "        A_inv (torch.Tensor): Inverse of A\n",
    "    \"\"\"\n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Step 1: Compute Cholesky decomposition A = L*L^T\n",
    "    L = torch.linalg.cholesky(A)\n",
    "    \n",
    "    # Step 2: Compute inverse of L (lower triangular)\n",
    "    L_inv = torch.zeros_like(L)\n",
    "    for i in range(n):\n",
    "        L_inv[i, i] = 1.0 / L[i, i]\n",
    "        for j in range(i-1, -1, -1):\n",
    "            L_inv[i, j] = -torch.sum(L_inv[i, j+1:i+1] * L[j+1:i+1, j]) / L[j, j]\n",
    "    \n",
    "    # Step 3: Compute A^{-1} = (L^{-1})^T * L^{-1}\n",
    "    A_inv = L_inv.T @ L_inv\n",
    "    \n",
    "    return A_inv\n",
    "\n",
    "# Create a symmetric positive definite matrix\n",
    "n = 4\n",
    "A = create_positive_definite_matrix(n)\n",
    "\n",
    "# Invert using Cholesky\n",
    "A_inv_cholesky = invert_matrix_cholesky(A)\n",
    "\n",
    "# Invert using PyTorch's inverse function for comparison\n",
    "A_inv_torch = torch.inverse(A)\n",
    "\n",
    "print(\"Matrix A:\")\n",
    "print(A)\n",
    "print(\"\\nInverse using Cholesky:\")\n",
    "print(A_inv_cholesky)\n",
    "print(\"\\nInverse using PyTorch:\")\n",
    "print(A_inv_torch)\n",
    "\n",
    "# Verify: A * A^{-1} should be close to the identity matrix\n",
    "I_cholesky = A @ A_inv_cholesky\n",
    "I_torch = A @ A_inv_torch\n",
    "\n",
    "print(\"\\nA * A^{-1} (Cholesky):\")\n",
    "print(I_cholesky)\n",
    "print(\"\\nA * A^{-1} (PyTorch):\")\n",
    "print(I_torch)\n",
    "\n",
    "print(\"\\nError in Cholesky inversion:\", torch.norm(I_cholesky - torch.eye(n)).item())\n",
    "print(\"Error in PyTorch inversion:\", torch.norm(I_torch - torch.eye(n)).item())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28e7706",
   "metadata": {},
   "source": [
    "Let's visualize the steps in matrix inversion:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fe57c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_matrix_inversion(A, L, L_inv, A_inv):\n",
    "    \"\"\"Visualize the steps in matrix inversion using Cholesky.\"\"\"\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Original matrix\n",
    "    plt.subplot(1, 4, 1)\n",
    "    sns.heatmap(A.numpy(), annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=.5)\n",
    "    plt.title(\"Original Matrix A\")\n",
    "    \n",
    "    # Cholesky factor\n",
    "    plt.subplot(1, 4, 2)\n",
    "    sns.heatmap(L.numpy(), annot=True, fmt=\".2f\", cmap=\"Greens\", linewidths=.5)\n",
    "    plt.title(\"Cholesky Factor L\")\n",
    "    \n",
    "    # Inverse of Cholesky factor\n",
    "    plt.subplot(1, 4, 3)\n",
    "    sns.heatmap(L_inv.numpy(), annot=True, fmt=\".2f\", cmap=\"Oranges\", linewidths=.5)\n",
    "    plt.title(\"Inverse of L\")\n",
    "    \n",
    "    # Inverse of original matrix\n",
    "    plt.subplot(1, 4, 4)\n",
    "    sns.heatmap(A_inv.numpy(), annot=True, fmt=\".2f\", cmap=\"Purples\", linewidths=.5)\n",
    "    plt.title(\"Inverse of A\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Compute intermediate results for visualization\n",
    "L = torch.linalg.cholesky(A)\n",
    "L_inv = torch.triangular_solve(torch.eye(n), L, upper=False)[0]\n",
    "\n",
    "# Visualize the inversion process\n",
    "visualize_matrix_inversion(A, L, L_inv, A_inv_cholesky)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fcf200",
   "metadata": {},
   "source": [
    "## 4. Sampling from Multivariate Gaussian Distributions\n",
    "\n",
    "Cholesky decomposition is often used for generating samples from a multivariate Gaussian distribution.\n",
    "\n",
    "To generate samples from $\\mathcal{N}(\\mu, \\Sigma)$, we:\n",
    "1. Compute the Cholesky decomposition $\\Sigma = LL^T$\n",
    "2. Generate a vector $z$ of independent standard normal samples\n",
    "3. Compute $x = \\mu + Lz$, which will have the desired distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb76c9cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_multivariate_gaussian(mu, cov, num_samples=1000):\n",
    "    \"\"\"\n",
    "    Generate samples from a multivariate Gaussian distribution.\n",
    "    \n",
    "    Parameters:\n",
    "        mu (torch.Tensor): Mean vector\n",
    "        cov (torch.Tensor): Covariance matrix\n",
    "        num_samples (int): Number of samples to generate\n",
    "        \n",
    "    Returns:\n",
    "        samples (torch.Tensor): Generated samples\n",
    "    \"\"\"\n",
    "    # Get dimensions\n",
    "    n = len(mu)\n",
    "    \n",
    "    # Compute Cholesky decomposition of covariance matrix\n",
    "    L = torch.linalg.cholesky(cov)\n",
    "    \n",
    "    # Generate independent standard normal samples\n",
    "    z = torch.randn(num_samples, n)\n",
    "    \n",
    "    # Transform to desired distribution: x = mu + L*z\n",
    "    samples = mu + z @ L.T\n",
    "    \n",
    "    return samples\n",
    "\n",
    "# Define a 2D Gaussian for visualization\n",
    "mu = torch.tensor([1.0, 2.0])\n",
    "cov = torch.tensor([[2.0, 0.5], \n",
    "                     [0.5, 1.0]])\n",
    "\n",
    "# Generate samples\n",
    "samples = sample_multivariate_gaussian(mu, cov, num_samples=1000)\n",
    "\n",
    "# Visualize the samples and distribution\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot the samples\n",
    "plt.scatter(samples[:, 0].numpy(), samples[:, 1].numpy(), alpha=0.5)\n",
    "plt.plot(mu[0].item(), mu[1].item(), 'ro', markersize=10, label='Mean')\n",
    "\n",
    "# Plot 95% confidence ellipse\n",
    "from matplotlib.patches import Ellipse\n",
    "import matplotlib.transforms as transforms\n",
    "\n",
    "def confidence_ellipse(x, y, ax, n_std=2.0, **kwargs):\n",
    "    \"\"\"\n",
    "    Create a plot of the covariance confidence ellipse of *x* and *y*.\n",
    "    \"\"\"\n",
    "    # Compute the covariance matrix\n",
    "    cov = torch.cov(torch.stack([x, y]))\n",
    "    pearson = cov[0, 1] / torch.sqrt(cov[0, 0] * cov[1, 1])\n",
    "    \n",
    "    # Using the Cholesky decomposition, we can get the \"standard deviation\"\n",
    "    ell_radius_x = torch.sqrt(1 + pearson) * torch.sqrt(cov[0, 0]) * n_std\n",
    "    ell_radius_y = torch.sqrt(1 - pearson) * torch.sqrt(cov[1, 1]) * n_std\n",
    "    \n",
    "    ellipse = Ellipse((x.mean(), y.mean()), width=ell_radius_x * 2, height=ell_radius_y * 2,\n",
    "                      **kwargs)\n",
    "    \n",
    "    # Compute the axis angles\n",
    "    if cov[0, 0] < cov[1, 1]:\n",
    "        # Horizontal axis is the shorter one\n",
    "        ell_theta = 0.5 * torch.arctan(2 * cov[0, 1] / (cov[1, 1] - cov[0, 0])).item()\n",
    "    else:\n",
    "        # Vertical axis is the shorter one\n",
    "        ell_theta = 0.5 * torch.arctan(2 * cov[0, 1] / (cov[1, 1] - cov[0, 0])).item() + np.pi/2\n",
    "    \n",
    "    # Rotate the ellipse\n",
    "    ellipse.angle = ell_theta * 180 / np.pi\n",
    "    \n",
    "    return ax.add_patch(ellipse)\n",
    "\n",
    "# Plot the confidence ellipse\n",
    "confidence_ellipse(samples[:, 0], samples[:, 1], plt.gca(), n_std=2.0,\n",
    "                  edgecolor='red', facecolor='none', label='95% Confidence Region')\n",
    "\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('y')\n",
    "plt.title('Samples from Multivariate Gaussian Distribution')\n",
    "plt.axis('equal')\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3594add",
   "metadata": {},
   "source": [
    "### Visualizing the Cholesky Approach\n",
    "\n",
    "Let's visualize how the Cholesky decomposition transforms standard normal samples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcdbfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gaussian_sampling(mu, cov, num_samples=500):\n",
    "    \"\"\"Visualize the process of sampling from a multivariate Gaussian.\"\"\"\n",
    "    n = len(mu)\n",
    "    L = torch.linalg.cholesky(cov)\n",
    "    \n",
    "    # Step 1: Generate standard normal samples\n",
    "    z = torch.randn(num_samples, n)\n",
    "    \n",
    "    # Step 2: Transform using Cholesky factor\n",
    "    samples = mu + z @ L.T\n",
    "    \n",
    "    fig = plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot standard normal samples\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.scatter(z[:, 0].numpy(), z[:, 1].numpy(), alpha=0.5)\n",
    "    plt.title('Step 1: Standard Normal Samples z')\n",
    "    plt.xlabel('z₁')\n",
    "    plt.ylabel('z₂')\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot the effect of the Cholesky factor\n",
    "    plt.subplot(1, 3, 2)\n",
    "    transformed = z @ L.T\n",
    "    plt.scatter(transformed[:, 0].numpy(), transformed[:, 1].numpy(), alpha=0.5)\n",
    "    plt.title('Step 2: After Applying Cholesky Factor L')\n",
    "    plt.xlabel('(Lz)₁')\n",
    "    plt.ylabel('(Lz)₂')\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Plot the final samples\n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(samples[:, 0].numpy(), samples[:, 1].numpy(), alpha=0.5)\n",
    "    plt.plot(mu[0].item(), mu[1].item(), 'ro', markersize=10)\n",
    "    plt.title('Step 3: Final Samples x = μ + Lz')\n",
    "    plt.xlabel('x₁')\n",
    "    plt.ylabel('x₂')\n",
    "    plt.axis('equal')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the Cholesky factor\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(L.numpy(), annot=True, fmt=\".2f\", cmap=\"Blues\", linewidths=.5)\n",
    "    plt.title('Cholesky Factor L of Covariance Matrix')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the sampling process\n",
    "visualize_gaussian_sampling(mu, cov)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62ea1428",
   "metadata": {},
   "source": [
    "## 5. Optimization Algorithms\n",
    "\n",
    "Cholesky decomposition is used in many optimization algorithms, particularly in Newton and quasi-Newton methods. Here, we'll implement a simple Newton's method for unconstrained optimization that uses Cholesky decomposition to compute the search direction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5170f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def newton_method_cholesky(f, grad_f, hessian_f, x0, tol=1e-6, max_iter=50):\n",
    "    \"\"\"\n",
    "    Newton's method for unconstrained optimization using Cholesky decomposition.\n",
    "    \n",
    "    Parameters:\n",
    "        f (function): Objective function to minimize\n",
    "        grad_f (function): Gradient of the objective function\n",
    "        hessian_f (function): Hessian of the objective function\n",
    "        x0 (torch.Tensor): Initial point\n",
    "        tol (float): Tolerance for convergence\n",
    "        max_iter (int): Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        x (torch.Tensor): Optimal point\n",
    "        f_values (list): Function values at each iteration\n",
    "    \"\"\"\n",
    "    x = x0.clone()\n",
    "    f_values = [f(x)]\n",
    "    grad_norm_values = [torch.norm(grad_f(x)).item()]\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        # Compute gradient and Hessian\n",
    "        g = grad_f(x)\n",
    "        H = hessian_f(x)\n",
    "        \n",
    "        # Check for convergence\n",
    "        grad_norm = torch.norm(g)\n",
    "        if grad_norm < tol:\n",
    "            print(f\"Converged after {i} iterations\")\n",
    "            break\n",
    "        \n",
    "        # Compute Newton direction: Solve H * p = -g\n",
    "        try:\n",
    "            L = torch.linalg.cholesky(H)\n",
    "            y = torch.triangular_solve(-g.unsqueeze(1), L, upper=False)[0].squeeze()\n",
    "            p = torch.triangular_solve(y.unsqueeze(1), L.T, upper=True)[0].squeeze()\n",
    "        except:\n",
    "            # If Hessian is not positive definite, add regularization\n",
    "            H_reg = H + 1e-3 * torch.eye(len(x))\n",
    "            L = torch.linalg.cholesky(H_reg)\n",
    "            y = torch.triangular_solve(-g.unsqueeze(1), L, upper=False)[0].squeeze()\n",
    "            p = torch.triangular_solve(y.unsqueeze(1), L.T, upper=True)[0].squeeze()\n",
    "        \n",
    "        # Line search (simple backtracking)\n",
    "        alpha = 1.0\n",
    "        while f(x + alpha * p) > f(x) + 1e-4 * alpha * g.dot(p) and alpha > 1e-10:\n",
    "            alpha *= 0.5\n",
    "        \n",
    "        # Update\n",
    "        x = x + alpha * p\n",
    "        \n",
    "        # Record function value\n",
    "        f_values.append(f(x))\n",
    "        grad_norm_values.append(grad_norm.item())\n",
    "        \n",
    "        print(f\"Iteration {i+1}: f(x) = {f_values[-1]:.6f}, |grad f(x)| = {grad_norm_values[-1]:.6f}, alpha = {alpha:.6f}\")\n",
    "    \n",
    "    return x, f_values, grad_norm_values\n",
    "\n",
    "# Define a simple quadratic function for testing\n",
    "def f(x):\n",
    "    A = torch.tensor([[2.0, 0.5], [0.5, 1.0]])\n",
    "    b = torch.tensor([-1.0, -2.0])\n",
    "    return 0.5 * x.dot(A @ x) + b.dot(x)\n",
    "\n",
    "def grad_f(x):\n",
    "    A = torch.tensor([[2.0, 0.5], [0.5, 1.0]])\n",
    "    b = torch.tensor([-1.0, -2.0])\n",
    "    return A @ x + b\n",
    "\n",
    "def hessian_f(x):\n",
    "    return torch.tensor([[2.0, 0.5], [0.5, 1.0]])\n",
    "\n",
    "# Starting point\n",
    "x0 = torch.tensor([3.0, 2.0])\n",
    "\n",
    "# Run Newton's method\n",
    "x_opt, f_values, grad_norm_values = newton_method_cholesky(f, grad_f, hessian_f, x0)\n",
    "\n",
    "print(\"\\nOptimal point:\", x_opt)\n",
    "print(\"Optimal value:\", f(x_opt))\n",
    "\n",
    "# Visualize the optimization process\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.semilogy(range(len(f_values)), f_values, 'o-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Function Value (log scale)')\n",
    "plt.title('Convergence of Function Values')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.semilogy(range(len(grad_norm_values)), grad_norm_values, 'o-')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Gradient Norm (log scale)')\n",
    "plt.title('Convergence of Gradient Norm')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12799343",
   "metadata": {},
   "source": [
    "## 6. Kalman Filtering\n",
    "\n",
    "Cholesky decomposition is also used in Kalman filtering, particularly for the covariance update step. Here, we'll implement a simple Kalman filter for a 1D tracking problem.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bba417b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kalman_filter_cholesky(z, x0, P0, F, H, Q, R):\n",
    "    \"\"\"\n",
    "    Kalman filter implementation using Cholesky decomposition for the covariance updates.\n",
    "    \n",
    "    Parameters:\n",
    "        z (torch.Tensor): Measurements, shape (time_steps, measurement_dim)\n",
    "        x0 (torch.Tensor): Initial state estimate\n",
    "        P0 (torch.Tensor): Initial state covariance\n",
    "        F (torch.Tensor): State transition matrix\n",
    "        H (torch.Tensor): Measurement matrix\n",
    "        Q (torch.Tensor): Process noise covariance\n",
    "        R (torch.Tensor): Measurement noise covariance\n",
    "        \n",
    "    Returns:\n",
    "        x_filtered (torch.Tensor): Filtered state estimates\n",
    "        P_filtered (torch.Tensor): Filtered state covariances\n",
    "    \"\"\"\n",
    "    time_steps = z.shape[0]\n",
    "    state_dim = x0.shape[0]\n",
    "    \n",
    "    # Initialize arrays to store results\n",
    "    x_filtered = torch.zeros(time_steps+1, state_dim)\n",
    "    P_filtered = torch.zeros(time_steps+1, state_dim, state_dim)\n",
    "    \n",
    "    # Initial state\n",
    "    x_filtered[0] = x0\n",
    "    P_filtered[0] = P0\n",
    "    \n",
    "    for t in range(1, time_steps+1):\n",
    "        # Prediction step\n",
    "        x_pred = F @ x_filtered[t-1]\n",
    "        P_pred = F @ P_filtered[t-1] @ F.T + Q\n",
    "        \n",
    "        # Update step with Cholesky decomposition\n",
    "        S = H @ P_pred @ H.T + R  # Innovation covariance\n",
    "        \n",
    "        # Compute Kalman gain using Cholesky decomposition\n",
    "        L_S = torch.linalg.cholesky(S)\n",
    "        temp = torch.triangular_solve((H @ P_pred).T.unsqueeze(2), L_S, upper=False)[0].squeeze()\n",
    "        K = torch.triangular_solve(temp.unsqueeze(2), L_S.T, upper=True)[0].squeeze().T\n",
    "        \n",
    "        # Update state and covariance\n",
    "        innovation = z[t-1] - H @ x_pred\n",
    "        x_filtered[t] = x_pred + K @ innovation\n",
    "        \n",
    "        # Joseph form for covariance update (more stable)\n",
    "        I = torch.eye(state_dim)\n",
    "        P_filtered[t] = (I - K @ H) @ P_pred @ (I - K @ H).T + K @ R @ K.T\n",
    "        \n",
    "    return x_filtered, P_filtered\n",
    "\n",
    "# Example: 1D tracking with position and velocity\n",
    "def generate_tracking_data(time_steps=100, dt=0.1, process_noise_std=0.1, measurement_noise_std=0.5):\n",
    "    \"\"\"Generate synthetic tracking data.\"\"\"\n",
    "    # State transition matrix (constant velocity model)\n",
    "    F = torch.tensor([[1.0, dt], [0.0, 1.0]])\n",
    "    \n",
    "    # Measurement matrix (we only observe position)\n",
    "    H = torch.tensor([[1.0, 0.0]])\n",
    "    \n",
    "    # Process noise covariance\n",
    "    G = torch.tensor([[0.5*dt**2], [dt]])\n",
    "    Q = G @ G.T * process_noise_std**2\n",
    "    \n",
    "    # Measurement noise covariance\n",
    "    R = torch.tensor([[measurement_noise_std**2]])\n",
    "    \n",
    "    # Generate true trajectory\n",
    "    x_true = torch.zeros(time_steps+1, 2)\n",
    "    x_true[0] = torch.tensor([0.0, 1.0])  # Initial position and velocity\n",
    "    \n",
    "    for t in range(1, time_steps+1):\n",
    "        # Add process noise\n",
    "        w = torch.randn(1) * process_noise_std\n",
    "        x_true[t] = F @ x_true[t-1] + G.squeeze() * w\n",
    "    \n",
    "    # Generate noisy measurements\n",
    "    z = torch.zeros(time_steps, 1)\n",
    "    for t in range(time_steps):\n",
    "        z[t] = H @ x_true[t+1] + torch.randn(1) * measurement_noise_std\n",
    "    \n",
    "    return z, x_true, F, H, Q, R\n",
    "\n",
    "# Generate data\n",
    "time_steps = 100\n",
    "z, x_true, F, H, Q, R = generate_tracking_data(time_steps)\n",
    "\n",
    "# Initial state and covariance\n",
    "x0 = torch.tensor([0.0, 1.0])\n",
    "P0 = torch.eye(2)\n",
    "\n",
    "# Run Kalman filter\n",
    "x_filtered, P_filtered = kalman_filter_cholesky(z, x0, P0, F, H, Q, R)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Position plot\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(time_steps+1), x_true[:, 0].numpy(), 'g-', label='True Position')\n",
    "plt.plot(range(1, time_steps+1), z.squeeze().numpy(), 'ro', alpha=0.5, label='Measurements')\n",
    "plt.plot(range(time_steps+1), x_filtered[:, 0].numpy(), 'b-', label='Filtered Position')\n",
    "\n",
    "# Add uncertainty bands (2 standard deviations)\n",
    "position_std = torch.sqrt(P_filtered[:, 0, 0])\n",
    "plt.fill_between(range(time_steps+1), \n",
    "                 (x_filtered[:, 0] - 2 * position_std).numpy(),\n",
    "                 (x_filtered[:, 0] + 2 * position_std).numpy(),\n",
    "                 color='b', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Position')\n",
    "plt.title('Kalman Filter: Position Tracking')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Velocity plot\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(time_steps+1), x_true[:, 1].numpy(), 'g-', label='True Velocity')\n",
    "plt.plot(range(time_steps+1), x_filtered[:, 1].numpy(), 'b-', label='Filtered Velocity')\n",
    "\n",
    "# Add uncertainty bands (2 standard deviations)\n",
    "velocity_std = torch.sqrt(P_filtered[:, 1, 1])\n",
    "plt.fill_between(range(time_steps+1), \n",
    "                 (x_filtered[:, 1] - 2 * velocity_std).numpy(),\n",
    "                 (x_filtered[:, 1] + 2 * velocity_std).numpy(),\n",
    "                 color='b', alpha=0.2)\n",
    "\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Velocity')\n",
    "plt.title('Kalman Filter: Velocity Estimation')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd7ddd91",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we explored several practical applications of Cholesky decomposition:\n",
    "\n",
    "1. **Solving Linear Systems**: Cholesky decomposition provides an efficient and numerically stable method for solving linear systems when the coefficient matrix is symmetric positive definite.\n",
    "\n",
    "2. **Least Squares Problems**: For overdetermined systems, Cholesky can efficiently solve the normal equations, though QR decomposition may be more stable for ill-conditioned problems.\n",
    "\n",
    "3. **Matrix Inversion**: Cholesky offers a structured approach to inverting symmetric positive definite matrices.\n",
    "\n",
    "4. **Sampling from Multivariate Gaussians**: The decomposition enables efficient generation of correlated random samples.\n",
    "\n",
    "5. **Optimization Algorithms**: In Newton and quasi-Newton methods, Cholesky is used to compute search directions.\n",
    "\n",
    "6. **Kalman Filtering**: Cholesky decomposition helps with numerically stable covariance updates in filtering applications.\n",
    "\n",
    "These applications highlight the versatility and importance of Cholesky decomposition in computational mathematics, statistics, optimization, and signal processing.\n",
    "\n",
    "The key advantages of Cholesky decomposition in these applications include:\n",
    "- Computational efficiency (roughly half the cost of LU decomposition)\n",
    "- Numerical stability for well-conditioned problems\n",
    "- Natural exploitation of symmetry and positive definiteness\n",
    "- Simple and elegant implementation"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
