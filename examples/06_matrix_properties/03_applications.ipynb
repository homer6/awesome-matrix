{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3eb60804",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/06_matrix_properties`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/06_matrix_properties/03_applications.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd0fd3a",
   "metadata": {},
   "source": [
    "# Matrix Properties: Applications\n",
    "\n",
    "In this notebook, we explore real-world applications of matrix properties. Understanding how matrix properties apply to practical problems helps develop intuition for why they matter in fields like machine learning, computer graphics, data analysis, and scientific computing.\n",
    "\n",
    "We'll focus on the following applications:\n",
    "\n",
    "1. **Image Processing and Compression**\n",
    "2. **Principal Component Analysis (PCA)**\n",
    "3. **Linear Systems and Condition Number**\n",
    "4. **Graph Analysis with Adjacency Matrices**\n",
    "5. **Markov Chains and Transition Matrices**\n",
    "\n",
    "Each application demonstrates how matrix properties inform algorithm design and practical solutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b157f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from PIL import Image\n",
    "import requests\n",
    "from io import BytesIO\n",
    "from scipy import linalg\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_digits, fetch_olivetti_faces, make_blobs\n",
    "import networkx as nx\n",
    "from matplotlib.patches import FancyArrowPatch\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# For better looking plots\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom colormap (light blue to dark blue)\n",
    "colors = [(0.95, 0.95, 1), (0.0, 0.2, 0.6)]  # light blue to dark blue\n",
    "blue_cmap = LinearSegmentedColormap.from_list('CustomBlue', colors, N=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0414035a",
   "metadata": {},
   "source": [
    "## 1. Image Processing and Compression\n",
    "\n",
    "Images can be represented as matrices where each element corresponds to a pixel value. Many image processing tasks involve matrix operations, such as:\n",
    "\n",
    "- **Filtering and convolution**: Using small matrices (kernels) to transform images\n",
    "- **Compression**: Reducing the storage requirements of images using matrix factorization\n",
    "- **Feature extraction**: Identifying important patterns in images\n",
    "\n",
    "Let's explore how matrix rank and Singular Value Decomposition (SVD) can be used for image compression.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9971fe3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_grayscale_image(url=None):\n",
    "    \"\"\"Load a grayscale image from a URL or use a default image.\"\"\"\n",
    "    if url is None:\n",
    "        # Default image URL (Claude Shannon)\n",
    "        url = \"https://upload.wikimedia.org/wikipedia/commons/9/9b/Claude_Shannon_MF.jpg\"\n",
    "    \n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        img = Image.open(BytesIO(response.content)).convert('L')  # Convert to grayscale\n",
    "        # Resize for faster processing\n",
    "        img = img.resize((256, 256))\n",
    "        return np.array(img)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        # Create a simple synthetic image\n",
    "        x = np.linspace(0, 1, 256)\n",
    "        y = np.linspace(0, 1, 256)\n",
    "        X, Y = np.meshgrid(x, y)\n",
    "        synthetic_image = (255 * (np.sin(10*X) * np.sin(10*Y))).astype(np.uint8)\n",
    "        return synthetic_image\n",
    "\n",
    "# Load a grayscale image\n",
    "image = load_grayscale_image()\n",
    "\n",
    "# Display the image\n",
    "plt.figure(figsize=(8, 8))\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(\"Original Image\")\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc4bfdf",
   "metadata": {},
   "source": [
    "### Image Compression with SVD\n",
    "\n",
    "Singular Value Decomposition (SVD) is a powerful matrix factorization technique that can be used for image compression. The SVD of a matrix $A$ is given by:\n",
    "\n",
    "$A = U \\Sigma V^T$\n",
    "\n",
    "where:\n",
    "- $U$ is an orthogonal matrix of left singular vectors\n",
    "- $\\Sigma$ is a diagonal matrix of singular values\n",
    "- $V^T$ is the transpose of an orthogonal matrix of right singular vectors\n",
    "\n",
    "By keeping only the largest singular values and their corresponding singular vectors, we can create a low-rank approximation of the original image.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b68d0c80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_image_compression(image, ranks_to_test):\n",
    "    \"\"\"Compress an image using SVD with different rank approximations.\"\"\"\n",
    "    # Perform SVD\n",
    "    U, S, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "    \n",
    "    # Calculate the storage requirements\n",
    "    original_size = image.size\n",
    "    \n",
    "    # Create the compressed images\n",
    "    compressed_images = []\n",
    "    compression_ratios = []\n",
    "    \n",
    "    for r in ranks_to_test:\n",
    "        # Low-rank approximation\n",
    "        compressed = U[:, :r] @ np.diag(S[:r]) @ Vt[:r, :]\n",
    "        compressed_images.append(compressed)\n",
    "        \n",
    "        # Calculate compression ratio\n",
    "        compressed_size = r * (U.shape[0] + Vt.shape[1] + 1)  # +1 for singular value\n",
    "        compression_ratio = original_size / compressed_size\n",
    "        compression_ratios.append(compression_ratio)\n",
    "    \n",
    "    return compressed_images, compression_ratios, S\n",
    "\n",
    "# Define ranks for compression\n",
    "ranks = [1, 5, 10, 20, 50, 100]\n",
    "\n",
    "# Compress the image\n",
    "compressed_images, compression_ratios, singular_values = svd_image_compression(image, ranks)\n",
    "\n",
    "# Display original and compressed images\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "# Original\n",
    "plt.subplot(2, 4, 1)\n",
    "plt.imshow(image, cmap='gray')\n",
    "plt.title(\"Original\")\n",
    "plt.axis('off')\n",
    "\n",
    "# Compressed images\n",
    "for i, (r, img, ratio) in enumerate(zip(ranks, compressed_images, compression_ratios)):\n",
    "    plt.subplot(2, 4, i + 2)\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.title(f\"Rank {r}\\nCompression Ratio: {ratio:.1f}:1\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cead28",
   "metadata": {},
   "source": [
    "### Analysis of Singular Values\n",
    "\n",
    "Let's analyze the singular values to understand how much information is captured by each rank approximation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57bda37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_singular_values(S):\n",
    "    \"\"\"Analyze and visualize singular values.\"\"\"\n",
    "    # Total energy (sum of all singular values squared)\n",
    "    total_energy = np.sum(S**2)\n",
    "    \n",
    "    # Calculate cumulative energy percentage\n",
    "    cumulative_energy = np.cumsum(S**2) / total_energy * 100\n",
    "    \n",
    "    # Plot singular values\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(S, 'o-', color='blue', markersize=4)\n",
    "    plt.yscale('log')\n",
    "    plt.xlabel('Index')\n",
    "    plt.ylabel('Singular Value (log scale)')\n",
    "    plt.title('Singular Values')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(cumulative_energy, 'o-', color='green', markersize=4)\n",
    "    plt.xlabel('Number of Singular Values')\n",
    "    plt.ylabel('Cumulative Energy (%)')\n",
    "    plt.title('Energy Captured vs. Rank')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add threshold lines for reference\n",
    "    thresholds = [90, 95, 99]\n",
    "    for t in thresholds:\n",
    "        # Find the rank needed to capture t% of the energy\n",
    "        rank = np.where(cumulative_energy >= t)[0][0] + 1\n",
    "        plt.axhline(y=t, color='red', linestyle='--', alpha=0.3)\n",
    "        plt.axvline(x=rank, color='red', linestyle='--', alpha=0.3)\n",
    "        plt.text(rank + 5, t - 2, f\"{rank} singular values = {t}% energy\", fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print some statistics\n",
    "    print(\"Singular Value Analysis:\")\n",
    "    for t in thresholds:\n",
    "        rank = np.where(cumulative_energy >= t)[0][0] + 1\n",
    "        print(f\"{rank} singular values capture {t}% of the image information\")\n",
    "    \n",
    "    return cumulative_energy\n",
    "\n",
    "# Analyze singular values\n",
    "cumulative_energy = analyze_singular_values(singular_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a640158",
   "metadata": {},
   "source": [
    "### Visual Interpretation of SVD Components\n",
    "\n",
    "Let's visualize how the image is constructed from the singular vectors. Each rank-1 component is the outer product of a left and right singular vector, weighted by the corresponding singular value.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eedce68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_svd_components(image, n_components=5):\n",
    "    \"\"\"Visualize the top SVD components of an image.\"\"\"\n",
    "    # Perform SVD\n",
    "    U, S, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "    \n",
    "    # Visualize original image and top components\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Original image\n",
    "    plt.subplot(2, n_components + 1, 1)\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"Original\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Cumulative reconstruction\n",
    "    plt.subplot(2, n_components + 1, n_components + 2)\n",
    "    cumulative = np.zeros_like(image, dtype=float)\n",
    "    plt.imshow(cumulative, cmap='gray')\n",
    "    plt.title(\"Rank 0\")\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Visualize individual components and cumulative reconstruction\n",
    "    for i in range(n_components):\n",
    "        # Rank-1 component from outer product of singular vectors\n",
    "        component = np.outer(U[:, i], Vt[i, :]) * S[i]\n",
    "        \n",
    "        # Scale for visualization\n",
    "        scaled_component = (component - component.min()) / (component.max() - component.min())\n",
    "        \n",
    "        # Display component\n",
    "        plt.subplot(2, n_components + 1, i + 2)\n",
    "        plt.imshow(scaled_component, cmap='gray')\n",
    "        plt.title(f\"Component {i+1}\\nλ = {S[i]:.1f}\")\n",
    "        plt.axis('off')\n",
    "        \n",
    "        # Update cumulative reconstruction\n",
    "        cumulative += component\n",
    "        scaled_cumulative = np.clip(cumulative, 0, 255)\n",
    "        \n",
    "        # Display cumulative\n",
    "        plt.subplot(2, n_components + 1, n_components + i + 3)\n",
    "        plt.imshow(scaled_cumulative, cmap='gray')\n",
    "        plt.title(f\"Rank {i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize SVD components\n",
    "visualize_svd_components(image, n_components=5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb7435bb",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that uses matrix properties to identify the most informative directions in a dataset. PCA relies on eigendecomposition of the covariance matrix to find these principal components.\n",
    "\n",
    "Let's apply PCA to visualize high-dimensional data and understand how it relates to matrix properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4615646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_digit_data():\n",
    "    \"\"\"Load the digits dataset for PCA demonstration.\"\"\"\n",
    "    digits = load_digits()\n",
    "    X = digits.data\n",
    "    y = digits.target\n",
    "    \n",
    "    return X, y, digits.images[0].shape\n",
    "\n",
    "# Load digits data\n",
    "X, y, image_shape = load_digit_data()\n",
    "\n",
    "# Standardize the data\n",
    "X_std = StandardScaler().fit_transform(X)\n",
    "\n",
    "# Perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_std)\n",
    "\n",
    "# Calculate explained variance\n",
    "explained_variance = pca.explained_variance_ratio_ * 100\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Show sample images\n",
    "plt.figure(figsize=(12, 4))\n",
    "for i in range(10):\n",
    "    plt.subplot(2, 5, i + 1)\n",
    "    plt.imshow(X[y == i][0].reshape(image_shape), cmap='gray')\n",
    "    plt.title(f\"Digit: {i}\")\n",
    "    plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Visualize explained variance\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(range(1, 11), explained_variance[:10], alpha=0.7, color='skyblue', edgecolor='blue')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance (%)')\n",
    "plt.title('Variance Explained by Each Principal Component')\n",
    "plt.xticks(range(1, 11))\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'o-', color='green', markersize=4)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance (%)')\n",
    "plt.title('Cumulative Explained Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add threshold lines\n",
    "thresholds = [70, 80, 90, 95]\n",
    "for t in thresholds:\n",
    "    # Find the number of components needed to explain t% of the variance\n",
    "    n_components = np.where(cumulative_variance >= t)[0][0] + 1\n",
    "    plt.axhline(y=t, color='red', linestyle='--', alpha=0.3)\n",
    "    plt.axvline(x=n_components, color='red', linestyle='--', alpha=0.3)\n",
    "    plt.text(n_components + 1, t - 3, f\"{n_components} PCs\", fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98ee9751",
   "metadata": {},
   "source": [
    "### Visualizing Principal Components\n",
    "\n",
    "Let's visualize the first few principal components to understand what features they capture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935ca08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_principal_components(pca_model, image_shape, n_components=8):\n",
    "    \"\"\"Visualize principal components as images.\"\"\"\n",
    "    # Get the components\n",
    "    components = pca_model.components_\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    for i in range(n_components):\n",
    "        plt.subplot(2, n_components // 2, i + 1)\n",
    "        # Reshape component to image shape\n",
    "        component = components[i].reshape(image_shape)\n",
    "        \n",
    "        # Scale for visualization\n",
    "        vmax = max(abs(component.max()), abs(component.min()))\n",
    "        plt.imshow(component, cmap='RdBu_r', vmin=-vmax, vmax=vmax)\n",
    "        plt.title(f\"PC {i+1}\\n({explained_variance[i]:.1f}%)\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize principal components\n",
    "visualize_principal_components(pca, image_shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440ec35",
   "metadata": {},
   "source": [
    "### Visualizing Data in Principal Component Space\n",
    "\n",
    "Let's visualize the digits data in the space defined by the first two principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1534eb00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pca_scatter(X_pca, y, n_classes=10):\n",
    "    \"\"\"Plot a scatter plot of data in PCA space.\"\"\"\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Create a colormap\n",
    "    cmap = plt.cm.get_cmap('tab10', n_classes)\n",
    "    \n",
    "    # Plot each class with a different color\n",
    "    for i in range(n_classes):\n",
    "        plt.scatter(X_pca[y == i, 0], X_pca[y == i, 1], \n",
    "                    alpha=0.7, color=cmap(i), label=f\"Digit {i}\")\n",
    "    \n",
    "    plt.xlabel(f\"Principal Component 1 ({explained_variance[0]:.1f}%)\")\n",
    "    plt.ylabel(f\"Principal Component 2 ({explained_variance[1]:.1f}%)\")\n",
    "    plt.title(\"Digits Dataset in PCA Space\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot PCA scatter plot\n",
    "plot_pca_scatter(X_pca, y)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb3fff",
   "metadata": {},
   "source": [
    "### Reconstructing Data from Principal Components\n",
    "\n",
    "We can also reconstruct the original data using a subset of principal components, which is useful for compression and denoising.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb702d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_from_pca(X, pca_model, n_components_list):\n",
    "    \"\"\"Reconstruct data using different numbers of principal components.\"\"\"\n",
    "    # Original data shape\n",
    "    X_std = StandardScaler().fit_transform(X)\n",
    "    \n",
    "    # Transform to PCA space\n",
    "    X_pca = pca_model.transform(X_std)\n",
    "    \n",
    "    # Sample a few digits for demonstration\n",
    "    digit_indices = [np.where(y == i)[0][0] for i in range(10)]\n",
    "    sample_digits = X_std[digit_indices]\n",
    "    sample_pca = X_pca[digit_indices]\n",
    "    \n",
    "    # Visualize reconstructions\n",
    "    reconstructions = []\n",
    "    for n_components in n_components_list:\n",
    "        # Zero out components we're not using\n",
    "        reduced_sample = sample_pca.copy()\n",
    "        reduced_sample[:, n_components:] = 0\n",
    "        \n",
    "        # Inverse transform to original space\n",
    "        reconstruction = pca_model.inverse_transform(reduced_sample)\n",
    "        reconstructions.append(reconstruction)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # For each digit\n",
    "    for i, idx in enumerate(range(10)):\n",
    "        # Original\n",
    "        plt.subplot(len(n_components_list) + 1, 10, i + 1)\n",
    "        plt.imshow(sample_digits[i].reshape(image_shape), cmap='gray')\n",
    "        if i == 0:\n",
    "            plt.ylabel(\"Original\", rotation=90, size=12)\n",
    "        plt.axis('off')\n",
    "        plt.title(f\"Digit {idx}\")\n",
    "        \n",
    "        # Reconstructions\n",
    "        for j, (n_components, reconstruction) in enumerate(zip(n_components_list, reconstructions)):\n",
    "            plt.subplot(len(n_components_list) + 1, 10, (j + 1) * 10 + i + 1)\n",
    "            plt.imshow(reconstruction[i].reshape(image_shape), cmap='gray')\n",
    "            if i == 0:\n",
    "                plt.ylabel(f\"{n_components} PCs\", rotation=90, size=12)\n",
    "            plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Reconstruct digits with different numbers of principal components\n",
    "n_components_list = [5, 10, 20, 40]\n",
    "reconstruct_from_pca(X, pca, n_components_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be83c9ac",
   "metadata": {},
   "source": [
    "## 3. Linear Systems and Condition Number\n",
    "\n",
    "The condition number of a matrix affects the stability of linear systems. A high condition number indicates that small changes in the input can cause large changes in the output, which can lead to numerical instability.\n",
    "\n",
    "Let's explore how the condition number affects the solution of linear systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b59472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_matrices_with_varying_condition(sizes=[10, 50, 100, 200], random_state=42):\n",
    "    \"\"\"Create matrices with different condition numbers.\"\"\"\n",
    "    np.random.seed(random_state)\n",
    "    \n",
    "    matrices = []\n",
    "    for size in sizes:\n",
    "        # Create a random matrix\n",
    "        A = np.random.rand(size, size)\n",
    "        \n",
    "        # Compute the SVD\n",
    "        U, S, Vt = np.linalg.svd(A)\n",
    "        \n",
    "        # Create matrices with different condition numbers by manipulating singular values\n",
    "        \n",
    "        # Well-conditioned matrix (condition number ≈ 1)\n",
    "        S_well = np.ones_like(S)\n",
    "        A_well = U @ np.diag(S_well) @ Vt\n",
    "        cond_well = np.linalg.cond(A_well)\n",
    "        matrices.append((\"Well-conditioned\", A_well, cond_well, size))\n",
    "        \n",
    "        # Moderately ill-conditioned matrix\n",
    "        S_moderate = np.logspace(0, 3, size)  # Linear decay\n",
    "        A_moderate = U @ np.diag(S_moderate) @ Vt\n",
    "        cond_moderate = np.linalg.cond(A_moderate)\n",
    "        matrices.append((\"Moderately ill-conditioned\", A_moderate, cond_moderate, size))\n",
    "        \n",
    "        # Severely ill-conditioned matrix\n",
    "        S_severe = np.logspace(0, 6, size)  # Exponential decay\n",
    "        A_severe = U @ np.diag(S_severe) @ Vt\n",
    "        cond_severe = np.linalg.cond(A_severe)\n",
    "        matrices.append((\"Severely ill-conditioned\", A_severe, cond_severe, size))\n",
    "    \n",
    "    return matrices\n",
    "\n",
    "# Create matrices with different condition numbers\n",
    "matrices = create_matrices_with_varying_condition()\n",
    "\n",
    "# Display condition numbers\n",
    "print(\"Matrix Condition Numbers:\")\n",
    "for name, A, cond, size in matrices:\n",
    "    print(f\"{name} ({size}x{size}): {cond:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9719bf2",
   "metadata": {},
   "source": [
    "### Solving Linear Systems with Different Condition Numbers\n",
    "\n",
    "Now let's see how the condition number affects the solution of linear systems when there's noise in the input.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acfdbb2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_linear_system_with_noise(matrices, noise_levels=[0, 1e-10, 1e-8, 1e-6, 1e-4]):\n",
    "    \"\"\"Simulate solving linear systems with different noise levels.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, A, cond, size in matrices:\n",
    "        # Create a known solution\n",
    "        x_true = np.ones(size)\n",
    "        \n",
    "        # Compute the right-hand side\n",
    "        b = A @ x_true\n",
    "        \n",
    "        # Solve the system with different noise levels\n",
    "        errors = []\n",
    "        for noise_level in noise_levels:\n",
    "            # Add noise to b\n",
    "            noise = np.random.normal(0, noise_level, size)\n",
    "            b_noisy = b + noise\n",
    "            \n",
    "            # Solve the system\n",
    "            try:\n",
    "                x_noisy = np.linalg.solve(A, b_noisy)\n",
    "                \n",
    "                # Compute relative error\n",
    "                rel_error = np.linalg.norm(x_noisy - x_true) / np.linalg.norm(x_true)\n",
    "                errors.append(rel_error)\n",
    "            except np.linalg.LinAlgError:\n",
    "                errors.append(np.nan)\n",
    "        \n",
    "        results.append((name, cond, size, errors))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Simulate linear systems with noise\n",
    "results = simulate_linear_system_with_noise(matrices[:9])  # Use the first 9 matrices\n",
    "\n",
    "# Plot the results\n",
    "plt.figure(figsize=(12, 8))\n",
    "\n",
    "# Group by condition number type\n",
    "for i, cond_type in enumerate([\"Well-conditioned\", \"Moderately ill-conditioned\", \"Severely ill-conditioned\"]):\n",
    "    plt.subplot(1, 3, i+1)\n",
    "    \n",
    "    # Filter results for this condition type\n",
    "    type_results = [r for r in results if r[0] == cond_type]\n",
    "    \n",
    "    # Plot for each matrix size\n",
    "    noise_levels = [0, 1e-10, 1e-8, 1e-6, 1e-4]\n",
    "    for name, cond, size, errors in type_results:\n",
    "        plt.semilogy(range(len(noise_levels)), errors, 'o-', label=f\"Size {size}, κ={cond:.2e}\")\n",
    "    \n",
    "    plt.xlabel(\"Noise Level\")\n",
    "    plt.ylabel(\"Relative Error\")\n",
    "    plt.title(f\"{cond_type} Matrices\")\n",
    "    plt.xticks(range(len(noise_levels)), [f\"{n:.0e}\" if n > 0 else \"0\" for n in noise_levels])\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72834f4d",
   "metadata": {},
   "source": [
    "### Visualizing the Effect of Condition Number on a 2D System\n",
    "\n",
    "For a 2D system, we can visualize how the condition number affects the solution geometrically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2109e8e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_condition_number_2d():\n",
    "    \"\"\"Visualize the effect of condition number on a 2D linear system.\"\"\"\n",
    "    # Create matrices with different condition numbers\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Well-conditioned matrix\n",
    "    A_well = np.array([[1.0, 0.2], \n",
    "                        [0.2, 1.0]])\n",
    "    \n",
    "    # Moderately ill-conditioned matrix\n",
    "    A_moderate = np.array([[1.0, 0.99], \n",
    "                           [0.99, 1.0]])\n",
    "    \n",
    "    # Severely ill-conditioned matrix\n",
    "    A_severe = np.array([[1.0, 0.999], \n",
    "                         [0.999, 1.0]])\n",
    "    \n",
    "    matrices = [\n",
    "        (\"Well-conditioned\", A_well, np.linalg.cond(A_well)),\n",
    "        (\"Moderately ill-conditioned\", A_moderate, np.linalg.cond(A_moderate)),\n",
    "        (\"Severely ill-conditioned\", A_severe, np.linalg.cond(A_severe))\n",
    "    ]\n",
    "    \n",
    "    # Define true solution\n",
    "    x_true = np.array([1.0, 1.0])\n",
    "    \n",
    "    # Create figure\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, (name, A, cond) in enumerate(matrices):\n",
    "        # Compute right-hand side\n",
    "        b = A @ x_true\n",
    "        \n",
    "        # Create grid for visualization\n",
    "        n = 100\n",
    "        x1 = np.linspace(-1, 3, n)\n",
    "        x2 = np.linspace(-1, 3, n)\n",
    "        X1, X2 = np.meshgrid(x1, x2)\n",
    "        \n",
    "        # Compute residual for each point on the grid\n",
    "        residuals = np.zeros((n, n))\n",
    "        for i1 in range(n):\n",
    "            for i2 in range(n):\n",
    "                x = np.array([X1[i1, i2], X2[i1, i2]])\n",
    "                residuals[i1, i2] = np.linalg.norm(A @ x - b)\n",
    "        \n",
    "        # Plot\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        \n",
    "        # Contour plot of residuals\n",
    "        contour = plt.contourf(X1, X2, residuals, 50, cmap='viridis', alpha=0.7)\n",
    "        plt.colorbar(contour, label=\"Residual ||Ax - b||\")\n",
    "        \n",
    "        # Plot true solution\n",
    "        plt.scatter(x_true[0], x_true[1], color='red', marker='x', s=100, label=\"True Solution\")\n",
    "        \n",
    "        # Add noise to b and solve\n",
    "        noise_levels = [1e-6, 1e-4, 1e-2]\n",
    "        for j, noise_level in enumerate(noise_levels):\n",
    "            np.random.seed(j)  # Make it reproducible\n",
    "            noise = np.random.normal(0, noise_level, 2)\n",
    "            b_noisy = b + noise\n",
    "            \n",
    "            try:\n",
    "                x_noisy = np.linalg.solve(A, b_noisy)\n",
    "                plt.scatter(x_noisy[0], x_noisy[1], color=f'C{j+1}', marker='o', \n",
    "                            label=f\"Noise {noise_level:.0e}\")\n",
    "                \n",
    "                # Draw line connecting true and noisy solutions\n",
    "                plt.plot([x_true[0], x_noisy[0]], [x_true[1], x_noisy[1]], \n",
    "                         color=f'C{j+1}', linestyle='--', alpha=0.5)\n",
    "            except np.linalg.LinAlgError:\n",
    "                print(f\"Could not solve system with noise level {noise_level}\")\n",
    "        \n",
    "        plt.title(f\"{name}\\nCondition Number: {cond:.2e}\")\n",
    "        plt.xlabel(\"x₁\")\n",
    "        plt.ylabel(\"x₂\")\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Plot SVD ellipse\n",
    "        plt.subplot(2, 3, i+4)\n",
    "        \n",
    "        # Compute SVD\n",
    "        U, S, Vt = np.linalg.svd(A)\n",
    "        \n",
    "        # Plot unit circle\n",
    "        theta = np.linspace(0, 2*np.pi, 100)\n",
    "        circle_x = np.cos(theta)\n",
    "        circle_y = np.sin(theta)\n",
    "        plt.plot(circle_x, circle_y, 'k--', alpha=0.5, label=\"Unit Circle\")\n",
    "        \n",
    "        # Plot transformed circle (ellipse)\n",
    "        transformed = np.zeros((100, 2))\n",
    "        for j in range(100):\n",
    "            point = np.array([circle_x[j], circle_y[j]])\n",
    "            transformed[j] = A @ point\n",
    "        \n",
    "        plt.plot(transformed[:, 0], transformed[:, 1], 'b-', label=\"Transformed Circle\")\n",
    "        \n",
    "        # Plot semi-major and semi-minor axes\n",
    "        plt.arrow(0, 0, S[0]*Vt[0, 0], S[0]*Vt[0, 1], color='red', head_width=0.1, \n",
    "                  head_length=0.1, linewidth=2, label=\"Major Axis\")\n",
    "        plt.arrow(0, 0, S[1]*Vt[1, 0], S[1]*Vt[1, 1], color='green', head_width=0.1, \n",
    "                  head_length=0.1, linewidth=2, label=\"Minor Axis\")\n",
    "        \n",
    "        plt.axhline(y=0, color='k', alpha=0.3)\n",
    "        plt.axvline(x=0, color='k', alpha=0.3)\n",
    "        plt.title(f\"SVD Visualization\\nσ₁={S[0]:.4f}, σ₂={S[1]:.4f}\")\n",
    "        plt.xlabel(\"x₁\")\n",
    "        plt.ylabel(\"x₂\")\n",
    "        plt.legend(fontsize=8)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.axis('equal')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize the effect of condition number in 2D\n",
    "visualize_condition_number_2d()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42786f94",
   "metadata": {},
   "source": [
    "## 4. Graph Analysis with Adjacency Matrices\n",
    "\n",
    "Graphs can be represented using adjacency matrices, where matrix properties provide insights into the graph structure and connectivity. Let's explore how matrix properties can be used for graph analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aedb4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_graph_examples():\n",
    "    \"\"\"Create and analyze different types of graphs.\"\"\"\n",
    "    # Create different types of graphs\n",
    "    graph_types = [\n",
    "        (\"Complete Graph\", nx.complete_graph(10)),\n",
    "        (\"Path Graph\", nx.path_graph(10)),\n",
    "        (\"Star Graph\", nx.star_graph(9)),\n",
    "        (\"Cycle Graph\", nx.cycle_graph(10)),\n",
    "        (\"Random Graph\", nx.gnp_random_graph(10, 0.3, seed=42))\n",
    "    ]\n",
    "    \n",
    "    # Convert to adjacency matrices\n",
    "    adjacency_matrices = []\n",
    "    for name, G in graph_types:\n",
    "        A = nx.to_numpy_array(G)\n",
    "        adjacency_matrices.append((name, G, A))\n",
    "    \n",
    "    return adjacency_matrices\n",
    "\n",
    "# Create graph examples\n",
    "graph_examples = create_graph_examples()\n",
    "\n",
    "# Visualize the graphs and their adjacency matrices\n",
    "plt.figure(figsize=(15, 12))\n",
    "\n",
    "for i, (name, G, A) in enumerate(graph_examples):\n",
    "    # Plot the graph\n",
    "    plt.subplot(len(graph_examples), 2, 2*i+1)\n",
    "    pos = nx.spring_layout(G, seed=42)\n",
    "    nx.draw(G, pos, with_labels=True, node_color='skyblue', node_size=500, \n",
    "            font_size=10, font_weight='bold', width=1.5, edge_color='gray')\n",
    "    plt.title(f\"{name} Graph\")\n",
    "    \n",
    "    # Plot the adjacency matrix\n",
    "    plt.subplot(len(graph_examples), 2, 2*i+2)\n",
    "    plt.imshow(A, cmap='Blues', interpolation='none')\n",
    "    plt.colorbar(label=\"Connection\")\n",
    "    plt.title(f\"{name} Adjacency Matrix\")\n",
    "    \n",
    "    # Add grid lines\n",
    "    plt.grid(False)\n",
    "    for j in range(A.shape[0] + 1):\n",
    "        plt.axhline(y=j-0.5, color='gray', linestyle='-', alpha=0.3)\n",
    "        plt.axvline(x=j-0.5, color='gray', linestyle='-', alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04288a0e",
   "metadata": {},
   "source": [
    "### Eigenvalues and Graph Properties\n",
    "\n",
    "The eigenvalues of the adjacency matrix, or related matrices like the Laplacian matrix, provide important information about the graph structure. Let's explore the relationship between eigenvalues and graph properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "face50ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_graph_spectra(graph_examples):\n",
    "    \"\"\"Analyze the eigenvalue spectra of graph adjacency and Laplacian matrices.\"\"\"\n",
    "    spectra = []\n",
    "    \n",
    "    for name, G, A in graph_examples:\n",
    "        # Adjacency matrix eigenvalues\n",
    "        eigvals_A = np.linalg.eigvals(A)\n",
    "        eigvals_A = np.sort(eigvals_A)[::-1]  # Sort in descending order\n",
    "        \n",
    "        # Laplacian matrix\n",
    "        L = nx.laplacian_matrix(G).toarray()\n",
    "        \n",
    "        # Laplacian eigenvalues\n",
    "        eigvals_L = np.linalg.eigvals(L)\n",
    "        eigvals_L = np.sort(eigvals_L)  # Sort in ascending order\n",
    "        \n",
    "        # Compute graph metrics\n",
    "        avg_degree = np.mean([d for n, d in G.degree()])\n",
    "        n_components = nx.number_connected_components(G)\n",
    "        diameter = max(nx.diameter(C) for C in (G.subgraph(c) for c in nx.connected_components(G)))\n",
    "        \n",
    "        spectra.append((name, eigvals_A, eigvals_L, avg_degree, n_components, diameter))\n",
    "    \n",
    "    return spectra\n",
    "\n",
    "# Analyze graph spectra\n",
    "graph_spectra = analyze_graph_spectra(graph_examples)\n",
    "\n",
    "# Visualize the eigenvalue spectra\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "for i, (name, eigvals_A, eigvals_L, avg_degree, n_components, diameter) in enumerate(graph_spectra):\n",
    "    # Plot adjacency matrix eigenvalues\n",
    "    plt.subplot(len(graph_spectra), 2, 2*i+1)\n",
    "    plt.stem(range(len(eigvals_A)), eigvals_A.real, markerfmt='o', linefmt='b-', basefmt='r-')\n",
    "    plt.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    plt.title(f\"{name} Adjacency Eigenvalues\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate with key eigenvalues and their interpretations\n",
    "    plt.annotate(f\"Largest eigenvalue: {eigvals_A[0]:.2f}\", xy=(0.02, 0.9), xycoords='axes fraction',\n",
    "                 fontsize=9, bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.annotate(f\"Avg degree: {avg_degree:.2f}\\nComponents: {n_components}\\nDiameter: {diameter}\",\n",
    "                 xy=(0.02, 0.75), xycoords='axes fraction', fontsize=9,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    # Plot Laplacian matrix eigenvalues\n",
    "    plt.subplot(len(graph_spectra), 2, 2*i+2)\n",
    "    plt.stem(range(len(eigvals_L)), eigvals_L.real, markerfmt='o', linefmt='g-', basefmt='r-')\n",
    "    plt.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    plt.title(f\"{name} Laplacian Eigenvalues\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate with key eigenvalues and their interpretations\n",
    "    plt.annotate(f\"Second eigenvalue: {eigvals_L[1]:.2f}\\n(algebraic connectivity)\",\n",
    "                 xy=(0.02, 0.9), xycoords='axes fraction', fontsize=9,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    plt.annotate(f\"Zero eigenvalues: {np.sum(np.abs(eigvals_L) < 1e-10)}\\n(= number of connected components)\",\n",
    "                 xy=(0.02, 0.75), xycoords='axes fraction', fontsize=9,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print some key insights\n",
    "print(\"Graph Spectral Analysis:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Graph Type':<20} {'Largest Adj. Eigval':<20} {'Algebraic Connectivity':<25} {'Spectral Gap':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, eigvals_A, eigvals_L, avg_degree, n_components, diameter in graph_spectra:\n",
    "    # Largest adjacency eigenvalue (related to average degree)\n",
    "    largest_adj = eigvals_A[0].real\n",
    "    \n",
    "    # Algebraic connectivity (second smallest Laplacian eigenvalue)\n",
    "    alg_connectivity = eigvals_L[1].real\n",
    "    \n",
    "    # Spectral gap (difference between largest and second largest adjacency eigenvalues)\n",
    "    spectral_gap = eigvals_A[0].real - eigvals_A[1].real\n",
    "    \n",
    "    print(f\"{name:<20} {largest_adj:<20.4f} {alg_connectivity:<25.4f} {spectral_gap:<15.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "521a91f7",
   "metadata": {},
   "source": [
    "### Random Walk and Markov Chains on Graphs\n",
    "\n",
    "The adjacency matrix can be normalized to create a transition matrix for a random walk on the graph. Let's explore how this relates to the stationary distribution and mixing properties of the Markov chain.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ee2300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_random_walks(graph_examples):\n",
    "    \"\"\"Analyze random walks on graphs using transition matrices.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, G, A in graph_examples:\n",
    "        # Calculate degree matrix\n",
    "        degrees = np.sum(A, axis=1)\n",
    "        D_inv = np.diag(1.0 / degrees)\n",
    "        \n",
    "        # Calculate transition matrix P = D^(-1)A\n",
    "        P = D_inv @ A\n",
    "        \n",
    "        # Calculate eigenvalues of P\n",
    "        eigvals_P = np.linalg.eigvals(P)\n",
    "        eigvals_P = np.sort(np.abs(eigvals_P))[::-1]  # Sort by absolute value\n",
    "        \n",
    "        # Compute stationary distribution\n",
    "        # For a simple random walk, it's proportional to node degrees\n",
    "        stationary = degrees / np.sum(degrees)\n",
    "        \n",
    "        # Estimate mixing time (related to second largest eigenvalue)\n",
    "        second_eigval = eigvals_P[1]\n",
    "        mixing_time_estimate = -1.0 / np.log(second_eigval) if second_eigval < 1.0 else np.inf\n",
    "        \n",
    "        results.append((name, P, eigvals_P, stationary, mixing_time_estimate))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze random walks\n",
    "random_walk_results = analyze_random_walks(graph_examples)\n",
    "\n",
    "# Visualize transition matrices and eigenvalues\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, (name, P, eigvals_P, stationary, mixing_time) in enumerate(random_walk_results):\n",
    "    # Plot transition matrix\n",
    "    plt.subplot(len(random_walk_results), 2, 2*i+1)\n",
    "    plt.imshow(P, cmap='Blues', interpolation='none')\n",
    "    plt.colorbar(label=\"Transition Probability\")\n",
    "    plt.title(f\"{name} Transition Matrix\")\n",
    "    \n",
    "    # Add grid lines\n",
    "    plt.grid(False)\n",
    "    for j in range(P.shape[0] + 1):\n",
    "        plt.axhline(y=j-0.5, color='gray', linestyle='-', alpha=0.3)\n",
    "        plt.axvline(x=j-0.5, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Plot eigenvalues of transition matrix\n",
    "    plt.subplot(len(random_walk_results), 2, 2*i+2)\n",
    "    plt.stem(range(len(eigvals_P)), eigvals_P, markerfmt='o', linefmt='m-', basefmt='r-')\n",
    "    plt.axhline(y=0, color='gray', linestyle='-', alpha=0.5)\n",
    "    plt.axhline(y=1, color='gray', linestyle='-', alpha=0.5)\n",
    "    plt.title(f\"{name} Transition Matrix Eigenvalues\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Absolute Eigenvalue\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate with key values\n",
    "    plt.annotate(f\"Second largest eigenvalue: {eigvals_P[1]:.4f}\\nEstimated mixing time: {mixing_time:.2f}\",\n",
    "                 xy=(0.02, 0.9), xycoords='axes fraction', fontsize=9,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nRandom Walk Analysis:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"{'Graph Type':<20} {'Second Largest Eigval':<25} {'Mixing Time Estimate':<20} {'Spectral Gap':<15}\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for name, P, eigvals_P, stationary, mixing_time in random_walk_results:\n",
    "    # Second largest eigenvalue (related to mixing time)\n",
    "    second_eigval = eigvals_P[1]\n",
    "    \n",
    "    # Spectral gap for transition matrix\n",
    "    spectral_gap = 1.0 - second_eigval\n",
    "    \n",
    "    print(f\"{name:<20} {second_eigval:<25.4f} {mixing_time:<20.4f} {spectral_gap:<15.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3145a10f",
   "metadata": {},
   "source": [
    "## 5. Markov Chains and Transition Matrices\n",
    "\n",
    "Markov chains are stochastic processes where the future state depends only on the current state. The transition matrix of a Markov chain has several important properties that we can analyze using matrix properties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d69731f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_markov_chain_examples():\n",
    "    \"\"\"Create example Markov chains with transition matrices.\"\"\"\n",
    "    examples = []\n",
    "    \n",
    "    # Example 1: Weather model (Sunny, Cloudy, Rainy)\n",
    "    weather = np.array([\n",
    "        [0.7, 0.2, 0.1],  # Sunny -> Sunny, Cloudy, Rainy\n",
    "        [0.3, 0.4, 0.3],  # Cloudy -> Sunny, Cloudy, Rainy\n",
    "        [0.2, 0.4, 0.4]   # Rainy -> Sunny, Cloudy, Rainy\n",
    "    ])\n",
    "    examples.append((\"Weather\", weather, [\"Sunny\", \"Cloudy\", \"Rainy\"]))\n",
    "    \n",
    "    # Example 2: Population migration between 4 cities\n",
    "    migration = np.array([\n",
    "        [0.7, 0.1, 0.1, 0.1],  # City A -> A, B, C, D\n",
    "        [0.2, 0.6, 0.1, 0.1],  # City B -> A, B, C, D\n",
    "        [0.1, 0.1, 0.7, 0.1],  # City C -> A, B, C, D\n",
    "        [0.1, 0.1, 0.1, 0.7]   # City D -> A, B, C, D\n",
    "    ])\n",
    "    examples.append((\"Migration\", migration, [\"City A\", \"City B\", \"City C\", \"City D\"]))\n",
    "    \n",
    "    # Example 3: Social class mobility\n",
    "    mobility = np.array([\n",
    "        [0.7, 0.2, 0.1],  # Low -> Low, Medium, High\n",
    "        [0.3, 0.5, 0.2],  # Medium -> Low, Medium, High\n",
    "        [0.1, 0.3, 0.6]   # High -> Low, Medium, High\n",
    "    ])\n",
    "    examples.append((\"Social Mobility\", mobility, [\"Low\", \"Medium\", \"High\"]))\n",
    "    \n",
    "    # Example 4: Queuing system (0, 1, 2, 3 customers in queue)\n",
    "    queue = np.array([\n",
    "        [0.3, 0.7, 0.0, 0.0],  # 0 -> 0, 1, 2, 3\n",
    "        [0.4, 0.3, 0.3, 0.0],  # 1 -> 0, 1, 2, 3\n",
    "        [0.0, 0.5, 0.2, 0.3],  # 2 -> 0, 1, 2, 3\n",
    "        [0.0, 0.0, 0.6, 0.4]   # 3 -> 0, 1, 2, 3\n",
    "    ])\n",
    "    examples.append((\"Queue\", queue, [\"0\", \"1\", \"2\", \"3\"]))\n",
    "    \n",
    "    return examples\n",
    "\n",
    "# Create Markov chain examples\n",
    "markov_examples = create_markov_chain_examples()\n",
    "\n",
    "# Visualize the transition matrices\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for i, (name, P, states) in enumerate(markov_examples):\n",
    "    plt.subplot(2, 2, i+1)\n",
    "    plt.imshow(P, cmap='Blues', interpolation='none')\n",
    "    plt.colorbar(label=\"Transition Probability\")\n",
    "    plt.title(f\"{name} Transition Matrix\")\n",
    "    \n",
    "    # Add state labels\n",
    "    plt.xticks(range(len(states)), states)\n",
    "    plt.yticks(range(len(states)), states)\n",
    "    \n",
    "    # Add grid lines\n",
    "    plt.grid(False)\n",
    "    for j in range(P.shape[0] + 1):\n",
    "        plt.axhline(y=j-0.5, color='gray', linestyle='-', alpha=0.3)\n",
    "        plt.axvline(x=j-0.5, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Annotate probabilities\n",
    "    for ii in range(P.shape[0]):\n",
    "        for jj in range(P.shape[1]):\n",
    "            plt.text(jj, ii, f\"{P[ii, jj]:.1f}\", ha='center', va='center', \n",
    "                     color='white' if P[ii, jj] > 0.5 else 'black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c73b0c03",
   "metadata": {},
   "source": [
    "### Analyzing Markov Chain Properties\n",
    "\n",
    "Let's analyze the properties of our Markov chains, including:\n",
    "\n",
    "- Stationary distribution\n",
    "- Eigenvalue spectrum\n",
    "- Mixing time\n",
    "- Classification of states\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c5a026c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_markov_chains(markov_examples):\n",
    "    \"\"\"Analyze properties of Markov chain transition matrices.\"\"\"\n",
    "    results = []\n",
    "    \n",
    "    for name, P, states in markov_examples:\n",
    "        # Calculate eigenvalues and eigenvectors\n",
    "        eigvals, eigvecs = np.linalg.eig(P.T)  # Transpose for left eigenvectors\n",
    "        \n",
    "        # Sort eigenvalues and eigenvectors\n",
    "        idx = np.argsort(np.abs(eigvals))[::-1]\n",
    "        eigvals = eigvals[idx]\n",
    "        eigvecs = eigvecs[:, idx]\n",
    "        \n",
    "        # Find stationary distribution (corresponds to eigenvalue 1)\n",
    "        stationary = np.real(eigvecs[:, 0])\n",
    "        stationary = stationary / np.sum(stationary)  # Normalize\n",
    "        \n",
    "        # Estimate mixing time\n",
    "        second_eigval = np.abs(eigvals[1])\n",
    "        mixing_time = -1.0 / np.log(second_eigval) if second_eigval < 1.0 else np.inf\n",
    "        \n",
    "        # Check for periodicity and irreducibility\n",
    "        is_irreducible = np.all(np.linalg.matrix_power(P, len(states)-1) > 0)\n",
    "        \n",
    "        # Check if any eigenvalues have magnitude 1 but are not 1\n",
    "        has_periodicity = np.any(np.abs(np.abs(eigvals) - 1.0) < 1e-10 & np.abs(eigvals - 1.0) > 1e-10)\n",
    "        \n",
    "        results.append((name, eigvals, stationary, mixing_time, \n",
    "                        is_irreducible, has_periodicity, states))\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Analyze Markov chains\n",
    "markov_results = analyze_markov_chains(markov_examples)\n",
    "\n",
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 15))\n",
    "\n",
    "for i, (name, eigvals, stationary, mixing_time, is_irreducible, has_periodicity, states) in enumerate(markov_results):\n",
    "    # Plot eigenvalues in the complex plane\n",
    "    plt.subplot(len(markov_results), 2, 2*i+1)\n",
    "    \n",
    "    # Draw unit circle\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    plt.plot(np.cos(theta), np.sin(theta), 'k--', alpha=0.5)\n",
    "    \n",
    "    # Plot eigenvalues\n",
    "    plt.scatter(eigvals.real, eigvals.imag, color='blue', alpha=0.7)\n",
    "    plt.axhline(y=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='gray', linestyle='-', alpha=0.3)\n",
    "    \n",
    "    # Highlight eigenvalue 1\n",
    "    plt.scatter([1], [0], color='red', s=100, edgecolor='black', zorder=3)\n",
    "    \n",
    "    plt.title(f\"{name} Eigenvalues\")\n",
    "    plt.xlabel(\"Real Part\")\n",
    "    plt.ylabel(\"Imaginary Part\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    # Annotate with key properties\n",
    "    chain_type = \"Irreducible\" if is_irreducible else \"Reducible\"\n",
    "    if has_periodicity:\n",
    "        chain_type += \", Periodic\"\n",
    "    else:\n",
    "        chain_type += \", Aperiodic\"\n",
    "    \n",
    "    plt.annotate(f\"Chain type: {chain_type}\\nMixing time: {mixing_time:.2f}\",\n",
    "                 xy=(0.02, 0.9), xycoords='axes fraction', fontsize=9,\n",
    "                 bbox=dict(boxstyle=\"round,pad=0.3\", fc=\"white\", ec=\"gray\", alpha=0.8))\n",
    "    \n",
    "    # Plot stationary distribution\n",
    "    plt.subplot(len(markov_results), 2, 2*i+2)\n",
    "    plt.bar(range(len(states)), stationary, color='skyblue', edgecolor='blue')\n",
    "    plt.xlabel(\"State\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.title(f\"{name} Stationary Distribution\")\n",
    "    plt.xticks(range(len(states)), states)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add horizontal line for uniform distribution\n",
    "    plt.axhline(y=1.0/len(states), color='red', linestyle='--', label=\"Uniform\")\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print key insights\n",
    "print(\"\\nMarkov Chain Analysis:\")\n",
    "print(\"-\" * 100)\n",
    "print(f\"{'Chain Type':<15} {'Stationary Distribution':<40} {'Mixing Time':<15} {'2nd Largest Eigval':<15} {'Type':<15}\")\n",
    "print(\"-\" * 100)\n",
    "\n",
    "for name, eigvals, stationary, mixing_time, is_irreducible, has_periodicity, states in markov_results:\n",
    "    stationary_str = \", \".join([f\"{s:.3f}\" for s in stationary])\n",
    "    \n",
    "    chain_type = \"Irreducible\" if is_irreducible else \"Reducible\"\n",
    "    if has_periodicity:\n",
    "        chain_type += \", Periodic\"\n",
    "    else:\n",
    "        chain_type += \", Aperiodic\"\n",
    "    \n",
    "    print(f\"{name:<15} {stationary_str:<40} {mixing_time:<15.2f} {np.abs(eigvals[1]):<15.4f} {chain_type:<15}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256f3246",
   "metadata": {},
   "source": [
    "### Simulating Markov Chain Evolution\n",
    "\n",
    "Let's simulate the evolution of state probabilities over time for our Markov chains.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa685f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulate_markov_evolution(markov_examples, n_steps=20):\n",
    "    \"\"\"Simulate the evolution of state probabilities over time.\"\"\"\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for i, (name, P, states) in enumerate(markov_examples):\n",
    "        plt.subplot(2, 2, i+1)\n",
    "        \n",
    "        # Create different initial distributions\n",
    "        n_states = len(states)\n",
    "        distributions = [\n",
    "            np.eye(n_states)[0],  # Start in state 0\n",
    "            np.eye(n_states)[-1],  # Start in last state\n",
    "            np.ones(n_states) / n_states,  # Uniform distribution\n",
    "            np.random.dirichlet(np.ones(n_states))  # Random distribution\n",
    "        ]\n",
    "        \n",
    "        labels = [\"Start in first state\", \"Start in last state\", \n",
    "                  \"Uniform distribution\", \"Random distribution\"]\n",
    "        \n",
    "        # Simulate evolution for each initial distribution\n",
    "        for j, (dist, label) in enumerate(zip(distributions, labels)):\n",
    "            # Initialize state vector\n",
    "            x = dist\n",
    "            \n",
    "            # Record evolution\n",
    "            evolution = np.zeros((n_steps, n_states))\n",
    "            evolution[0] = x\n",
    "            \n",
    "            # Evolve the state\n",
    "            for t in range(1, n_steps):\n",
    "                x = x @ P\n",
    "                evolution[t] = x\n",
    "            \n",
    "            # Plot the evolution of each state probability\n",
    "            for k in range(n_states):\n",
    "                plt.plot(range(n_steps), evolution[:, k], \n",
    "                         color=f'C{j}', linestyle=['-', '--', ':', '-.'][j], \n",
    "                         alpha=0.7 if j == 0 else 0.5,\n",
    "                         label=f\"{label}, State {states[k]}\" if k == 0 else \"\")\n",
    "        \n",
    "        plt.title(f\"{name} Markov Chain Evolution\")\n",
    "        plt.xlabel(\"Time Step\")\n",
    "        plt.ylabel(\"State Probability\")\n",
    "        plt.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Calculate and plot stationary distribution\n",
    "        eigvals, eigvecs = np.linalg.eig(P.T)\n",
    "        idx = np.abs(eigvals - 1.0) < 1e-10\n",
    "        stationary = np.real(eigvecs[:, idx][:, 0])\n",
    "        stationary = stationary / np.sum(stationary)\n",
    "        \n",
    "        for k in range(n_states):\n",
    "            plt.axhline(y=stationary[k], color=f'C{k}', linestyle='-', alpha=0.2)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Simulate Markov chain evolution\n",
    "simulate_markov_evolution(markov_examples[:4])  # Use all examples\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e89e46a",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored various applications of matrix properties:\n",
    "\n",
    "1. **Image Processing and Compression**:\n",
    "   - Using SVD for image compression\n",
    "   - Understanding how singular values relate to image information\n",
    "\n",
    "2. **Principal Component Analysis (PCA)**:\n",
    "   - Using eigendecomposition of the covariance matrix to find principal components\n",
    "   - Visualizing data in reduced dimensions\n",
    "\n",
    "3. **Linear Systems and Condition Number**:\n",
    "   - Understanding how condition number affects numerical stability\n",
    "   - Visualizing the effect of noise on solution accuracy\n",
    "\n",
    "4. **Graph Analysis with Adjacency Matrices**:\n",
    "   - Using eigenvalues to understand graph properties\n",
    "   - Analyzing random walks on graphs\n",
    "\n",
    "5. **Markov Chains and Transition Matrices**:\n",
    "   - Finding stationary distributions\n",
    "   - Analyzing mixing times and convergence properties\n",
    "\n",
    "These applications demonstrate the power and versatility of matrix properties in solving real-world problems across various domains."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
