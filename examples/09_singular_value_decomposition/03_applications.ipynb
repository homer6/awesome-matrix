{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "33248309",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/09_singular_value_decomposition`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/09_singular_value_decomposition/03_applications.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dff374f",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition: Applications\n",
    "\n",
    "The Singular Value Decomposition (SVD) is a powerful matrix factorization with applications across numerous fields. This notebook explores practical applications of SVD in various domains, demonstrating its versatility and utility.\n",
    "\n",
    "We'll explore the following applications:\n",
    "\n",
    "1. **Image Compression**\n",
    "2. **Principal Component Analysis (PCA)**\n",
    "3. **Latent Semantic Analysis (LSA)**\n",
    "4. **Recommendation Systems**\n",
    "5. **Signal Processing and Denoising**\n",
    "6. **Pseudoinverse and Least Squares Solutions**\n",
    "\n",
    "Each application showcases how the mathematical properties of SVD can be leveraged to solve real-world problems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ae94a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.linalg\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from sklearn.datasets import fetch_olivetti_faces, make_blobs, load_digits\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import requests\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "import pandas as pd\n",
    "from scipy import signal\n",
    "\n",
    "# For better looking plots\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom colormap (light blue to dark blue)\n",
    "colors = [(0.95, 0.95, 1), (0.0, 0.2, 0.6)]\n",
    "blue_cmap = LinearSegmentedColormap.from_list('CustomBlue', colors, N=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98475f50",
   "metadata": {},
   "source": [
    "## 1. Image Compression\n",
    "\n",
    "SVD provides an effective way to compress images by representing them with a lower-rank approximation. This is particularly useful because the singular values typically decay rapidly, meaning that we can capture most of the image's information with just a few components.\n",
    "\n",
    "Let's demonstrate image compression using SVD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5218a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(url=None, size=(512, 512)):\n",
    "    \"\"\"Load an image from a URL or use a local file.\"\"\"\n",
    "    try:\n",
    "        if url:\n",
    "            response = requests.get(url)\n",
    "            img = Image.open(BytesIO(response.content))\n",
    "        else:\n",
    "            # Use a scikit-learn sample image\n",
    "            faces = fetch_olivetti_faces()\n",
    "            face = faces.images[0]\n",
    "            img = Image.fromarray((face * 255).astype(np.uint8))\n",
    "        \n",
    "        # Convert to grayscale and resize\n",
    "        img = img.convert('L').resize(size)\n",
    "        return np.array(img)\n",
    "    except:\n",
    "        # Create a synthetic image if loading fails\n",
    "        print(\"Failed to load image, creating a synthetic one\")\n",
    "        x, y = np.meshgrid(np.linspace(-3, 3, size[0]), np.linspace(-3, 3, size[1]))\n",
    "        img = np.exp(-(x**2 + y**2) / 2) * 255\n",
    "        return img.astype(np.uint8)\n",
    "\n",
    "def compress_image_with_svd(image, k_values=None):\n",
    "    \"\"\"\n",
    "    Compress an image using SVD with different numbers of singular values.\n",
    "    \n",
    "    Args:\n",
    "        image: Input image (2D numpy array)\n",
    "        k_values: List of k values (numbers of singular components) to use\n",
    "        \n",
    "    Returns:\n",
    "        compressed_images: Dictionary mapping k values to compressed images\n",
    "        U, S, V: SVD components of the image\n",
    "    \"\"\"\n",
    "    # Compute SVD of the image\n",
    "    U, S, Vt = np.linalg.svd(image, full_matrices=False)\n",
    "    \n",
    "    # Default k values if not provided\n",
    "    if k_values is None:\n",
    "        max_k = min(image.shape)\n",
    "        k_values = [1, 5, 10, 20, 50, 100, max_k]\n",
    "        k_values = [k for k in k_values if k <= max_k]\n",
    "    \n",
    "    # Reconstruct the image with different numbers of singular values\n",
    "    compressed_images = {}\n",
    "    for k in k_values:\n",
    "        # Truncate the SVD\n",
    "        U_k = U[:, :k]\n",
    "        S_k = S[:k]\n",
    "        Vt_k = Vt[:k, :]\n",
    "        \n",
    "        # Reconstruct\n",
    "        compressed = U_k @ np.diag(S_k) @ Vt_k\n",
    "        compressed_images[k] = np.clip(compressed, 0, 255).astype(np.uint8)\n",
    "    \n",
    "    return compressed_images, U, S, Vt\n",
    "\n",
    "def calculate_compression_ratio(image, k):\n",
    "    \"\"\"Calculate the compression ratio achieved with k singular values.\"\"\"\n",
    "    m, n = image.shape\n",
    "    original_size = m * n\n",
    "    compressed_size = k * (m + n + 1)  # k values in each of U, S, V\n",
    "    return original_size / compressed_size\n",
    "\n",
    "def calculate_image_quality(original, compressed):\n",
    "    \"\"\"Calculate the PSNR (Peak Signal-to-Noise Ratio) between images.\"\"\"\n",
    "    mse = np.mean((original - compressed) ** 2)\n",
    "    if mse == 0:\n",
    "        return float('inf')\n",
    "    psnr = 20 * np.log10(255.0 / np.sqrt(mse))\n",
    "    return psnr\n",
    "\n",
    "def demonstrate_image_compression():\n",
    "    \"\"\"Demonstrate image compression using SVD.\"\"\"\n",
    "    # Load an image\n",
    "    image = load_image(size=(256, 256))\n",
    "    \n",
    "    # Display the original image\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.imshow(image, cmap='gray')\n",
    "    plt.title(\"Original Image\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "    # Compress the image with different numbers of singular values\n",
    "    max_k = min(image.shape)\n",
    "    k_values = [1, 5, 10, 20, 50, 100, int(max_k/4), int(max_k/2)]\n",
    "    k_values = sorted(list(set([k for k in k_values if k <= max_k])))\n",
    "    \n",
    "    compressed_images, U, S, Vt = compress_image_with_svd(image, k_values)\n",
    "    \n",
    "    # Display the compressed images\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    for i, k in enumerate(k_values):\n",
    "        plt.subplot(2, 4, i+1)\n",
    "        plt.imshow(compressed_images[k], cmap='gray')\n",
    "        compression_ratio = calculate_compression_ratio(image, k)\n",
    "        psnr = calculate_image_quality(image, compressed_images[k])\n",
    "        plt.title(f\"k={k}\\nRatio: {compression_ratio:.1f}:1\\nPSNR: {psnr:.1f} dB\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot the singular values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(S, 'o-')\n",
    "    plt.title(\"Singular Values (Log Scale)\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Plot cumulative energy\n",
    "    energy = S**2 / np.sum(S**2)\n",
    "    cumulative_energy = np.cumsum(energy)\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cumulative_energy, 'o-')\n",
    "    plt.axhline(y=0.9, color='r', linestyle='--', label='90%')\n",
    "    plt.axhline(y=0.99, color='g', linestyle='--', label='99%')\n",
    "    plt.title(\"Cumulative Energy of Singular Values\")\n",
    "    plt.xlabel(\"Number of Singular Values\")\n",
    "    plt.ylabel(\"Cumulative Energy\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find the number of singular values needed to capture 90% and 99% of energy\n",
    "    k_90 = np.sum(cumulative_energy < 0.9) + 1\n",
    "    k_99 = np.sum(cumulative_energy < 0.99) + 1\n",
    "    \n",
    "    print(f\"Number of singular values needed to capture 90% of energy: {k_90}\")\n",
    "    print(f\"Number of singular values needed to capture 99% of energy: {k_99}\")\n",
    "    \n",
    "    # Visualize compression ratio vs. image quality\n",
    "    compression_ratios = [calculate_compression_ratio(image, k) for k in k_values]\n",
    "    psnr_values = [calculate_image_quality(image, compressed_images[k]) for k in k_values]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(compression_ratios, psnr_values, 'o-')\n",
    "    for i, k in enumerate(k_values):\n",
    "        plt.annotate(f\"k={k}\", (compression_ratios[i], psnr_values[i]), \n",
    "                     textcoords=\"offset points\", xytext=(0, 10), ha='center')\n",
    "    plt.title(\"Compression Ratio vs. Image Quality\")\n",
    "    plt.xlabel(\"Compression Ratio\")\n",
    "    plt.ylabel(\"PSNR (dB)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    return image, compressed_images, U, S, Vt\n",
    "\n",
    "# Demonstrate image compression with SVD\n",
    "image_orig, image_compressed, U_img, S_img, Vt_img = demonstrate_image_compression()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a1e986d",
   "metadata": {},
   "source": [
    "## 2. Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a dimensionality reduction technique that can be implemented using SVD. PCA finds the directions of maximum variance in high-dimensional data and projects the data onto a lower-dimensional subspace.\n",
    "\n",
    "The relationship between SVD and PCA is as follows:\n",
    "- For a centered data matrix $X$ (where each column has zero mean), the principal components are the right singular vectors of $X$\n",
    "- The singular values give the standard deviations of the data along each principal component\n",
    "\n",
    "Let's demonstrate PCA using SVD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b7a127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pca_with_svd(X, n_components=2):\n",
    "    \"\"\"\n",
    "    Perform PCA using SVD.\n",
    "    \n",
    "    Args:\n",
    "        X: Data matrix (samples x features)\n",
    "        n_components: Number of principal components to keep\n",
    "        \n",
    "    Returns:\n",
    "        X_transformed: Data projected onto principal components\n",
    "        components: Principal components (eigenvectors)\n",
    "        explained_variance: Variance explained by each component\n",
    "    \"\"\"\n",
    "    # Center the data\n",
    "    X_centered = X - np.mean(X, axis=0)\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, S, Vt = np.linalg.svd(X_centered, full_matrices=False)\n",
    "    \n",
    "    # Principal components are right singular vectors (rows of Vt)\n",
    "    components = Vt[:n_components]\n",
    "    \n",
    "    # Project data onto principal components\n",
    "    X_transformed = X_centered @ components.T\n",
    "    \n",
    "    # Variance explained is proportional to squared singular values\n",
    "    total_variance = np.sum(S**2)\n",
    "    explained_variance = (S**2)[:n_components] / total_variance\n",
    "    \n",
    "    return X_transformed, components, explained_variance\n",
    "\n",
    "def demonstrate_pca():\n",
    "    \"\"\"Demonstrate PCA using SVD.\"\"\"\n",
    "    # Generate a 2D dataset with a clear structure\n",
    "    X, y = make_blobs(n_samples=300, centers=3, n_features=2, random_state=42)\n",
    "    X = X @ np.array([[1, 0.7], [0.7, 1]])  # Add correlation\n",
    "    \n",
    "    # Perform PCA\n",
    "    X_pca, components, explained_variance = pca_with_svd(X, n_components=2)\n",
    "    \n",
    "    # Visualize the original data and principal components\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "    \n",
    "    # Plot principal components\n",
    "    mean = np.mean(X, axis=0)\n",
    "    for i, (comp, var) in enumerate(zip(components, explained_variance)):\n",
    "        plt.arrow(mean[0], mean[1], comp[0]*3, comp[1]*3, \n",
    "                 head_width=0.3, head_length=0.5, fc=f'C{i}', ec=f'C{i}',\n",
    "                 label=f\"PC{i+1} ({var:.1%})\")\n",
    "    \n",
    "    plt.title(\"Original Data with Principal Components\")\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', alpha=0.7)\n",
    "    plt.title(\"Data Projected onto Principal Components\")\n",
    "    plt.xlabel(\"Principal Component 1\")\n",
    "    plt.ylabel(\"Principal Component 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Now demonstrate PCA on a high-dimensional dataset\n",
    "    # Load the digits dataset\n",
    "    digits = load_digits()\n",
    "    X_digits = digits.data\n",
    "    y_digits = digits.target\n",
    "    \n",
    "    # Perform PCA\n",
    "    X_digits_pca, digits_components, digits_variance = pca_with_svd(X_digits, n_components=2)\n",
    "    \n",
    "    # Visualize the projection\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    scatter = plt.scatter(X_digits_pca[:, 0], X_digits_pca[:, 1], c=y_digits, \n",
    "                         cmap='tab10', alpha=0.7)\n",
    "    plt.title(\"Handwritten Digits Projected onto First Two Principal Components\")\n",
    "    plt.xlabel(f\"PC1 ({digits_variance[0]:.1%} variance)\")\n",
    "    plt.ylabel(f\"PC2 ({digits_variance[1]:.1%} variance)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ticks=range(10), label=\"Digit\")\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize some of the principal components (as images)\n",
    "    n_components_to_show = 10\n",
    "    plt.figure(figsize=(15, 3))\n",
    "    for i in range(n_components_to_show):\n",
    "        plt.subplot(1, n_components_to_show, i+1)\n",
    "        component = digits_components[i].reshape(8, 8)\n",
    "        plt.imshow(component, cmap='viridis')\n",
    "        plt.title(f\"PC{i+1}\")\n",
    "        plt.axis('off')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Determine the number of components needed to explain 90% of variance\n",
    "    cumulative_variance = np.cumsum(digits_variance)\n",
    "    n_components_90 = np.sum(cumulative_variance < 0.9) + 1\n",
    "    \n",
    "    # Visualize the explained variance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(cumulative_variance, 'o-')\n",
    "    plt.axhline(y=0.9, color='r', linestyle='--', label='90% Variance')\n",
    "    plt.axvline(x=n_components_90, color='g', linestyle='--', \n",
    "               label=f'{n_components_90} components')\n",
    "    plt.title(\"Cumulative Explained Variance\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"Cumulative Explained Variance\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"Number of components needed to explain 90% of variance: {n_components_90}\")\n",
    "    print(f\"Compression ratio: {X_digits.shape[1] / n_components_90:.1f}:1\")\n",
    "    \n",
    "    return X, X_pca, components, X_digits, X_digits_pca, digits_components\n",
    "\n",
    "# Demonstrate PCA using SVD\n",
    "X_2d, X_2d_pca, pca_components, X_digits, X_digits_pca, digits_components = demonstrate_pca()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5022e14",
   "metadata": {},
   "source": [
    "## 3. Latent Semantic Analysis (LSA)\n",
    "\n",
    "Latent Semantic Analysis (LSA) is a technique in natural language processing for analyzing relationships between documents and the terms they contain. It uses SVD to identify patterns in the relationships between the terms and concepts contained in an unstructured collection of text.\n",
    "\n",
    "In LSA, we start with a term-document matrix where each entry represents the occurrence of a term in a document. SVD is then applied to reduce the dimensionality and uncover latent (hidden) semantic structure.\n",
    "\n",
    "Let's demonstrate LSA on a small text corpus:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e31f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_lsa():\n",
    "    \"\"\"Demonstrate Latent Semantic Analysis using SVD.\"\"\"\n",
    "    # Create a small corpus of documents\n",
    "    documents = [\n",
    "        \"The quick brown fox jumps over the lazy dog\",\n",
    "        \"The dog barks at the fox\",\n",
    "        \"The fox is quick and brown\",\n",
    "        \"The lazy dog sleeps all day\",\n",
    "        \"Quick animals include foxes and rabbits\",\n",
    "        \"Dogs are domesticated animals\",\n",
    "        \"Some animals are lazy while others are quick\"\n",
    "    ]\n",
    "    \n",
    "    # Create a term-document matrix using CountVectorizer\n",
    "    vectorizer = CountVectorizer(stop_words='english')\n",
    "    X = vectorizer.fit_transform(documents).toarray()\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Display the term-document matrix\n",
    "    df_tdm = pd.DataFrame(X, columns=terms, index=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
    "    print(\"Term-Document Matrix:\")\n",
    "    print(df_tdm)\n",
    "    \n",
    "    # Apply SVD for LSA\n",
    "    n_components = 2  # Number of topics\n",
    "    U, S, Vt = np.linalg.svd(X, full_matrices=False)\n",
    "    \n",
    "    # Project documents into the LSA space\n",
    "    X_lsa = U[:, :n_components] @ np.diag(S[:n_components])\n",
    "    \n",
    "    # Get the term-topic matrix (how much each term contributes to each topic)\n",
    "    term_topic = Vt[:n_components, :]\n",
    "    \n",
    "    # Visualize document-topic relationships\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(X_lsa[:, 0], X_lsa[:, 1])\n",
    "    \n",
    "    # Label each point with the document number\n",
    "    for i in range(len(documents)):\n",
    "        plt.annotate(f\"Doc {i+1}\", (X_lsa[i, 0], X_lsa[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.title(\"Documents in LSA Space\")\n",
    "    plt.xlabel(\"Topic 1\")\n",
    "    plt.ylabel(\"Topic 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize term-topic relationships\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(term_topic[0, :], term_topic[1, :])\n",
    "    \n",
    "    # Label each point with the term\n",
    "    for i, term in enumerate(terms):\n",
    "        plt.annotate(term, (term_topic[0, i], term_topic[1, i]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.title(\"Terms in LSA Space\")\n",
    "    plt.xlabel(\"Topic 1\")\n",
    "    plt.ylabel(\"Topic 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Display the most important terms for each topic\n",
    "    n_top_terms = 5\n",
    "    for i in range(n_components):\n",
    "        top_terms_idx = np.argsort(np.abs(term_topic[i, :]))[::-1][:n_top_terms]\n",
    "        top_terms = [(terms[idx], term_topic[i, idx]) for idx in top_terms_idx]\n",
    "        \n",
    "        print(f\"\\nTop terms for Topic {i+1}:\")\n",
    "        for term, weight in top_terms:\n",
    "            print(f\"{term}: {weight:.3f}\")\n",
    "    \n",
    "    # Calculate document similarity in the LSA space\n",
    "    from sklearn.metrics.pairwise import cosine_similarity\n",
    "    doc_sim = cosine_similarity(X_lsa)\n",
    "    \n",
    "    # Visualize document similarity\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(doc_sim, annot=True, fmt=\".2f\", cmap=\"YlGnBu\",\n",
    "               xticklabels=[f\"Doc {i+1}\" for i in range(len(documents))],\n",
    "               yticklabels=[f\"Doc {i+1}\" for i in range(len(documents))])\n",
    "    plt.title(\"Document Similarity (Cosine Similarity in LSA Space)\")\n",
    "    plt.show()\n",
    "    \n",
    "    return X, U, S, Vt, X_lsa, terms, documents\n",
    "\n",
    "# Demonstrate Latent Semantic Analysis\n",
    "tdm, U_lsa, S_lsa, Vt_lsa, X_lsa, terms_lsa, docs_lsa = demonstrate_lsa()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f83090",
   "metadata": {},
   "source": [
    "## 4. Recommendation Systems\n",
    "\n",
    "SVD can be used for collaborative filtering in recommendation systems. The idea is to decompose a user-item interaction matrix into user and item feature matrices, which can then be used to predict missing entries (i.e., potential recommendations).\n",
    "\n",
    "This approach is often called \"matrix factorization\" in the context of recommendation systems. Let's implement a simple SVD-based recommendation system:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd55b73b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_user_item_matrix(n_users=10, n_items=8, density=0.6, noise_level=0.1):\n",
    "    \"\"\"Create a synthetic user-item rating matrix with some missing values.\"\"\"\n",
    "    # Create latent factors (true underlying structure)\n",
    "    n_factors = 3\n",
    "    user_factors = np.random.randn(n_users, n_factors)\n",
    "    item_factors = np.random.randn(n_items, n_factors)\n",
    "    \n",
    "    # Generate full matrix\n",
    "    true_ratings = user_factors @ item_factors.T\n",
    "    \n",
    "    # Scale to rating range (1-5)\n",
    "    true_ratings = 3 + 2 * (true_ratings - np.min(true_ratings)) / (np.max(true_ratings) - np.min(true_ratings))\n",
    "    \n",
    "    # Add noise\n",
    "    ratings = true_ratings + noise_level * np.random.randn(n_users, n_items)\n",
    "    ratings = np.clip(ratings, 1, 5)\n",
    "    \n",
    "    # Set some entries to be missing\n",
    "    mask = np.random.rand(n_users, n_items) < density\n",
    "    ratings_with_missing = ratings * mask\n",
    "    \n",
    "    # Replace 0s with NaNs to indicate missing values\n",
    "    ratings_with_missing[~mask] = np.nan\n",
    "    \n",
    "    return ratings_with_missing, true_ratings\n",
    "\n",
    "def svd_for_recommendation(ratings_matrix, k=None):\n",
    "    \"\"\"\n",
    "    Use SVD for collaborative filtering in recommendations.\n",
    "    \n",
    "    Args:\n",
    "        ratings_matrix: User-item matrix with missing values (NaNs)\n",
    "        k: Number of singular values to use\n",
    "        \n",
    "    Returns:\n",
    "        completed_matrix: Matrix with predictions for missing values\n",
    "        U, S, Vt: SVD components of the imputed matrix\n",
    "    \"\"\"\n",
    "    # Create a copy of the ratings matrix\n",
    "    ratings = ratings_matrix.copy()\n",
    "    \n",
    "    # Replace NaNs with zeros for SVD\n",
    "    # (This is a simple approach; more sophisticated methods would use mean imputation or iterative approaches)\n",
    "    ratings_imputed = np.nan_to_num(ratings, nan=0)\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, S, Vt = np.linalg.svd(ratings_imputed, full_matrices=False)\n",
    "    \n",
    "    # Use only the top k singular values if specified\n",
    "    if k is not None:\n",
    "        U = U[:, :k]\n",
    "        S = S[:k]\n",
    "        Vt = Vt[:k, :]\n",
    "    \n",
    "    # Reconstruct the matrix\n",
    "    completed_matrix = U @ np.diag(S) @ Vt\n",
    "    \n",
    "    # Replace known values with the original ratings\n",
    "    completed_matrix[~np.isnan(ratings_matrix)] = ratings_matrix[~np.isnan(ratings_matrix)]\n",
    "    \n",
    "    return completed_matrix, U, S, Vt\n",
    "\n",
    "def evaluate_recommendations(true_ratings, predicted_ratings, mask):\n",
    "    \"\"\"Evaluate the quality of recommendations.\"\"\"\n",
    "    # Calculate RMSE on the test set (masked entries)\n",
    "    test_mask = ~mask\n",
    "    test_ratings = true_ratings[test_mask]\n",
    "    test_predictions = predicted_ratings[test_mask]\n",
    "    \n",
    "    rmse = np.sqrt(np.mean((test_ratings - test_predictions) ** 2))\n",
    "    return rmse\n",
    "\n",
    "def demonstrate_recommendation_system():\n",
    "    \"\"\"Demonstrate an SVD-based recommendation system.\"\"\"\n",
    "    # Create a synthetic user-item matrix\n",
    "    n_users, n_items = 10, 8\n",
    "    ratings_matrix, true_ratings = create_user_item_matrix(n_users, n_items)\n",
    "    \n",
    "    # Create user and item names for better visualization\n",
    "    users = [f\"User {i+1}\" for i in range(n_users)]\n",
    "    items = [f\"Item {i+1}\" for i in range(n_items)]\n",
    "    \n",
    "    # Visualize the original matrix with missing values\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    mask = ~np.isnan(ratings_matrix)\n",
    "    sns.heatmap(ratings_matrix, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", \n",
    "               mask=~mask, cbar_kws={'label': 'Rating'}, \n",
    "               xticklabels=items, yticklabels=users)\n",
    "    plt.title(\"Original Ratings Matrix with Missing Values\")\n",
    "    \n",
    "    # Apply SVD for collaborative filtering\n",
    "    k = 3  # Number of latent factors\n",
    "    completed_matrix, U, S, Vt = svd_for_recommendation(ratings_matrix, k=k)\n",
    "    \n",
    "    # Visualize the completed matrix\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(completed_matrix, annot=True, fmt=\".1f\", cmap=\"YlGnBu\", \n",
    "               cbar_kws={'label': 'Rating'}, \n",
    "               xticklabels=items, yticklabels=users)\n",
    "    plt.title(f\"Completed Ratings Matrix (k={k})\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate the recommendations for different values of k\n",
    "    k_values = range(1, min(n_users, n_items) + 1)\n",
    "    rmse_values = []\n",
    "    \n",
    "    for k in k_values:\n",
    "        completed_k, _, _, _ = svd_for_recommendation(ratings_matrix, k=k)\n",
    "        rmse = evaluate_recommendations(true_ratings, completed_k, mask)\n",
    "        rmse_values.append(rmse)\n",
    "    \n",
    "    # Plot RMSE vs. number of factors\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(k_values, rmse_values, 'o-')\n",
    "    plt.title(\"Recommendation Quality vs. Number of Latent Factors\")\n",
    "    plt.xlabel(\"Number of Factors (k)\")\n",
    "    plt.ylabel(\"RMSE\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(k_values)\n",
    "    plt.show()\n",
    "    \n",
    "    # Find the optimal number of factors\n",
    "    best_k = k_values[np.argmin(rmse_values)]\n",
    "    print(f\"Best number of latent factors: {best_k}\")\n",
    "    print(f\"Lowest RMSE: {min(rmse_values):.4f}\")\n",
    "    \n",
    "    # Visualize user and item embeddings\n",
    "    # Project users and items into a 2D latent space\n",
    "    k_viz = min(2, min(n_users, n_items))\n",
    "    completed_viz, U_viz, S_viz, Vt_viz = svd_for_recommendation(ratings_matrix, k=k_viz)\n",
    "    \n",
    "    # User embeddings: U * sqrt(S)\n",
    "    user_emb = U_viz @ np.diag(np.sqrt(S_viz))\n",
    "    \n",
    "    # Item embeddings: Vt.T * sqrt(S)\n",
    "    item_emb = Vt_viz.T @ np.diag(np.sqrt(S_viz))\n",
    "    \n",
    "    # Visualize in 2D\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    \n",
    "    # Plot users\n",
    "    plt.scatter(user_emb[:, 0], user_emb[:, 1], marker='o', s=100, label='Users')\n",
    "    for i, user in enumerate(users):\n",
    "        plt.annotate(user, (user_emb[i, 0], user_emb[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    # Plot items\n",
    "    plt.scatter(item_emb[:, 0], item_emb[:, 1], marker='^', s=100, label='Items')\n",
    "    for i, item in enumerate(items):\n",
    "        plt.annotate(item, (item_emb[i, 0], item_emb[i, 1]), \n",
    "                    xytext=(5, 5), textcoords='offset points')\n",
    "    \n",
    "    plt.title(\"Users and Items in Latent Space\")\n",
    "    plt.xlabel(\"Latent Factor 1\")\n",
    "    plt.ylabel(\"Latent Factor 2\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Generate top recommendations for each user\n",
    "    top_n = 3\n",
    "    recommendations = {}\n",
    "    \n",
    "    for i, user in enumerate(users):\n",
    "        # Get items the user hasn't rated\n",
    "        unrated_items = [j for j in range(n_items) if np.isnan(ratings_matrix[i, j])]\n",
    "        \n",
    "        if unrated_items:\n",
    "            # Get predictions for unrated items\n",
    "            predictions = [(items[j], completed_matrix[i, j]) for j in unrated_items]\n",
    "            \n",
    "            # Sort by prediction value\n",
    "            predictions.sort(key=lambda x: x[1], reverse=True)\n",
    "            \n",
    "            # Get top N recommendations\n",
    "            top_recommendations = predictions[:top_n]\n",
    "            recommendations[user] = top_recommendations\n",
    "    \n",
    "    # Display recommendations\n",
    "    print(\"\\nTop Recommendations for Each User:\")\n",
    "    for user, recs in recommendations.items():\n",
    "        print(f\"{user}:\")\n",
    "        for item, score in recs:\n",
    "            print(f\"  {item}: {score:.2f}\")\n",
    "    \n",
    "    return ratings_matrix, completed_matrix, U, S, Vt, rmse_values\n",
    "\n",
    "# Demonstrate recommendation system\n",
    "ratings_orig, ratings_completed, U_rec, S_rec, Vt_rec, rmse_rec = demonstrate_recommendation_system()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4663b907",
   "metadata": {},
   "source": [
    "## 5. Signal Processing and Denoising\n",
    "\n",
    "SVD can be used for signal processing tasks such as denoising. By keeping only the most significant singular values, we can filter out noise while preserving the important features of the signal.\n",
    "\n",
    "Let's demonstrate SVD-based denoising on a signal:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d701339",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_noisy_signal(n_samples=1000, noise_level=0.2):\n",
    "    \"\"\"Create a synthetic signal with added noise.\"\"\"\n",
    "    t = np.linspace(0, 10, n_samples)\n",
    "    \n",
    "    # Create a clean signal with multiple frequencies\n",
    "    signal_clean = (\n",
    "        np.sin(2 * np.pi * 0.5 * t) +  # 0.5 Hz component\n",
    "        0.5 * np.sin(2 * np.pi * 1.5 * t) +  # 1.5 Hz component\n",
    "        0.3 * np.sin(2 * np.pi * 3.0 * t)    # 3.0 Hz component\n",
    "    )\n",
    "    \n",
    "    # Add noise\n",
    "    noise = noise_level * np.random.randn(n_samples)\n",
    "    signal_noisy = signal_clean + noise\n",
    "    \n",
    "    return t, signal_clean, signal_noisy\n",
    "\n",
    "def svd_denoise_signal(signal, window_size=20, n_components=5):\n",
    "    \"\"\"\n",
    "    Denoise a signal using SVD.\n",
    "    \n",
    "    Args:\n",
    "        signal: Input signal (1D array)\n",
    "        window_size: Size of the window for constructing the trajectory matrix\n",
    "        n_components: Number of singular values/vectors to keep\n",
    "        \n",
    "    Returns:\n",
    "        denoised_signal: Denoised signal\n",
    "        trajectory_matrix: Trajectory matrix\n",
    "        U, S, Vt: SVD components of the trajectory matrix\n",
    "    \"\"\"\n",
    "    n_samples = len(signal)\n",
    "    \n",
    "    # Construct the trajectory matrix (Hankel matrix)\n",
    "    n_cols = n_samples - window_size + 1\n",
    "    trajectory_matrix = np.zeros((window_size, n_cols))\n",
    "    \n",
    "    for i in range(n_cols):\n",
    "        trajectory_matrix[:, i] = signal[i:i+window_size]\n",
    "    \n",
    "    # Apply SVD to the trajectory matrix\n",
    "    U, S, Vt = np.linalg.svd(trajectory_matrix, full_matrices=False)\n",
    "    \n",
    "    # Keep only the top n_components\n",
    "    U_k = U[:, :n_components]\n",
    "    S_k = S[:n_components]\n",
    "    Vt_k = Vt[:n_components, :]\n",
    "    \n",
    "    # Reconstruct the trajectory matrix\n",
    "    trajectory_matrix_denoised = U_k @ np.diag(S_k) @ Vt_k\n",
    "    \n",
    "    # Convert back to a signal by averaging along anti-diagonals\n",
    "    denoised_signal = np.zeros(n_samples)\n",
    "    count = np.zeros(n_samples)\n",
    "    \n",
    "    for i in range(window_size):\n",
    "        for j in range(n_cols):\n",
    "            denoised_signal[i+j] += trajectory_matrix_denoised[i, j]\n",
    "            count[i+j] += 1\n",
    "    \n",
    "    denoised_signal /= count\n",
    "    \n",
    "    return denoised_signal, trajectory_matrix, U, S, Vt\n",
    "\n",
    "def demonstrate_signal_denoising():\n",
    "    \"\"\"Demonstrate signal denoising using SVD.\"\"\"\n",
    "    # Create a noisy signal\n",
    "    t, signal_clean, signal_noisy = create_noisy_signal(n_samples=1000, noise_level=0.2)\n",
    "    \n",
    "    # Visualize the clean and noisy signals\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(t, signal_clean, 'b-', label='Clean Signal')\n",
    "    plt.plot(t, signal_noisy, 'r-', alpha=0.5, label='Noisy Signal')\n",
    "    plt.title(\"Original and Noisy Signals\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Apply SVD-based denoising\n",
    "    window_size = 20\n",
    "    n_components = 3\n",
    "    signal_denoised, trajectory_matrix, U, S, Vt = svd_denoise_signal(\n",
    "        signal_noisy, window_size=window_size, n_components=n_components)\n",
    "    \n",
    "    # Visualize the denoised signal\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(t, signal_clean, 'b-', label='Clean Signal')\n",
    "    plt.plot(t, signal_noisy, 'r-', alpha=0.3, label='Noisy Signal')\n",
    "    plt.plot(t, signal_denoised, 'g-', label='Denoised Signal')\n",
    "    plt.title(f\"Signal Denoising with SVD (window_size={window_size}, n_components={n_components})\")\n",
    "    plt.xlabel(\"Time\")\n",
    "    plt.ylabel(\"Amplitude\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Calculate error metrics\n",
    "    noise_error = np.sqrt(np.mean((signal_clean - signal_noisy) ** 2))\n",
    "    denoised_error = np.sqrt(np.mean((signal_clean - signal_denoised) ** 2))\n",
    "    \n",
    "    print(f\"RMS error of noisy signal: {noise_error:.4f}\")\n",
    "    print(f\"RMS error of denoised signal: {denoised_error:.4f}\")\n",
    "    print(f\"Improvement: {(noise_error - denoised_error) / noise_error:.1%}\")\n",
    "    \n",
    "    # Visualize trajectory matrix\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.imshow(trajectory_matrix, aspect='auto', cmap='viridis')\n",
    "    plt.colorbar(label='Amplitude')\n",
    "    plt.title(\"Trajectory Matrix (Hankel Matrix)\")\n",
    "    plt.xlabel(\"Column Index\")\n",
    "    plt.ylabel(\"Row Index\")\n",
    "    \n",
    "    # Plot singular values\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.semilogy(S, 'o-')\n",
    "    plt.axhline(y=S[n_components-1], color='r', linestyle='--', \n",
    "               label=f'Threshold (top {n_components})')\n",
    "    plt.title(\"Singular Values (Log Scale)\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot frequency response\n",
    "    plt.subplot(2, 2, 3)\n",
    "    freq_clean = np.abs(np.fft.rfft(signal_clean))\n",
    "    freq_noisy = np.abs(np.fft.rfft(signal_noisy))\n",
    "    freq_denoised = np.abs(np.fft.rfft(signal_denoised))\n",
    "    freqs = np.fft.rfftfreq(len(signal_clean), d=(t[1]-t[0]))\n",
    "    \n",
    "    plt.plot(freqs, freq_clean, 'b-', label='Clean')\n",
    "    plt.plot(freqs, freq_noisy, 'r-', alpha=0.5, label='Noisy')\n",
    "    plt.plot(freqs, freq_denoised, 'g-', label='Denoised')\n",
    "    plt.title(\"Frequency Domain Comparison\")\n",
    "    plt.xlabel(\"Frequency (Hz)\")\n",
    "    plt.ylabel(\"Magnitude\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlim(0, 5)  # Focus on relevant frequency range\n",
    "    \n",
    "    # Experiment with different numbers of components\n",
    "    plt.subplot(2, 2, 4)\n",
    "    n_components_range = range(1, 11)\n",
    "    errors = []\n",
    "    \n",
    "    for n in n_components_range:\n",
    "        signal_denoised_n, _, _, _, _ = svd_denoise_signal(\n",
    "            signal_noisy, window_size=window_size, n_components=n)\n",
    "        error = np.sqrt(np.mean((signal_clean - signal_denoised_n) ** 2))\n",
    "        errors.append(error)\n",
    "    \n",
    "    plt.plot(n_components_range, errors, 'o-')\n",
    "    plt.title(\"Error vs. Number of Components\")\n",
    "    plt.xlabel(\"Number of Components\")\n",
    "    plt.ylabel(\"RMS Error\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(n_components_range)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return t, signal_clean, signal_noisy, signal_denoised, S\n",
    "\n",
    "# Demonstrate signal denoising\n",
    "t_signal, signal_clean, signal_noisy, signal_denoised, S_signal = demonstrate_signal_denoising()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4d92fc0",
   "metadata": {},
   "source": [
    "## 6. Pseudoinverse and Least Squares Solutions\n",
    "\n",
    "SVD provides a robust way to compute the pseudoinverse of a matrix, which is useful for solving least squares problems and inverting ill-conditioned or non-square matrices.\n",
    "\n",
    "The pseudoinverse of a matrix $A$ is given by:\n",
    "\n",
    "$$A^+ = V \\Sigma^+ U^T$$\n",
    "\n",
    "where $\\Sigma^+$ is formed by taking the reciprocal of each non-zero singular value in $\\Sigma$ and leaving the zeros as zeros.\n",
    "\n",
    "Let's demonstrate how SVD can be used to solve linear systems:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1080e54a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pseudoinverse(A, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Compute the pseudoinverse of a matrix using SVD.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        tol: Tolerance for zero singular values\n",
    "        \n",
    "    Returns:\n",
    "        A_pinv: Pseudoinverse of A\n",
    "    \"\"\"\n",
    "    # Compute SVD\n",
    "    U, S, Vt = np.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Compute reciprocals of singular values, with zeros for small values\n",
    "    S_inv = np.zeros_like(S)\n",
    "    S_inv[S > tol] = 1.0 / S[S > tol]\n",
    "    \n",
    "    # Compute pseudoinverse\n",
    "    A_pinv = Vt.T @ np.diag(S_inv) @ U.T\n",
    "    \n",
    "    return A_pinv\n",
    "\n",
    "def svd_least_squares(A, b, tol=1e-10):\n",
    "    \"\"\"\n",
    "    Solve the least squares problem min ||Ax - b||_2 using SVD.\n",
    "    \n",
    "    Args:\n",
    "        A: Coefficient matrix\n",
    "        b: Right-hand side vector\n",
    "        tol: Tolerance for zero singular values\n",
    "        \n",
    "    Returns:\n",
    "        x: Solution\n",
    "        residual: Residual norm ||Ax - b||_2\n",
    "    \"\"\"\n",
    "    # Compute pseudoinverse\n",
    "    A_pinv = compute_pseudoinverse(A, tol)\n",
    "    \n",
    "    # Solve the system\n",
    "    x = A_pinv @ b\n",
    "    \n",
    "    # Compute residual\n",
    "    residual = np.linalg.norm(A @ x - b)\n",
    "    \n",
    "    return x, residual\n",
    "\n",
    "def demonstrate_pseudoinverse():\n",
    "    \"\"\"Demonstrate pseudoinverse and least squares solutions using SVD.\"\"\"\n",
    "    # Create an overdetermined system (more equations than unknowns)\n",
    "    m, n = 10, 3  # 10 equations, 3 unknowns\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create a coefficient matrix\n",
    "    A = np.random.randn(m, n)\n",
    "    \n",
    "    # Create a true solution\n",
    "    x_true = np.random.randn(n)\n",
    "    \n",
    "    # Create the right-hand side (with some noise)\n",
    "    noise_level = 0.1\n",
    "    b_clean = A @ x_true\n",
    "    b = b_clean + noise_level * np.random.randn(m)\n",
    "    \n",
    "    # Solve using SVD pseudoinverse\n",
    "    x_svd, residual_svd = svd_least_squares(A, b)\n",
    "    \n",
    "    # For comparison, solve using NumPy's least squares function\n",
    "    x_np, residual_np, rank_np, s_np = np.linalg.lstsq(A, b, rcond=None)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Least Squares Solutions:\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'Method':<15} {'Solution':<30} {'Residual':<15}\")\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"{'True':<15} {np.array2string(x_true, precision=4):<30} {'N/A':<15}\")\n",
    "    print(f\"{'SVD':<15} {np.array2string(x_svd, precision=4):<30} {residual_svd:.6f}\")\n",
    "    print(f\"{'NumPy':<15} {np.array2string(x_np, precision=4):<30} {residual_np:.6f}\")\n",
    "    \n",
    "    # Visualize the solutions\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    width = 0.3\n",
    "    indices = np.arange(n)\n",
    "    \n",
    "    plt.bar(indices - width, x_true, width, label='True')\n",
    "    plt.bar(indices, x_svd, width, label='SVD')\n",
    "    plt.bar(indices + width, x_np, width, label='NumPy')\n",
    "    \n",
    "    plt.title(\"Least Squares Solutions\")\n",
    "    plt.xlabel(\"Variable Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.xticks(indices)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the residuals\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    residual_vector_svd = A @ x_svd - b\n",
    "    residual_vector_np = A @ x_np - b\n",
    "    \n",
    "    plt.plot(residual_vector_svd, 'o-', label='SVD')\n",
    "    plt.plot(residual_vector_np, 's-', label='NumPy')\n",
    "    \n",
    "    plt.title(\"Residuals\")\n",
    "    plt.xlabel(\"Equation Index\")\n",
    "    plt.ylabel(\"Residual\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Now demonstrate with an ill-conditioned matrix\n",
    "    A_ill = np.array([\n",
    "        [1, 1, 1],\n",
    "        [1, 1, 1.001],\n",
    "        [1, 1.001, 1],\n",
    "        [1.001, 1, 1]\n",
    "    ])\n",
    "    \n",
    "    # Create a right-hand side\n",
    "    b_ill = np.array([1, 2, 3, 4])\n",
    "    \n",
    "    # Compute the condition number\n",
    "    _, S_ill, _ = np.linalg.svd(A_ill)\n",
    "    cond_num = S_ill[0] / S_ill[-1]\n",
    "    \n",
    "    print(f\"\\nIll-conditioned matrix with condition number: {cond_num:.1e}\")\n",
    "    \n",
    "    # Solve using different tolerances\n",
    "    tolerances = [1e-12, 1e-10, 1e-8, 1e-6, 1e-4]\n",
    "    solutions = []\n",
    "    residuals = []\n",
    "    \n",
    "    for tol in tolerances:\n",
    "        x_tol, residual_tol = svd_least_squares(A_ill, b_ill, tol=tol)\n",
    "        solutions.append(x_tol)\n",
    "        residuals.append(residual_tol)\n",
    "    \n",
    "    # Visualize the effect of the tolerance\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 1, 1)\n",
    "    for i, (tol, x) in enumerate(zip(tolerances, solutions)):\n",
    "        plt.plot(x, 'o-', label=f'Tolerance = {tol:.1e}')\n",
    "    \n",
    "    plt.title(\"Solutions for Different Tolerances\")\n",
    "    plt.xlabel(\"Variable Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.semilogx(tolerances, residuals, 'o-')\n",
    "    plt.title(\"Residual vs. Tolerance\")\n",
    "    plt.xlabel(\"Tolerance\")\n",
    "    plt.ylabel(\"Residual Norm\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the singular values of the ill-conditioned matrix\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.semilogy(S_ill, 'o-')\n",
    "    \n",
    "    for tol in tolerances:\n",
    "        plt.axhline(y=tol, linestyle='--', alpha=0.5, label=f'Tolerance = {tol:.1e}')\n",
    "    \n",
    "    plt.title(\"Singular Values of Ill-conditioned Matrix\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value (log scale)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return A, b, x_true, x_svd, A_ill, b_ill, solutions, residuals\n",
    "\n",
    "# Demonstrate pseudoinverse and least squares\n",
    "A_ls, b_ls, x_true_ls, x_svd_ls, A_ill, b_ill, solutions_ill, residuals_ill = demonstrate_pseudoinverse()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7edc8d0",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored a wide range of applications of Singular Value Decomposition (SVD) across different domains:\n",
    "\n",
    "1. **Image Compression**: We showed how SVD enables efficient image compression by keeping only the most significant singular values and their corresponding vectors.\n",
    "\n",
    "2. **Principal Component Analysis (PCA)**: We demonstrated how SVD forms the mathematical foundation for PCA, a widely-used dimensionality reduction technique.\n",
    "\n",
    "3. **Latent Semantic Analysis (LSA)**: We explored how SVD can uncover hidden semantic relationships in text data, enabling document similarity analysis and topic modeling.\n",
    "\n",
    "4. **Recommendation Systems**: We implemented a collaborative filtering algorithm using SVD to predict user preferences and generate recommendations.\n",
    "\n",
    "5. **Signal Processing and Denoising**: We applied SVD to denoise signals by separating signal components from noise.\n",
    "\n",
    "6. **Pseudoinverse and Least Squares**: We showed how SVD provides a robust way to compute the pseudoinverse of a matrix, which is useful for solving linear systems and least squares problems.\n",
    "\n",
    "These applications highlight the versatility and power of SVD as a fundamental tool in data analysis, machine learning, signal processing, and many other fields. The ability to decompose a matrix into its principal components makes SVD a cornerstone technique in computational linear algebra.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
