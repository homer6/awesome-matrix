{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0f226d5a",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/09_singular_value_decomposition`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/09_singular_value_decomposition/02_svd_algorithms.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a968b62",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition: Algorithms\n",
    "\n",
    "This notebook explores different algorithms for computing the Singular Value Decomposition (SVD) and their implementation details. We'll examine the tradeoffs between these methods in terms of computational efficiency, numerical stability, and accuracy.\n",
    "\n",
    "We'll focus on:\n",
    "\n",
    "1. **Power Method for SVD**\n",
    "2. **Bidiagonalization Method**\n",
    "3. **Jacobi SVD Algorithm**\n",
    "4. **Randomized SVD**\n",
    "5. **Block and Parallelized SVD**\n",
    "\n",
    "Each algorithm has specific strengths and weaknesses that make it suitable for different types of matrices and applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2999d6d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.linalg\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# For better looking plots\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom colormap (light blue to dark blue)\n",
    "colors = [(0.95, 0.95, 1), (0.0, 0.2, 0.6)]\n",
    "blue_cmap = LinearSegmentedColormap.from_list('CustomBlue', colors, N=100)\n",
    "\n",
    "# Helper functions for visualization\n",
    "def plot_matrix(matrix, title=\"Matrix\", annotate=True, cmap=blue_cmap):\n",
    "    \"\"\"Plot a matrix as a heatmap with annotations.\"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix_np = matrix.numpy()\n",
    "    else:\n",
    "        matrix_np = matrix\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(matrix_np, annot=annotate, fmt=\".2f\", cmap=cmap, \n",
    "                    linewidths=1, cbar=True)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add row and column indices\n",
    "    ax.set_xticks(np.arange(matrix_np.shape[1]) + 0.5)\n",
    "    ax.set_yticks(np.arange(matrix_np.shape[0]) + 0.5)\n",
    "    ax.set_xticklabels([f\"Col {i+1}\" for i in range(matrix_np.shape[1])])\n",
    "    ax.set_yticklabels([f\"Row {i+1}\" for i in range(matrix_np.shape[0])])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Functions to create example matrices\n",
    "def create_example_matrix(m=4, n=3, method=\"random\"):\n",
    "    \"\"\"Create a matrix for SVD demonstration.\"\"\"\n",
    "    if method == \"random\":\n",
    "        # Create a random matrix\n",
    "        A = torch.rand(m, n) * 10 - 5  # Values between -5 and 5\n",
    "    elif method == \"simple\":\n",
    "        # Simple predefined matrix for clear demonstration\n",
    "        if m == 3 and n == 2:\n",
    "            A = torch.tensor([\n",
    "                [4.0, 0.0],\n",
    "                [3.0, -5.0],\n",
    "                [0.0, 4.0]\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(f\"Simple method doesn't support dimensions {m}x{n}\")\n",
    "    elif method == \"image\":\n",
    "        # Create a simple image-like matrix with structure\n",
    "        A = torch.zeros(m, n)\n",
    "        # Add a gradient pattern\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                A[i, j] = i + j * 0.5\n",
    "        # Add some noise\n",
    "        A = A + torch.randn(m, n) * 0.2\n",
    "    elif method == \"lowrank\":\n",
    "        # Create a low-rank matrix\n",
    "        rank = min(min(m, n) - 1, 5)  # low rank, but not too low\n",
    "        # Create factors\n",
    "        U = torch.randn(m, rank)\n",
    "        V = torch.randn(n, rank)\n",
    "        # Create matrix as a sum of rank-1 matrices\n",
    "        A = U @ V.T\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Function to evaluate SVD quality\n",
    "def evaluate_svd(A, U, S, V):\n",
    "    \"\"\"\n",
    "    Evaluate the quality of an SVD decomposition.\n",
    "    \n",
    "    Args:\n",
    "        A: Original matrix\n",
    "        U: Left singular vectors\n",
    "        S: Singular values (or diagonal matrix)\n",
    "        V: Right singular vectors\n",
    "        \n",
    "    Returns:\n",
    "        Dict with reconstruction error and orthogonality measures\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    # Handle different forms of S (vector or matrix)\n",
    "    if len(S.shape) == 1:\n",
    "        # S is a vector, create a diagonal matrix\n",
    "        m, n = A.shape\n",
    "        S_matrix = torch.zeros(m, n, dtype=A.dtype)\n",
    "        for i in range(min(m, n)):\n",
    "            if i < len(S):\n",
    "                S_matrix[i, i] = S[i]\n",
    "    else:\n",
    "        # S is already a matrix\n",
    "        S_matrix = S\n",
    "    \n",
    "    # Handle different forms of V (V or V^T)\n",
    "    if V.shape[0] == A.shape[1]:\n",
    "        # V is provided (columns are eigenvectors)\n",
    "        V_matrix = V\n",
    "    else:\n",
    "        # V^T is provided\n",
    "        V_matrix = V.T\n",
    "    \n",
    "    # Reconstruction error\n",
    "    A_reconstructed = U @ S_matrix @ V_matrix.T\n",
    "    reconstruction_error = torch.norm(A - A_reconstructed).item() / torch.norm(A).item()\n",
    "    \n",
    "    # Orthogonality of U\n",
    "    U_orthogonality_error = torch.norm(U.T @ U - torch.eye(U.shape[1], dtype=U.dtype)).item()\n",
    "    \n",
    "    # Orthogonality of V\n",
    "    V_orthogonality_error = torch.norm(V_matrix.T @ V_matrix - torch.eye(V_matrix.shape[1], dtype=V_matrix.dtype)).item()\n",
    "    \n",
    "    return {\n",
    "        'reconstruction_error': reconstruction_error,\n",
    "        'U_orthogonality_error': U_orthogonality_error,\n",
    "        'V_orthogonality_error': V_orthogonality_error\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89375cf4",
   "metadata": {},
   "source": [
    "## 1. Power Method for SVD\n",
    "\n",
    "The power method is a simple iterative approach for finding the largest singular value and its corresponding singular vectors. The basic idea is:\n",
    "\n",
    "1. Start with a random vector $v$\n",
    "2. Repeatedly apply $A^TA$ to $v$ (i.e., compute $(A^TA)^k v$ for increasing $k$)\n",
    "3. The vector will converge to the eigenvector corresponding to the largest eigenvalue of $A^TA$\n",
    "4. This gives the right singular vector corresponding to the largest singular value\n",
    "\n",
    "By deflation (subtracting the component along the found singular vectors), we can find subsequent singular values and vectors.\n",
    "\n",
    "Let's implement the power method for SVD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034bf143",
   "metadata": {},
   "outputs": [],
   "source": [
    "def power_method_svd(A, k=None, max_iter=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Compute SVD using the power method.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        k: Number of singular values/vectors to compute (default: min(m,n))\n",
    "        max_iter: Maximum number of iterations for each singular value\n",
    "        tol: Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors\n",
    "        S: Singular values\n",
    "        V: Right singular vectors\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    if k is None:\n",
    "        k = min(m, n)\n",
    "    \n",
    "    # Initialize output matrices\n",
    "    U = torch.zeros((m, k), dtype=A.dtype)\n",
    "    S = torch.zeros(k, dtype=A.dtype)\n",
    "    V = torch.zeros((n, k), dtype=A.dtype)\n",
    "    \n",
    "    # Make a copy of A for deflation\n",
    "    A_deflated = A.clone()\n",
    "    \n",
    "    # Compute SVD one component at a time\n",
    "    for i in range(k):\n",
    "        # Initialize a random vector\n",
    "        v = torch.randn(n, dtype=A.dtype)\n",
    "        v = v / torch.norm(v)\n",
    "        \n",
    "        # Apply power iteration\n",
    "        for _ in range(max_iter):\n",
    "            # v = (A^T A) v\n",
    "            v_new = A_deflated.T @ (A_deflated @ v)\n",
    "            \n",
    "            # Normalize\n",
    "            v_new_norm = torch.norm(v_new)\n",
    "            if v_new_norm < tol:\n",
    "                # If we get a zero vector, the remaining singular values are zero\n",
    "                break\n",
    "                \n",
    "            v_new = v_new / v_new_norm\n",
    "            \n",
    "            # Check for convergence\n",
    "            if torch.norm(v_new - v) < tol or torch.norm(v_new + v) < tol:\n",
    "                break\n",
    "                \n",
    "            v = v_new\n",
    "        \n",
    "        # Compute singular value and left singular vector\n",
    "        u = A_deflated @ v\n",
    "        s = torch.norm(u)\n",
    "        \n",
    "        if s > tol:\n",
    "            u = u / s\n",
    "            \n",
    "            # Store results\n",
    "            U[:, i] = u\n",
    "            S[i] = s\n",
    "            V[:, i] = v\n",
    "            \n",
    "            # Deflate the matrix: A_next = A - s*u*v^T\n",
    "            A_deflated = A_deflated - s * torch.outer(u, v)\n",
    "        else:\n",
    "            # Zero (or very small) singular value\n",
    "            break\n",
    "    \n",
    "    # Return only the non-zero components\n",
    "    non_zero = torch.sum(S > tol).item()\n",
    "    return U[:, :non_zero], S[:non_zero], V[:, :non_zero]\n",
    "\n",
    "def demonstrate_power_method():\n",
    "    \"\"\"Demonstrate the power method for SVD.\"\"\"\n",
    "    # Create a simple matrix\n",
    "    A = torch.tensor([\n",
    "        [3.0, 2.0, 2.0],\n",
    "        [2.0, 3.0, -2.0]\n",
    "    ])\n",
    "    \n",
    "    # Display the original matrix\n",
    "    plot_matrix(A, \"Original Matrix\")\n",
    "    \n",
    "    # Compute SVD using power method\n",
    "    U_power, S_power, V_power = power_method_svd(A)\n",
    "    \n",
    "    # For comparison, use torch.linalg.svd\n",
    "    U_torch, S_torch, V_torch = torch.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Display the results\n",
    "    print(\"Power Method SVD:\")\n",
    "    print(\"Left Singular Vectors (U):\")\n",
    "    print(U_power.numpy())\n",
    "    print(\"\\nSingular Values (S):\")\n",
    "    print(S_power.numpy())\n",
    "    print(\"\\nRight Singular Vectors (V):\")\n",
    "    print(V_power.numpy())\n",
    "    \n",
    "    print(\"\\nPyTorch SVD:\")\n",
    "    print(\"Left Singular Vectors (U):\")\n",
    "    print(U_torch.numpy())\n",
    "    print(\"\\nSingular Values (S):\")\n",
    "    print(S_torch.numpy())\n",
    "    print(\"\\nRight Singular Vectors (V):\")\n",
    "    print(V_torch.T.numpy())\n",
    "    \n",
    "    # Create matrices for visualization\n",
    "    m, n = A.shape\n",
    "    S_power_matrix = torch.zeros(m, n)\n",
    "    for i in range(len(S_power)):\n",
    "        S_power_matrix[i, i] = S_power[i]\n",
    "    \n",
    "    # Visualize the power method SVD\n",
    "    plot_matrix(U_power, \"Left Singular Vectors (U) - Power Method\")\n",
    "    plot_matrix(S_power_matrix, \"Singular Values (Σ) - Power Method\")\n",
    "    plot_matrix(V_power, \"Right Singular Vectors (V) - Power Method\")\n",
    "    \n",
    "    # Reconstruct the matrix\n",
    "    A_reconstructed = U_power @ S_power_matrix @ V_power.T\n",
    "    plot_matrix(A_reconstructed, \"Reconstructed Matrix - Power Method\")\n",
    "    \n",
    "    # Calculate errors\n",
    "    power_evaluation = evaluate_svd(A, U_power, S_power, V_power)\n",
    "    torch_evaluation = evaluate_svd(A, U_torch, S_torch, V_torch)\n",
    "    \n",
    "    print(\"\\nReconstruction Error:\")\n",
    "    print(f\"Power Method: {power_evaluation['reconstruction_error']:.2e}\")\n",
    "    print(f\"PyTorch SVD: {torch_evaluation['reconstruction_error']:.2e}\")\n",
    "    \n",
    "    print(\"\\nU Orthogonality Error:\")\n",
    "    print(f\"Power Method: {power_evaluation['U_orthogonality_error']:.2e}\")\n",
    "    print(f\"PyTorch SVD: {torch_evaluation['U_orthogonality_error']:.2e}\")\n",
    "    \n",
    "    print(\"\\nV Orthogonality Error:\")\n",
    "    print(f\"Power Method: {power_evaluation['V_orthogonality_error']:.2e}\")\n",
    "    print(f\"PyTorch SVD: {torch_evaluation['V_orthogonality_error']:.2e}\")\n",
    "    \n",
    "    # Visualize the convergence of power method\n",
    "    def visualize_power_convergence(A, max_iter=20):\n",
    "        \"\"\"Visualize how the power method converges to singular values.\"\"\"\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # Initialize a random vector\n",
    "        v = torch.randn(n, dtype=A.dtype)\n",
    "        v = v / torch.norm(v)\n",
    "        \n",
    "        # Keep track of approximations at each iteration\n",
    "        approximations = []\n",
    "        \n",
    "        # Apply power iteration\n",
    "        for i in range(max_iter):\n",
    "            # v = (A^T A) v\n",
    "            v_new = A.T @ (A @ v)\n",
    "            \n",
    "            # Compute the Rayleigh quotient (approximation of largest eigenvalue)\n",
    "            rayleigh = (v_new @ v) / (v @ v)\n",
    "            \n",
    "            # Normalize\n",
    "            v_new = v_new / torch.norm(v_new)\n",
    "            \n",
    "            # Compute current approximation of largest singular value\n",
    "            u = A @ v\n",
    "            sigma = torch.norm(u)\n",
    "            u = u / sigma\n",
    "            \n",
    "            # Store approximation\n",
    "            approximations.append(sigma.item())\n",
    "            \n",
    "            v = v_new\n",
    "        \n",
    "        # Get the true largest singular value\n",
    "        _, S_true, _ = torch.linalg.svd(A)\n",
    "        largest_sv = S_true[0].item()\n",
    "        \n",
    "        # Plot convergence\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.plot(range(1, max_iter+1), approximations, 'o-', label='Power Method Approximation')\n",
    "        plt.axhline(y=largest_sv, color='r', linestyle='--', label=f'True Value ({largest_sv:.4f})')\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Approximation of Largest Singular Value\")\n",
    "        plt.title(\"Convergence of Power Method\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        # Plot relative error\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        relative_errors = [abs(approx - largest_sv) / largest_sv for approx in approximations]\n",
    "        plt.semilogy(range(1, max_iter+1), relative_errors, 'o-')\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Relative Error\")\n",
    "        plt.title(\"Convergence Rate of Power Method\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.show()\n",
    "        \n",
    "        return approximations\n",
    "    \n",
    "    # Visualize convergence for our example matrix\n",
    "    approximations = visualize_power_convergence(A)\n",
    "    \n",
    "    return A, U_power, S_power, V_power, approximations\n",
    "\n",
    "# Demonstrate power method for SVD\n",
    "A_power, U_power, S_power, V_power, approximations_power = demonstrate_power_method()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da46f3db",
   "metadata": {},
   "source": [
    "### 1.1 Shifted Power Method for Convergence Acceleration\n",
    "\n",
    "The basic power method can be slow to converge, especially when the largest singular values are close in magnitude. The shifted power method can accelerate convergence by applying a shift:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7bf7fbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_power_method_svd(A, k=None, max_iter=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Compute SVD using the shifted power method for faster convergence.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        k: Number of singular values/vectors to compute (default: min(m,n))\n",
    "        max_iter: Maximum number of iterations for each singular value\n",
    "        tol: Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors\n",
    "        S: Singular values\n",
    "        V: Right singular vectors\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    if k is None:\n",
    "        k = min(m, n)\n",
    "    \n",
    "    # Initialize output matrices\n",
    "    U = torch.zeros((m, k), dtype=A.dtype)\n",
    "    S = torch.zeros(k, dtype=A.dtype)\n",
    "    V = torch.zeros((n, k), dtype=A.dtype)\n",
    "    \n",
    "    # Make a copy of A for deflation\n",
    "    A_deflated = A.clone()\n",
    "    \n",
    "    # Compute SVD one component at a time\n",
    "    for i in range(k):\n",
    "        # Estimate the largest singular value using a few iterations of the power method\n",
    "        v = torch.randn(n, dtype=A.dtype)\n",
    "        v = v / torch.norm(v)\n",
    "        \n",
    "        for _ in range(5):  # Just a few iterations to get an estimate\n",
    "            v = A_deflated.T @ (A_deflated @ v)\n",
    "            v = v / torch.norm(v)\n",
    "        \n",
    "        # Estimate the largest eigenvalue of A^T A\n",
    "        rayleigh = v @ (A_deflated.T @ (A_deflated @ v)) / (v @ v)\n",
    "        \n",
    "        # Use a shift slightly smaller than the estimated largest eigenvalue\n",
    "        shift = 0.95 * rayleigh\n",
    "        \n",
    "        # Initialize a random vector\n",
    "        v = torch.randn(n, dtype=A.dtype)\n",
    "        v = v / torch.norm(v)\n",
    "        \n",
    "        # Apply shifted power iteration\n",
    "        for _ in range(max_iter):\n",
    "            # v = (A^T A - shift*I)^-1 v\n",
    "            # This is equivalent to solving (A^T A - shift*I) x = v\n",
    "            # But for simplicity, we'll just use the shifted iteration without inversion\n",
    "            \n",
    "            v_new = A_deflated.T @ (A_deflated @ v) - shift * v\n",
    "            \n",
    "            # Normalize\n",
    "            v_new_norm = torch.norm(v_new)\n",
    "            if v_new_norm < tol:\n",
    "                break\n",
    "                \n",
    "            v_new = v_new / v_new_norm\n",
    "            \n",
    "            # Check for convergence\n",
    "            if torch.norm(v_new - v) < tol or torch.norm(v_new + v) < tol:\n",
    "                break\n",
    "                \n",
    "            v = v_new\n",
    "        \n",
    "        # Compute singular value and left singular vector\n",
    "        u = A_deflated @ v\n",
    "        s = torch.norm(u)\n",
    "        \n",
    "        if s > tol:\n",
    "            u = u / s\n",
    "            \n",
    "            # Store results\n",
    "            U[:, i] = u\n",
    "            S[i] = s\n",
    "            V[:, i] = v\n",
    "            \n",
    "            # Deflate the matrix: A_next = A - s*u*v^T\n",
    "            A_deflated = A_deflated - s * torch.outer(u, v)\n",
    "        else:\n",
    "            # Zero (or very small) singular value\n",
    "            break\n",
    "    \n",
    "    # Return only the non-zero components\n",
    "    non_zero = torch.sum(S > tol).item()\n",
    "    return U[:, :non_zero], S[:non_zero], V[:, :non_zero]\n",
    "\n",
    "def compare_power_methods():\n",
    "    \"\"\"Compare regular and shifted power methods.\"\"\"\n",
    "    # Create a matrix with close singular values\n",
    "    A = torch.tensor([\n",
    "        [3.0, 2.0, 1.0],\n",
    "        [2.0, 3.0, 1.0],\n",
    "        [1.0, 1.0, 3.0]\n",
    "    ])\n",
    "    \n",
    "    # Compute true singular values\n",
    "    _, S_true, _ = torch.linalg.svd(A)\n",
    "    \n",
    "    print(\"True Singular Values:\")\n",
    "    print(S_true.numpy())\n",
    "    \n",
    "    # Compare convergence rates for largest singular value\n",
    "    def compare_convergence(A, max_iter=30):\n",
    "        \"\"\"Compare convergence rates of regular and shifted power methods.\"\"\"\n",
    "        m, n = A.shape\n",
    "        \n",
    "        # Regular power method\n",
    "        v_regular = torch.randn(n, dtype=A.dtype)\n",
    "        v_regular = v_regular / torch.norm(v_regular)\n",
    "        regular_approximations = []\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            v_regular = A.T @ (A @ v_regular)\n",
    "            v_regular = v_regular / torch.norm(v_regular)\n",
    "            u_regular = A @ v_regular\n",
    "            sigma_regular = torch.norm(u_regular)\n",
    "            regular_approximations.append(sigma_regular.item())\n",
    "        \n",
    "        # Shifted power method\n",
    "        v_shifted = torch.randn(n, dtype=A.dtype)\n",
    "        v_shifted = v_shifted / torch.norm(v_shifted)\n",
    "        shifted_approximations = []\n",
    "        \n",
    "        # Estimate the largest eigenvalue\n",
    "        v_est = v_shifted.clone()\n",
    "        for _ in range(5):\n",
    "            v_est = A.T @ (A @ v_est)\n",
    "            v_est = v_est / torch.norm(v_est)\n",
    "        \n",
    "        rayleigh = v_est @ (A.T @ (A @ v_est)) / (v_est @ v_est)\n",
    "        shift = 0.95 * rayleigh\n",
    "        \n",
    "        for i in range(max_iter):\n",
    "            v_shifted = A.T @ (A @ v_shifted) - shift * v_shifted\n",
    "            v_shifted = v_shifted / torch.norm(v_shifted)\n",
    "            u_shifted = A @ v_shifted\n",
    "            sigma_shifted = torch.norm(u_shifted)\n",
    "            shifted_approximations.append(sigma_shifted.item())\n",
    "        \n",
    "        # Plot convergence comparison\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.plot(range(1, max_iter+1), regular_approximations, 'o-', label='Regular Power Method')\n",
    "        plt.plot(range(1, max_iter+1), shifted_approximations, 's-', label='Shifted Power Method')\n",
    "        plt.axhline(y=S_true[0].item(), color='r', linestyle='--', label=f'True Value ({S_true[0].item():.4f})')\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Approximation of Largest Singular Value\")\n",
    "        plt.title(\"Convergence Comparison\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        regular_errors = [abs(approx - S_true[0].item()) / S_true[0].item() for approx in regular_approximations]\n",
    "        shifted_errors = [abs(approx - S_true[0].item()) / S_true[0].item() for approx in shifted_approximations]\n",
    "        \n",
    "        plt.semilogy(range(1, max_iter+1), regular_errors, 'o-', label='Regular Power Method')\n",
    "        plt.semilogy(range(1, max_iter+1), shifted_errors, 's-', label='Shifted Power Method')\n",
    "        plt.xlabel(\"Iteration\")\n",
    "        plt.ylabel(\"Relative Error (log scale)\")\n",
    "        plt.title(\"Convergence Rate Comparison\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Return the number of iterations needed to reach a given accuracy\n",
    "        target_error = 1e-6\n",
    "        regular_iter = next((i for i, error in enumerate(regular_errors) if error < target_error), max_iter)\n",
    "        shifted_iter = next((i for i, error in enumerate(shifted_errors) if error < target_error), max_iter)\n",
    "        \n",
    "        print(f\"Iterations to reach {target_error:.1e} relative error:\")\n",
    "        print(f\"Regular Power Method: {regular_iter + 1}\")\n",
    "        print(f\"Shifted Power Method: {shifted_iter + 1}\")\n",
    "        \n",
    "        return regular_approximations, shifted_approximations\n",
    "    \n",
    "    # Compare convergence rates\n",
    "    regular_approx, shifted_approx = compare_convergence(A)\n",
    "    \n",
    "    # Compare full SVD computation timing\n",
    "    def time_methods(A, k=None):\n",
    "        \"\"\"Compare timing of regular and shifted power methods.\"\"\"\n",
    "        # Time regular power method\n",
    "        start_time = time.time()\n",
    "        U_regular, S_regular, V_regular = power_method_svd(A, k=k)\n",
    "        regular_time = time.time() - start_time\n",
    "        \n",
    "        # Time shifted power method\n",
    "        start_time = time.time()\n",
    "        U_shifted, S_shifted, V_shifted = shifted_power_method_svd(A, k=k)\n",
    "        shifted_time = time.time() - start_time\n",
    "        \n",
    "        # Time PyTorch SVD\n",
    "        start_time = time.time()\n",
    "        U_torch, S_torch, V_torch = torch.linalg.svd(A)\n",
    "        torch_time = time.time() - start_time\n",
    "        \n",
    "        # Calculate errors\n",
    "        regular_evaluation = evaluate_svd(A, U_regular, S_regular, V_regular)\n",
    "        shifted_evaluation = evaluate_svd(A, U_shifted, S_shifted, V_shifted)\n",
    "        torch_evaluation = evaluate_svd(A, U_torch, S_torch, V_torch)\n",
    "        \n",
    "        print(\"\\nTiming Comparison:\")\n",
    "        print(f\"Regular Power Method: {regular_time:.6f} seconds\")\n",
    "        print(f\"Shifted Power Method: {shifted_time:.6f} seconds\")\n",
    "        print(f\"PyTorch SVD: {torch_time:.6f} seconds\")\n",
    "        \n",
    "        print(\"\\nReconstruction Error:\")\n",
    "        print(f\"Regular Power Method: {regular_evaluation['reconstruction_error']:.2e}\")\n",
    "        print(f\"Shifted Power Method: {shifted_evaluation['reconstruction_error']:.2e}\")\n",
    "        print(f\"PyTorch SVD: {torch_evaluation['reconstruction_error']:.2e}\")\n",
    "        \n",
    "        # Plot results\n",
    "        methods = ['Regular Power', 'Shifted Power', 'PyTorch SVD']\n",
    "        times = [regular_time, shifted_time, torch_time]\n",
    "        errors = [regular_evaluation['reconstruction_error'], \n",
    "                  shifted_evaluation['reconstruction_error'],\n",
    "                  torch_evaluation['reconstruction_error']]\n",
    "        \n",
    "        plt.figure(figsize=(12, 5))\n",
    "        \n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.bar(methods, times)\n",
    "        plt.ylabel(\"Time (seconds)\")\n",
    "        plt.title(\"Computation Time\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.bar(methods, errors)\n",
    "        plt.ylabel(\"Reconstruction Error\")\n",
    "        plt.title(\"Accuracy\")\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return {\n",
    "            'methods': methods,\n",
    "            'times': times,\n",
    "            'errors': errors,\n",
    "            'results': {\n",
    "                'regular': (U_regular, S_regular, V_regular),\n",
    "                'shifted': (U_shifted, S_shifted, V_shifted),\n",
    "                'torch': (U_torch, S_torch, V_torch)\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    # Compare timing and accuracy\n",
    "    comparison_results = time_methods(A)\n",
    "    \n",
    "    return A, comparison_results\n",
    "\n",
    "# Compare regular and shifted power methods\n",
    "A_shifted, comparison_shifted = compare_power_methods()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56554b80",
   "metadata": {},
   "source": [
    "## 2. Bidiagonalization Method\n",
    "\n",
    "The bidiagonalization method, also known as Golub-Kahan-Lanczos bidiagonalization, is a more efficient approach for computing SVD, especially for large sparse matrices. It first reduces the matrix to a bidiagonal form, and then computes the SVD of the bidiagonal matrix.\n",
    "\n",
    "The basic algorithm has two phases:\n",
    "1. Reduce $A$ to a bidiagonal form $B$ using orthogonal transformations: $A = UBV^T$\n",
    "2. Compute the SVD of $B$: $B = \\hat{U} \\Sigma \\hat{V}^T$\n",
    "3. Combine the transformations: $A = (U\\hat{U}) \\Sigma (\\hat{V}^T V^T)$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b03b658",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bidiagonalization(A, full_matrices=True):\n",
    "    \"\"\"\n",
    "    Reduce a matrix to bidiagonal form using Householder reflections.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        full_matrices: Whether to return full or economy-sized U and V\n",
    "        \n",
    "    Returns:\n",
    "        U: Left orthogonal matrix\n",
    "        B: Bidiagonal matrix\n",
    "        V: Right orthogonal matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    k = min(m, n)\n",
    "    \n",
    "    # Initialize U, B, and V\n",
    "    if full_matrices:\n",
    "        U = torch.eye(m, dtype=A.dtype)\n",
    "        V = torch.eye(n, dtype=A.dtype)\n",
    "    else:\n",
    "        U = torch.eye(m, k, dtype=A.dtype)\n",
    "        V = torch.eye(n, k, dtype=A.dtype)\n",
    "    \n",
    "    # Create a copy of A to work with\n",
    "    B = A.clone()\n",
    "    \n",
    "    # Bidiagonalization process\n",
    "    for i in range(k):\n",
    "        # Apply Householder reflection to zero elements below the diagonal\n",
    "        if i < m-1:\n",
    "            # Extract the column vector\n",
    "            x = B[i:, i]\n",
    "            \n",
    "            # Construct the Householder vector\n",
    "            alpha = torch.norm(x)\n",
    "            if x[0] < 0:\n",
    "                alpha = -alpha\n",
    "            \n",
    "            # Handle the case where x is already a multiple of the first unit vector\n",
    "            if alpha == 0 or (x.shape[0] == 1 and x[0] == alpha):\n",
    "                continue\n",
    "                \n",
    "            u = x.clone()\n",
    "            u[0] = u[0] + alpha\n",
    "            u = u / torch.norm(u)\n",
    "            \n",
    "            # Apply the Householder reflection to B\n",
    "            B[i:, i:] = B[i:, i:] - 2.0 * torch.outer(u, (u @ B[i:, i:]))\n",
    "            \n",
    "            # Update U\n",
    "            if full_matrices:\n",
    "                U[:, i:] = U[:, i:] - 2.0 * torch.outer(U[:, i:] @ u, u)\n",
    "            else:\n",
    "                U[:, i:k] = U[:, i:k] - 2.0 * torch.outer(U[:, i:k] @ u, u[:k-i])\n",
    "        \n",
    "        # Apply Householder reflection to zero elements to the right of the superdiagonal\n",
    "        if i < k-1 and i < n-1:\n",
    "            # Extract the row vector\n",
    "            x = B[i, i+1:]\n",
    "            \n",
    "            # Construct the Householder vector\n",
    "            alpha = torch.norm(x)\n",
    "            if x[0] < 0:\n",
    "                alpha = -alpha\n",
    "            \n",
    "            # Handle the case where x is already a multiple of the first unit vector\n",
    "            if alpha == 0 or (x.shape[0] == 1 and x[0] == alpha):\n",
    "                continue\n",
    "                \n",
    "            u = x.clone()\n",
    "            u[0] = u[0] + alpha\n",
    "            u = u / torch.norm(u)\n",
    "            \n",
    "            # Apply the Householder reflection to B\n",
    "            B[i:, i+1:] = B[i:, i+1:] - 2.0 * torch.outer((B[i:, i+1:] @ u), u)\n",
    "            \n",
    "            # Update V\n",
    "            if full_matrices:\n",
    "                V[:, i+1:] = V[:, i+1:] - 2.0 * torch.outer(V[:, i+1:] @ u, u)\n",
    "            else:\n",
    "                V[:, (i+1):k] = V[:, (i+1):k] - 2.0 * torch.outer(V[:, (i+1):k] @ u, u[:k-(i+1)])\n",
    "    \n",
    "    return U, B, V\n",
    "\n",
    "def svd_of_bidiagonal(B, tol=1e-8, max_iter=100):\n",
    "    \"\"\"\n",
    "    Compute the SVD of a bidiagonal matrix using the QR algorithm.\n",
    "    \n",
    "    Args:\n",
    "        B: Bidiagonal matrix\n",
    "        tol: Convergence tolerance\n",
    "        max_iter: Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors\n",
    "        S: Singular values\n",
    "        V: Right singular vectors\n",
    "    \"\"\"\n",
    "    if isinstance(B, np.ndarray):\n",
    "        B = torch.tensor(B, dtype=torch.float64)\n",
    "    \n",
    "    m, n = B.shape\n",
    "    k = min(m, n)\n",
    "    \n",
    "    # Initialize U and V as identity matrices\n",
    "    U = torch.eye(m, dtype=B.dtype)\n",
    "    V = torch.eye(n, dtype=B.dtype)\n",
    "    \n",
    "    # Make a copy of B to work with\n",
    "    B_work = B.clone()\n",
    "    \n",
    "    # Apply QR algorithm to B^T B to find its eigenvalues and eigenvectors\n",
    "    # This is a simplified approach; in practice, specialized algorithms for\n",
    "    # bidiagonal SVD would be used (like the implicit QR algorithm with shifts)\n",
    "    \n",
    "    # For simplicity, we'll use a direct eigendecomposition here\n",
    "    BTB = B_work.T @ B_work\n",
    "    eigvals, eigvecs = torch.linalg.eigh(BTB)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices = torch.argsort(eigvals, descending=True)\n",
    "    eigvals = eigvals[sorted_indices]\n",
    "    eigvecs = eigvecs[:, sorted_indices]\n",
    "    \n",
    "    # Singular values are square roots of eigenvalues of B^T B\n",
    "    S = torch.sqrt(torch.clamp(eigvals, min=0))\n",
    "    \n",
    "    # Right singular vectors are eigenvectors of B^T B\n",
    "    V = eigvecs\n",
    "    \n",
    "    # Compute left singular vectors: u_i = (1/sigma_i) * B * v_i\n",
    "    U = torch.zeros((m, k), dtype=B.dtype)\n",
    "    for i in range(k):\n",
    "        if S[i] > tol:\n",
    "            U[:, i] = (B_work @ V[:, i]) / S[i]\n",
    "        else:\n",
    "            # For zero singular values, use an arbitrary orthogonal vector\n",
    "            if i == 0:\n",
    "                U[:, i] = torch.zeros(m, dtype=B.dtype)\n",
    "                U[0, i] = 1.0\n",
    "            else:\n",
    "                # Make orthogonal to the previous vectors\n",
    "                u = torch.randn(m, dtype=B.dtype)\n",
    "                for j in range(i):\n",
    "                    u = u - torch.dot(u, U[:, j]) * U[:, j]\n",
    "                U[:, i] = u / torch.norm(u)\n",
    "    \n",
    "    return U, S, V\n",
    "\n",
    "def bidiagonal_svd(A, full_matrices=True):\n",
    "    \"\"\"\n",
    "    Compute SVD using bidiagonalization followed by SVD of the bidiagonal matrix.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        full_matrices: Whether to return full or economy-sized U and V\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors\n",
    "        S: Singular values\n",
    "        V: Right singular vectors\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    # Step 1: Reduce A to bidiagonal form\n",
    "    U_bidiag, B, V_bidiag = bidiagonalization(A, full_matrices=full_matrices)\n",
    "    \n",
    "    # Step 2: Compute SVD of the bidiagonal matrix\n",
    "    U_svd, S, V_svd = svd_of_bidiagonal(B)\n",
    "    \n",
    "    # Step 3: Combine the transformations\n",
    "    U = U_bidiag @ U_svd\n",
    "    V = V_bidiag @ V_svd\n",
    "    \n",
    "    return U, S, V\n",
    "\n",
    "def demonstrate_bidiagonalization():\n",
    "    \"\"\"Demonstrate the bidiagonalization method for SVD.\"\"\"\n",
    "    # Create a small matrix\n",
    "    A = torch.tensor([\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [4.0, 5.0, 6.0],\n",
    "        [7.0, 8.0, 9.0],\n",
    "        [10.0, 11.0, 12.0]\n",
    "    ])\n",
    "    \n",
    "    # Display the original matrix\n",
    "    plot_matrix(A, \"Original Matrix\")\n",
    "    \n",
    "    # Step 1: Reduce to bidiagonal form\n",
    "    U_bidiag, B, V_bidiag = bidiagonalization(A)\n",
    "    \n",
    "    # Display the bidiagonal matrix\n",
    "    plot_matrix(B, \"Bidiagonal Matrix B\")\n",
    "    \n",
    "    # Verify that A = U_bidiag @ B @ V_bidiag.T\n",
    "    A_reconstructed = U_bidiag @ B @ V_bidiag.T\n",
    "    plot_matrix(A_reconstructed, \"Reconstructed Matrix (U_bidiag @ B @ V_bidiag^T)\")\n",
    "    \n",
    "    # Check reconstruction error\n",
    "    recon_error = torch.norm(A - A_reconstructed).item() / torch.norm(A).item()\n",
    "    print(f\"Bidiagonalization reconstruction error: {recon_error:.2e}\")\n",
    "    \n",
    "    # Step 2: Compute SVD of the bidiagonal matrix\n",
    "    U_svd, S, V_svd = svd_of_bidiagonal(B)\n",
    "    \n",
    "    # Combine the transformations\n",
    "    U = U_bidiag @ U_svd\n",
    "    V = V_bidiag @ V_svd\n",
    "    \n",
    "    # Create a diagonal matrix from S\n",
    "    m, n = A.shape\n",
    "    S_matrix = torch.zeros(m, n)\n",
    "    for i in range(min(m, n)):\n",
    "        if i < len(S):\n",
    "            S_matrix[i, i] = S[i]\n",
    "    \n",
    "    # Display the SVD components\n",
    "    plot_matrix(U, \"Left Singular Vectors (U)\")\n",
    "    plot_matrix(S_matrix, \"Singular Values (Σ)\")\n",
    "    plot_matrix(V, \"Right Singular Vectors (V)\")\n",
    "    \n",
    "    # Reconstruct the matrix from SVD\n",
    "    A_svd = U @ S_matrix @ V.T\n",
    "    plot_matrix(A_svd, \"Reconstructed Matrix (U @ Σ @ V^T)\")\n",
    "    \n",
    "    # Check reconstruction error\n",
    "    svd_error = torch.norm(A - A_svd).item() / torch.norm(A).item()\n",
    "    print(f\"SVD reconstruction error: {svd_error:.2e}\")\n",
    "    \n",
    "    # For comparison, use torch.linalg.svd\n",
    "    U_torch, S_torch, V_torch = torch.linalg.svd(A, full_matrices=True)\n",
    "    \n",
    "    # Create a diagonal matrix from S_torch\n",
    "    S_torch_matrix = torch.zeros(m, n)\n",
    "    for i in range(min(m, n)):\n",
    "        S_torch_matrix[i, i] = S_torch[i]\n",
    "    \n",
    "    # Reconstruct using PyTorch SVD\n",
    "    A_torch = U_torch @ S_torch_matrix @ V_torch\n",
    "    torch_error = torch.norm(A - A_torch).item() / torch.norm(A).item()\n",
    "    \n",
    "    print(f\"PyTorch SVD reconstruction error: {torch_error:.2e}\")\n",
    "    \n",
    "    # Compare singular values\n",
    "    print(\"\\nSingular Values:\")\n",
    "    print(\"Bidiagonal SVD:\", S.numpy())\n",
    "    print(\"PyTorch SVD:\", S_torch.numpy())\n",
    "    \n",
    "    return A, U, S, V, B\n",
    "\n",
    "# Demonstrate bidiagonalization method\n",
    "A_bidiag, U_bidiag, S_bidiag, V_bidiag, B_bidiag = demonstrate_bidiagonalization()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "171d0171",
   "metadata": {},
   "source": [
    "## 3. Jacobi SVD Algorithm\n",
    "\n",
    "The Jacobi SVD algorithm is an iterative method that directly computes the SVD by applying a sequence of plane rotations (Givens rotations) to diagonalize the matrix. Unlike the previous methods, it doesn't reduce the matrix to a bidiagonal form first.\n",
    "\n",
    "The algorithm focuses on eliminating off-diagonal elements of $A^TA$ by iteratively applying rotations to both rows and columns:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8529d384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def jacobi_svd(A, tol=1e-8, max_iter=100):\n",
    "    \"\"\"\n",
    "    Compute SVD using the Jacobi method.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        tol: Convergence tolerance\n",
    "        max_iter: Maximum number of iterations\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors\n",
    "        S: Singular values\n",
    "        V: Right singular vectors\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    # Initialize U and V as identity matrices\n",
    "    U = torch.eye(m, dtype=A.dtype)\n",
    "    V = torch.eye(n, dtype=A.dtype)\n",
    "    \n",
    "    # Make a copy of A to work with\n",
    "    B = A.clone()\n",
    "    \n",
    "    # Compute the squared Frobenius norm of the off-diagonal elements of B^T B\n",
    "    BtB = B.T @ B\n",
    "    off_diag_norm = torch.sum(BtB**2) - torch.sum(torch.diag(BtB)**2)\n",
    "    \n",
    "    # Iterate until convergence\n",
    "    num_iter = 0\n",
    "    while off_diag_norm > tol and num_iter < max_iter:\n",
    "        # Find the largest off-diagonal element of B^T B\n",
    "        p, q = 0, 1  # Default indices\n",
    "        max_val = 0.0\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i+1, n):\n",
    "                if abs(BtB[i, j]) > max_val:\n",
    "                    max_val = abs(BtB[i, j])\n",
    "                    p, q = i, j\n",
    "        \n",
    "        # Compute the Jacobi rotation parameters\n",
    "        if max_val < tol:\n",
    "            break\n",
    "        \n",
    "        # Compute the Jacobi rotation\n",
    "        alpha = BtB[p, p]\n",
    "        beta = BtB[q, q]\n",
    "        gamma = BtB[p, q]\n",
    "        \n",
    "        if abs(gamma) < tol:\n",
    "            continue\n",
    "        \n",
    "        # Compute the cosine and sine of the rotation angle\n",
    "        if abs(alpha - beta) < tol:\n",
    "            c = 1.0 / np.sqrt(2)\n",
    "            s = c\n",
    "        else:\n",
    "            zeta = (beta - alpha) / (2 * gamma)\n",
    "            t = 1.0 / (abs(zeta) + np.sqrt(1 + zeta**2))\n",
    "            if zeta < 0:\n",
    "                t = -t\n",
    "            c = 1.0 / np.sqrt(1 + t**2)\n",
    "            s = t * c\n",
    "        \n",
    "        # Create the Givens rotation matrix\n",
    "        G = torch.eye(n, dtype=A.dtype)\n",
    "        G[p, p] = c\n",
    "        G[p, q] = -s\n",
    "        G[q, p] = s\n",
    "        G[q, q] = c\n",
    "        \n",
    "        # Apply the rotation to B and update V\n",
    "        B = B @ G\n",
    "        V = V @ G\n",
    "        \n",
    "        # Recompute B^T B and the off-diagonal norm\n",
    "        BtB = B.T @ B\n",
    "        off_diag_norm = torch.sum(BtB**2) - torch.sum(torch.diag(BtB)**2)\n",
    "        \n",
    "        num_iter += 1\n",
    "    \n",
    "    # Compute the singular values and left singular vectors\n",
    "    S = torch.zeros(min(m, n), dtype=A.dtype)\n",
    "    for i in range(min(m, n)):\n",
    "        S[i] = torch.norm(B[:, i])\n",
    "        if S[i] > tol:\n",
    "            U[:, i] = B[:, i] / S[i]\n",
    "        else:\n",
    "            # For zero singular values, use an arbitrary orthogonal vector\n",
    "            if i == 0:\n",
    "                U[:, i] = torch.zeros(m, dtype=A.dtype)\n",
    "                U[0, i] = 1.0\n",
    "            else:\n",
    "                # Make orthogonal to the previous vectors\n",
    "                u = torch.randn(m, dtype=A.dtype)\n",
    "                for j in range(i):\n",
    "                    u = u - torch.dot(u, U[:, j]) * U[:, j]\n",
    "                U[:, i] = u / torch.norm(u)\n",
    "    \n",
    "    # Sort singular values and vectors in descending order\n",
    "    sorted_indices = torch.argsort(S, descending=True)\n",
    "    S = S[sorted_indices]\n",
    "    U = U[:, sorted_indices]\n",
    "    V = V[:, sorted_indices]\n",
    "    \n",
    "    return U, S, V\n",
    "\n",
    "def demonstrate_jacobi_svd():\n",
    "    \"\"\"Demonstrate the Jacobi SVD algorithm.\"\"\"\n",
    "    # Create a small matrix\n",
    "    A = torch.tensor([\n",
    "        [3.0, 2.0, 2.0],\n",
    "        [2.0, 3.0, -2.0]\n",
    "    ])\n",
    "    \n",
    "    # Display the original matrix\n",
    "    plot_matrix(A, \"Original Matrix\")\n",
    "    \n",
    "    # Compute SVD using Jacobi method\n",
    "    U_jacobi, S_jacobi, V_jacobi = jacobi_svd(A)\n",
    "    \n",
    "    # Create a diagonal matrix from S\n",
    "    m, n = A.shape\n",
    "    S_matrix = torch.zeros(m, n)\n",
    "    for i in range(min(m, n)):\n",
    "        S_matrix[i, i] = S_jacobi[i]\n",
    "    \n",
    "    # Display the SVD components\n",
    "    plot_matrix(U_jacobi, \"Left Singular Vectors (U) - Jacobi\")\n",
    "    plot_matrix(S_matrix, \"Singular Values (Σ) - Jacobi\")\n",
    "    plot_matrix(V_jacobi, \"Right Singular Vectors (V) - Jacobi\")\n",
    "    \n",
    "    # Reconstruct the matrix from SVD\n",
    "    A_jacobi = U_jacobi @ S_matrix @ V_jacobi.T\n",
    "    plot_matrix(A_jacobi, \"Reconstructed Matrix (U @ Σ @ V^T) - Jacobi\")\n",
    "    \n",
    "    # Check reconstruction error\n",
    "    jacobi_error = torch.norm(A - A_jacobi).item() / torch.norm(A).item()\n",
    "    print(f\"Jacobi SVD reconstruction error: {jacobi_error:.2e}\")\n",
    "    \n",
    "    # For comparison, use torch.linalg.svd\n",
    "    U_torch, S_torch, V_torch = torch.linalg.svd(A, full_matrices=True)\n",
    "    \n",
    "    # Create a diagonal matrix from S_torch\n",
    "    S_torch_matrix = torch.zeros(m, n)\n",
    "    for i in range(min(m, n)):\n",
    "        S_torch_matrix[i, i] = S_torch[i]\n",
    "    \n",
    "    # Reconstruct using PyTorch SVD\n",
    "    A_torch = U_torch @ S_torch_matrix @ V_torch\n",
    "    torch_error = torch.norm(A - A_torch).item() / torch.norm(A).item()\n",
    "    \n",
    "    print(f\"PyTorch SVD reconstruction error: {torch_error:.2e}\")\n",
    "    \n",
    "    # Compare singular values\n",
    "    print(\"\\nSingular Values:\")\n",
    "    print(\"Jacobi SVD:\", S_jacobi.numpy())\n",
    "    print(\"PyTorch SVD:\", S_torch.numpy())\n",
    "    \n",
    "    # Compare orthogonality\n",
    "    jacobi_eval = evaluate_svd(A, U_jacobi, S_jacobi, V_jacobi)\n",
    "    torch_eval = evaluate_svd(A, U_torch, S_torch, V_torch)\n",
    "    \n",
    "    print(\"\\nU Orthogonality Error:\")\n",
    "    print(f\"Jacobi SVD: {jacobi_eval['U_orthogonality_error']:.2e}\")\n",
    "    print(f\"PyTorch SVD: {torch_eval['U_orthogonality_error']:.2e}\")\n",
    "    \n",
    "    print(\"\\nV Orthogonality Error:\")\n",
    "    print(f\"Jacobi SVD: {jacobi_eval['V_orthogonality_error']:.2e}\")\n",
    "    print(f\"PyTorch SVD: {torch_eval['V_orthogonality_error']:.2e}\")\n",
    "    \n",
    "    return A, U_jacobi, S_jacobi, V_jacobi\n",
    "\n",
    "# Demonstrate Jacobi SVD\n",
    "A_jacobi, U_jacobi, S_jacobi, V_jacobi = demonstrate_jacobi_svd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "580d9a1b",
   "metadata": {},
   "source": [
    "## 4. Randomized SVD\n",
    "\n",
    "Randomized SVD is a modern approach that uses random projections to efficiently compute an approximate SVD, particularly for large matrices. The idea is to first identify a low-dimensional subspace that captures most of the action of the matrix, and then compute the SVD within this subspace.\n",
    "\n",
    "The basic algorithm is:\n",
    "1. Generate a random matrix $\\Omega$ with $r$ columns\n",
    "2. Form $Y = A\\Omega$ to sample the range of $A$\n",
    "3. Orthogonalize $Y$ to get a basis $Q$ for the range of $A$\n",
    "4. Form $B = Q^T A$, which is a smaller matrix\n",
    "5. Compute the SVD of $B = \\tilde{U} \\Sigma V^T$\n",
    "6. Compute $U = Q \\tilde{U}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac02099",
   "metadata": {},
   "outputs": [],
   "source": [
    "def randomized_svd(A, k, p=5, q=2):\n",
    "    \"\"\"\n",
    "    Compute randomized SVD.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        k: Target rank\n",
    "        p: Oversampling parameter\n",
    "        q: Number of power iterations\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors\n",
    "        S: Singular values\n",
    "        V: Right singular vectors\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    # Step 1: Generate a random matrix\n",
    "    r = min(k + p, min(m, n))  # Rank with oversampling\n",
    "    Omega = torch.randn(n, r, dtype=A.dtype)\n",
    "    \n",
    "    # Step 2: Sample the range of A\n",
    "    Y = A @ Omega\n",
    "    \n",
    "    # Optional: Power iterations to improve accuracy for low-rank approximation\n",
    "    for _ in range(q):\n",
    "        Y = A @ (A.T @ Y)\n",
    "    \n",
    "    # Step 3: Orthogonalize Y to get a basis Q\n",
    "    Q, _ = torch.linalg.qr(Y, mode='reduced')\n",
    "    \n",
    "    # Step 4: Form the smaller matrix B\n",
    "    B = Q.T @ A\n",
    "    \n",
    "    # Step 5: Compute SVD of B\n",
    "    Uhat, S, Vt = torch.linalg.svd(B, full_matrices=False)\n",
    "    \n",
    "    # Step 6: Compute U\n",
    "    U = Q @ Uhat\n",
    "    \n",
    "    # Keep only the top k components\n",
    "    return U[:, :k], S[:k], Vt[:k, :]\n",
    "\n",
    "def demonstrate_randomized_svd():\n",
    "    \"\"\"Demonstrate randomized SVD algorithm.\"\"\"\n",
    "    # Create a low-rank matrix (rank 3 in a 50x30 matrix)\n",
    "    m, n = 50, 30\n",
    "    rank = 3\n",
    "    \n",
    "    # Create factors\n",
    "    U_true = torch.nn.functional.normalize(torch.randn(m, rank), dim=0)\n",
    "    V_true = torch.nn.functional.normalize(torch.randn(n, rank), dim=0)\n",
    "    S_true = torch.tensor([10.0, 5.0, 1.0])  # Singular values\n",
    "    \n",
    "    # Create the matrix\n",
    "    A = U_true @ torch.diag(S_true) @ V_true.T\n",
    "    \n",
    "    # Add some noise\n",
    "    noise_level = 0.01\n",
    "    A_noisy = A + noise_level * torch.randn(m, n)\n",
    "    \n",
    "    # Compute full SVD using PyTorch\n",
    "    start_time = time.time()\n",
    "    U_full, S_full, V_full = torch.linalg.svd(A_noisy, full_matrices=False)\n",
    "    full_time = time.time() - start_time\n",
    "    \n",
    "    # Compute randomized SVD\n",
    "    start_time = time.time()\n",
    "    U_rand, S_rand, V_rand = randomized_svd(A_noisy, k=rank, p=5, q=2)\n",
    "    rand_time = time.time() - start_time\n",
    "    \n",
    "    # Compare timing and accuracy\n",
    "    print(\"Matrix size:\", A.shape)\n",
    "    print(f\"Full SVD time: {full_time:.6f} seconds\")\n",
    "    print(f\"Randomized SVD time: {rand_time:.6f} seconds\")\n",
    "    print(f\"Speedup: {full_time / rand_time:.2f}x\")\n",
    "    \n",
    "    # Convert V_rand from V^T to V\n",
    "    V_rand = V_rand.T\n",
    "    \n",
    "    # Evaluate accuracy\n",
    "    full_eval = evaluate_svd(A_noisy, U_full[:, :rank], S_full[:rank], V_full[:, :rank])\n",
    "    rand_eval = evaluate_svd(A_noisy, U_rand, S_rand, V_rand)\n",
    "    \n",
    "    print(\"\\nReconstruction Error (with noise):\")\n",
    "    print(f\"Full SVD (top {rank} components): {full_eval['reconstruction_error']:.2e}\")\n",
    "    print(f\"Randomized SVD (rank {rank}): {rand_eval['reconstruction_error']:.2e}\")\n",
    "    \n",
    "    # Compare to original low-rank matrix\n",
    "    full_to_true = torch.norm(A - U_full[:, :rank] @ torch.diag(S_full[:rank]) @ V_full[:rank, :]).item() / torch.norm(A).item()\n",
    "    rand_to_true = torch.norm(A - U_rand @ torch.diag(S_rand) @ V_rand.T).item() / torch.norm(A).item()\n",
    "    \n",
    "    print(\"\\nReconstruction Error (to true low-rank matrix):\")\n",
    "    print(f\"Full SVD (top {rank} components): {full_to_true:.2e}\")\n",
    "    print(f\"Randomized SVD (rank {rank}): {rand_to_true:.2e}\")\n",
    "    \n",
    "    # Compare singular values\n",
    "    print(\"\\nSingular Values:\")\n",
    "    print(\"True:\", S_true.numpy())\n",
    "    print(\"Full SVD:\", S_full[:rank].numpy())\n",
    "    print(\"Randomized SVD:\", S_rand.numpy())\n",
    "    \n",
    "    # Plot singular values comparison\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar([f\"True {i+1}\" for i in range(rank)], S_true.numpy(), alpha=0.7, label=\"True\")\n",
    "    plt.bar([f\"Full {i+1}\" for i in range(rank)], S_full[:rank].numpy(), alpha=0.7, label=\"Full SVD\")\n",
    "    plt.bar([f\"Rand {i+1}\" for i in range(rank)], S_rand.numpy(), alpha=0.7, label=\"Randomized SVD\")\n",
    "    plt.title(\"Singular Values Comparison\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Examine the effect of oversampling and power iterations\n",
    "    def study_randomized_parameters():\n",
    "        \"\"\"Study the effect of oversampling and power iterations on randomized SVD.\"\"\"\n",
    "        oversampling_values = [0, 5, 10, 20]\n",
    "        power_iterations = [0, 1, 2, 4]\n",
    "        \n",
    "        results = np.zeros((len(oversampling_values), len(power_iterations)))\n",
    "        timings = np.zeros((len(oversampling_values), len(power_iterations)))\n",
    "        \n",
    "        for i, p in enumerate(oversampling_values):\n",
    "            for j, q in enumerate(power_iterations):\n",
    "                start_time = time.time()\n",
    "                U_r, S_r, V_r = randomized_svd(A_noisy, k=rank, p=p, q=q)\n",
    "                timings[i, j] = time.time() - start_time\n",
    "                \n",
    "                # Convert V_r from V^T to V\n",
    "                V_r = V_r.T\n",
    "                \n",
    "                # Calculate reconstruction error\n",
    "                error = torch.norm(A - U_r @ torch.diag(S_r) @ V_r.T).item() / torch.norm(A).item()\n",
    "                results[i, j] = error\n",
    "        \n",
    "        # Plot the results\n",
    "        plt.figure(figsize=(12, 10))\n",
    "        \n",
    "        plt.subplot(2, 1, 1)\n",
    "        sns.heatmap(results, annot=True, fmt=\".2e\", cmap=\"viridis_r\",\n",
    "                   xticklabels=power_iterations, yticklabels=oversampling_values)\n",
    "        plt.title(\"Reconstruction Error\")\n",
    "        plt.xlabel(\"Power Iterations (q)\")\n",
    "        plt.ylabel(\"Oversampling (p)\")\n",
    "        \n",
    "        plt.subplot(2, 1, 2)\n",
    "        sns.heatmap(timings, annot=True, fmt=\".4f\", cmap=\"viridis\",\n",
    "                   xticklabels=power_iterations, yticklabels=oversampling_values)\n",
    "        plt.title(\"Computation Time (seconds)\")\n",
    "        plt.xlabel(\"Power Iterations (q)\")\n",
    "        plt.ylabel(\"Oversampling (p)\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        return results, timings\n",
    "    \n",
    "    # Study the effect of randomized SVD parameters\n",
    "    param_results, param_timings = study_randomized_parameters()\n",
    "    \n",
    "    return A, A_noisy, U_rand, S_rand, V_rand, param_results, param_timings\n",
    "\n",
    "# Demonstrate randomized SVD\n",
    "A_rand, A_noisy_rand, U_rand, S_rand, V_rand, param_results, param_timings = demonstrate_randomized_svd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96f8113",
   "metadata": {},
   "source": [
    "## 5. Comparing SVD Algorithms\n",
    "\n",
    "Let's compare the different SVD algorithms in terms of accuracy, speed, and scalability for various types of matrices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e78d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_svd_algorithms():\n",
    "    \"\"\"Compare different SVD algorithms.\"\"\"\n",
    "    # Create matrices of different types and sizes\n",
    "    matrices = {\n",
    "        \"Small Dense (10x8)\": create_example_matrix(10, 8, \"random\"),\n",
    "        \"Medium Dense (100x80)\": create_example_matrix(100, 80, \"random\"),\n",
    "        \"Low Rank (100x80, rank≈10)\": create_example_matrix(100, 80, \"lowrank\"),\n",
    "        \"Image-like (50x30)\": create_example_matrix(50, 30, \"image\")\n",
    "    }\n",
    "    \n",
    "    # Define the algorithms to compare\n",
    "    algorithms = {\n",
    "        \"Power Method\": lambda A: power_method_svd(A),\n",
    "        \"Shifted Power\": lambda A: shifted_power_method_svd(A),\n",
    "        \"Bidiagonal\": lambda A: bidiagonal_svd(A, full_matrices=False),\n",
    "        \"Jacobi\": lambda A: jacobi_svd(A),\n",
    "        \"Randomized\": lambda A: randomized_svd(A, k=min(A.shape)-1, p=5, q=2),\n",
    "        \"PyTorch\": lambda A: torch.linalg.svd(A, full_matrices=False)\n",
    "    }\n",
    "    \n",
    "    # Collect results\n",
    "    results = {}\n",
    "    \n",
    "    for matrix_name, A in matrices.items():\n",
    "        results[matrix_name] = {\"time\": {}, \"error\": {}, \"ortho_U\": {}, \"ortho_V\": {}}\n",
    "        \n",
    "        for algo_name, algo_func in algorithms.items():\n",
    "            try:\n",
    "                # Skip Jacobi for larger matrices (too slow)\n",
    "                if algo_name == \"Jacobi\" and A.shape[0] > 20:\n",
    "                    results[matrix_name][\"time\"][algo_name] = float('nan')\n",
    "                    results[matrix_name][\"error\"][algo_name] = float('nan')\n",
    "                    results[matrix_name][\"ortho_U\"][algo_name] = float('nan')\n",
    "                    results[matrix_name][\"ortho_V\"][algo_name] = float('nan')\n",
    "                    continue\n",
    "                \n",
    "                # Measure time\n",
    "                start_time = time.time()\n",
    "                U, S, V = algo_func(A)\n",
    "                results[matrix_name][\"time\"][algo_name] = time.time() - start_time\n",
    "                \n",
    "                # Evaluate results\n",
    "                if algo_name == \"PyTorch\":\n",
    "                    V = V.T  # Convert V\n",
    "                \n",
    "                evaluation = evaluate_svd(A, U, S, V)\n",
    "                results[matrix_name][\"error\"][algo_name] = evaluation[\"reconstruction_error\"]\n",
    "                results[matrix_name][\"ortho_U\"][algo_name] = evaluation[\"U_orthogonality_error\"]\n",
    "                results[matrix_name][\"ortho_V\"][algo_name] = evaluation[\"V_orthogonality_error\"]\n",
    "            except Exception as e:\n",
    "                print(f\"Error with {algo_name} on {matrix_name}: {e}\")\n",
    "                results[matrix_name][\"time\"][algo_name] = float('nan')\n",
    "                results[matrix_name][\"error\"][algo_name] = float('nan')\n",
    "                results[matrix_name][\"ortho_U\"][algo_name] = float('nan')\n",
    "                results[matrix_name][\"ortho_V\"][algo_name] = float('nan')\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    # Plot computation time\n",
    "    plt.subplot(2, 2, 1)\n",
    "    for i, matrix_name in enumerate(matrices.keys()):\n",
    "        times = [results[matrix_name][\"time\"].get(algo, float('nan')) for algo in algorithms.keys()]\n",
    "        plt.semilogy([algo for algo in algorithms.keys()], times, 'o-', label=matrix_name)\n",
    "    \n",
    "    plt.title(\"Computation Time\")\n",
    "    plt.xlabel(\"Algorithm\")\n",
    "    plt.ylabel(\"Time (seconds, log scale)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot reconstruction error\n",
    "    plt.subplot(2, 2, 2)\n",
    "    for i, matrix_name in enumerate(matrices.keys()):\n",
    "        errors = [results[matrix_name][\"error\"].get(algo, float('nan')) for algo in algorithms.keys()]\n",
    "        plt.semilogy([algo for algo in algorithms.keys()], errors, 'o-', label=matrix_name)\n",
    "    \n",
    "    plt.title(\"Reconstruction Error (||A - USV^T||/||A||)\")\n",
    "    plt.xlabel(\"Algorithm\")\n",
    "    plt.ylabel(\"Relative Error (log scale)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot U orthogonality error\n",
    "    plt.subplot(2, 2, 3)\n",
    "    for i, matrix_name in enumerate(matrices.keys()):\n",
    "        ortho_U = [results[matrix_name][\"ortho_U\"].get(algo, float('nan')) for algo in algorithms.keys()]\n",
    "        plt.semilogy([algo for algo in algorithms.keys()], ortho_U, 'o-', label=matrix_name)\n",
    "    \n",
    "    plt.title(\"U Orthogonality Error (||U^T U - I||)\")\n",
    "    plt.xlabel(\"Algorithm\")\n",
    "    plt.ylabel(\"Error (log scale)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Plot V orthogonality error\n",
    "    plt.subplot(2, 2, 4)\n",
    "    for i, matrix_name in enumerate(matrices.keys()):\n",
    "        ortho_V = [results[matrix_name][\"ortho_V\"].get(algo, float('nan')) for algo in algorithms.keys()]\n",
    "        plt.semilogy([algo for algo in algorithms.keys()], ortho_V, 'o-', label=matrix_name)\n",
    "    \n",
    "    plt.title(\"V Orthogonality Error (||V^T V - I||)\")\n",
    "    plt.xlabel(\"Algorithm\")\n",
    "    plt.ylabel(\"Error (log scale)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print a summary of the results\n",
    "    print(\"Summary of SVD Algorithms Comparison:\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for matrix_name in matrices.keys():\n",
    "        print(f\"\\nMatrix: {matrix_name}\")\n",
    "        print(\"-\" * 80)\n",
    "        print(f\"{'Algorithm':<15} {'Time (s)':<15} {'Reconstruction Error':<25} {'U Orthogonality':<20} {'V Orthogonality':<20}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        for algo_name in algorithms.keys():\n",
    "            time_val = results[matrix_name][\"time\"].get(algo_name, float('nan'))\n",
    "            error_val = results[matrix_name][\"error\"].get(algo_name, float('nan'))\n",
    "            ortho_U_val = results[matrix_name][\"ortho_U\"].get(algo_name, float('nan'))\n",
    "            ortho_V_val = results[matrix_name][\"ortho_V\"].get(algo_name, float('nan'))\n",
    "            \n",
    "            print(f\"{algo_name:<15} {time_val:<15.6f} {error_val:<25.6e} {ortho_U_val:<20.6e} {ortho_V_val:<20.6e}\")\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Compare SVD algorithms\n",
    "svd_algorithm_comparison = compare_svd_algorithms()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d6b70a2",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored various algorithms for computing the Singular Value Decomposition (SVD):\n",
    "\n",
    "1. **Power Method**: A simple iterative approach that computes singular values and vectors one at a time. We also implemented a shifted variant for faster convergence.\n",
    "\n",
    "2. **Bidiagonalization Method**: A more efficient two-step approach that first reduces the matrix to bidiagonal form, then computes the SVD of the bidiagonal matrix.\n",
    "\n",
    "3. **Jacobi SVD Algorithm**: An iterative method that directly computes the SVD by applying a sequence of plane rotations to diagonalize the matrix.\n",
    "\n",
    "4. **Randomized SVD**: A modern approach that uses random projections to efficiently compute an approximate SVD, particularly for large matrices.\n",
    "\n",
    "5. **Comparison of Algorithms**: We compared these different algorithms in terms of accuracy, speed, and scalability for various types of matrices.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- Standard libraries like PyTorch and NumPy typically use highly optimized implementations of the bidiagonalization approach\n",
    "- The power method is simple but can be slow for matrices with clustered singular values\n",
    "- Jacobi SVD can be accurate but is generally slower for larger matrices\n",
    "- Randomized SVD offers excellent performance for large matrices, especially when an approximate low-rank SVD is sufficient\n",
    "- The choice of algorithm depends on the specific requirements of the application, including matrix size, structure, and accuracy needs\n",
    "\n",
    "In the next notebook, we'll explore practical applications of SVD in various domains.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
