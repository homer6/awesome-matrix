{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5cfcdd9a",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/09_singular_value_decomposition`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/09_singular_value_decomposition/01_introduction.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d8a3f03",
   "metadata": {},
   "source": [
    "# Singular Value Decomposition: Introduction\n",
    "\n",
    "Singular Value Decomposition (SVD) is a fundamental matrix factorization technique with wide-ranging applications in signal processing, statistics, machine learning, and many other fields. It decomposes a matrix into the product of three matrices, revealing important properties of the original matrix.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Understand the concept of SVD and its geometric interpretation\n",
    "2. Implement SVD from scratch\n",
    "3. Visualize the decomposition\n",
    "4. Compare with built-in SVD functions\n",
    "\n",
    "SVD is particularly valuable because it works for any matrix (not just square or full-rank matrices) and provides a way to decompose a matrix into its fundamental components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e913863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.linalg\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from matplotlib.patches import Ellipse\n",
    "\n",
    "# For better looking plots\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom colormap (light blue to dark blue)\n",
    "colors = [(0.95, 0.95, 1), (0.0, 0.2, 0.6)]\n",
    "blue_cmap = LinearSegmentedColormap.from_list('CustomBlue', colors, N=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "520b80dc",
   "metadata": {},
   "source": [
    "## Basic Concept of Singular Value Decomposition\n",
    "\n",
    "For any matrix $A \\in \\mathbb{R}^{m \\times n}$, the Singular Value Decomposition (SVD) is given by:\n",
    "\n",
    "$$A = U \\Sigma V^T$$\n",
    "\n",
    "where:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix whose columns are the left singular vectors of $A$\n",
    "- $\\Sigma \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix containing the singular values of $A$ in descending order\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix whose columns are the right singular vectors of $A$\n",
    "\n",
    "The singular values are non-negative real numbers and represent the \"strength\" or \"importance\" of the corresponding singular vectors in the decomposition.\n",
    "\n",
    "Let's start by creating an example matrix and visualizing its SVD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fcdfeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example_matrix(m=4, n=3, method=\"random\"):\n",
    "    \"\"\"Create a matrix for SVD demonstration.\"\"\"\n",
    "    if method == \"random\":\n",
    "        # Create a random matrix\n",
    "        A = torch.rand(m, n) * 10 - 5  # Values between -5 and 5\n",
    "    elif method == \"simple\":\n",
    "        # Simple predefined matrix for clear demonstration\n",
    "        if m == 3 and n == 2:\n",
    "            A = torch.tensor([\n",
    "                [4.0, 0.0],\n",
    "                [3.0, -5.0],\n",
    "                [0.0, 4.0]\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(f\"Simple method doesn't support dimensions {m}x{n}\")\n",
    "    elif method == \"image\":\n",
    "        # Create a simple image-like matrix with structure\n",
    "        A = torch.zeros(m, n)\n",
    "        # Add a gradient pattern\n",
    "        for i in range(m):\n",
    "            for j in range(n):\n",
    "                A[i, j] = i + j * 0.5\n",
    "        # Add some noise\n",
    "        A = A + torch.randn(m, n) * 0.2\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Create a simple example matrix\n",
    "A_simple = create_example_matrix(m=3, n=2, method=\"simple\")\n",
    "\n",
    "# Display the matrix\n",
    "def plot_matrix(matrix, title=\"Matrix\", annotate=True, cmap=blue_cmap):\n",
    "    \"\"\"Plot a matrix as a heatmap with annotations.\"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix_np = matrix.numpy()\n",
    "    else:\n",
    "        matrix_np = matrix\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(matrix_np, annot=annotate, fmt=\".2f\", cmap=cmap, \n",
    "                    linewidths=1, cbar=True)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add row and column indices\n",
    "    ax.set_xticks(np.arange(matrix_np.shape[1]) + 0.5)\n",
    "    ax.set_yticks(np.arange(matrix_np.shape[0]) + 0.5)\n",
    "    ax.set_xticklabels([f\"Col {i+1}\" for i in range(matrix_np.shape[1])])\n",
    "    ax.set_yticklabels([f\"Row {i+1}\" for i in range(matrix_np.shape[0])])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_matrix(A_simple, \"Example Matrix A (3×2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c908cb39",
   "metadata": {},
   "source": [
    "## Computing SVD using Eigendecomposition\n",
    "\n",
    "One approach to compute the SVD is through the eigendecomposition of $A^TA$ and $AA^T$:\n",
    "\n",
    "1. The columns of $V$ are the eigenvectors of $A^TA$\n",
    "2. The columns of $U$ are the eigenvectors of $AA^T$\n",
    "3. The singular values in $\\Sigma$ are the square roots of the eigenvalues of $A^TA$ (or $AA^T$)\n",
    "\n",
    "Let's implement this approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592a04fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_svd_via_eigen(A):\n",
    "    \"\"\"\n",
    "    Compute SVD using eigendecomposition of A^T A and A A^T.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors\n",
    "        S: Singular values\n",
    "        V: Right singular vectors\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    # Compute A^T A and A A^T\n",
    "    ATA = A.T @ A  # n x n\n",
    "    AAT = A @ A.T  # m x m\n",
    "    \n",
    "    # Get eigenvalues and eigenvectors of A^T A\n",
    "    eigvals_ATA, eigvecs_ATA = torch.linalg.eigh(ATA)\n",
    "    \n",
    "    # Get eigenvalues and eigenvectors of A A^T\n",
    "    eigvals_AAT, eigvecs_AAT = torch.linalg.eigh(AAT)\n",
    "    \n",
    "    # Sort eigenvalues and eigenvectors in descending order\n",
    "    sorted_indices_ATA = torch.argsort(eigvals_ATA, descending=True)\n",
    "    eigvals_ATA = eigvals_ATA[sorted_indices_ATA]\n",
    "    eigvecs_ATA = eigvecs_ATA[:, sorted_indices_ATA]\n",
    "    \n",
    "    sorted_indices_AAT = torch.argsort(eigvals_AAT, descending=True)\n",
    "    eigvals_AAT = eigvals_AAT[sorted_indices_AAT]\n",
    "    eigvecs_AAT = eigvecs_AAT[:, sorted_indices_AAT]\n",
    "    \n",
    "    # Compute singular values\n",
    "    # We use eigenvalues from A^T A, but could also use those from A A^T\n",
    "    S = torch.sqrt(torch.clamp(eigvals_ATA, min=0))\n",
    "    \n",
    "    # Set V as eigenvectors of A^T A\n",
    "    V = eigvecs_ATA\n",
    "    \n",
    "    # Set U as eigenvectors of A A^T\n",
    "    U = eigvecs_AAT\n",
    "    \n",
    "    # When A is not square, we need to handle the cases m > n or n > m\n",
    "    min_dim = min(m, n)\n",
    "    r = min_dim  # Assuming full rank for simplicity\n",
    "    \n",
    "    # If m > n, then we need to compute some columns of U using A V / sigma\n",
    "    # U[:, :r] = A @ V[:, :r] @ torch.diag(1.0 / S[:r])\n",
    "    \n",
    "    # Alternative approach: use the identity A V_i = sigma_i U_i\n",
    "    for i in range(min_dim):\n",
    "        if S[i] > 1e-10:  # Check for numerical stability\n",
    "            u_i = (A @ V[:, i]) / S[i]\n",
    "            U[:, i] = u_i\n",
    "    \n",
    "    # Return the decomposition\n",
    "    # For a non-square matrix, we need to adjust the dimensions of Sigma\n",
    "    S_full = torch.zeros(m, n)\n",
    "    for i in range(min(m, n)):\n",
    "        if i < len(S):\n",
    "            S_full[i, i] = S[i]\n",
    "    \n",
    "    return U, S_full, V\n",
    "\n",
    "# Test SVD on the example matrix\n",
    "U_simple, S_simple, V_simple = compute_svd_via_eigen(A_simple)\n",
    "\n",
    "# Display the matrices\n",
    "plot_matrix(U_simple, \"Left Singular Vectors (U)\")\n",
    "plot_matrix(S_simple, \"Singular Values (Σ)\")\n",
    "plot_matrix(V_simple, \"Right Singular Vectors (V)\")\n",
    "\n",
    "# Verify that A = U Σ V^T\n",
    "reconstructed_A = U_simple @ S_simple @ V_simple.T\n",
    "plot_matrix(reconstructed_A, \"Reconstructed Matrix (U Σ V^T)\")\n",
    "\n",
    "# Calculate reconstruction error\n",
    "reconstruction_error = torch.norm(A_simple - reconstructed_A).item()\n",
    "print(f\"Reconstruction error: {reconstruction_error:.2e}\")\n",
    "\n",
    "# Verify that U and V are orthogonal\n",
    "U_orthogonality = U_simple.T @ U_simple\n",
    "V_orthogonality = V_simple.T @ V_simple\n",
    "\n",
    "plot_matrix(U_orthogonality, \"U^T U (should be identity)\")\n",
    "plot_matrix(V_orthogonality, \"V^T V (should be identity)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf2c8ea",
   "metadata": {},
   "source": [
    "## Geometric Interpretation of SVD\n",
    "\n",
    "SVD has a beautiful geometric interpretation. It can be thought of as a sequence of three transformations:\n",
    "\n",
    "1. $V^T$ rotates the space\n",
    "2. $\\Sigma$ scales the space along the coordinate axes\n",
    "3. $U$ rotates the space again\n",
    "\n",
    "Let's visualize this for a 2D case:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31b6c23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_svd_2d():\n",
    "    \"\"\"Visualize the geometric interpretation of SVD in 2D.\"\"\"\n",
    "    # Create a 2×2 matrix for visualization\n",
    "    A = torch.tensor([[3.0, 1.0],\n",
    "                      [1.0, 2.0]])\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, S, V = torch.linalg.svd(A)\n",
    "    \n",
    "    # Make sure S is a diagonal matrix\n",
    "    S_matrix = torch.zeros_like(A)\n",
    "    for i in range(min(A.shape)):\n",
    "        S_matrix[i, i] = S[i]\n",
    "    \n",
    "    # Create a set of points on a circle\n",
    "    theta = np.linspace(0, 2*np.pi, 100)\n",
    "    circle_x = np.cos(theta)\n",
    "    circle_y = np.sin(theta)\n",
    "    circle_points = np.vstack([circle_x, circle_y]).T\n",
    "    \n",
    "    # Convert to PyTorch tensor\n",
    "    circle_points = torch.tensor(circle_points, dtype=torch.float32)\n",
    "    \n",
    "    # Apply the transformations\n",
    "    step1_points = circle_points  # Original circle\n",
    "    step2_points = step1_points @ V  # After V^T (transposed for right multiplication)\n",
    "    step3_points = step2_points @ S_matrix  # After Σ\n",
    "    step4_points = step3_points @ U.T  # After U (transposed for right multiplication)\n",
    "    \n",
    "    # Create the visualization\n",
    "    plt.figure(figsize=(16, 4))\n",
    "    \n",
    "    # Original circle\n",
    "    plt.subplot(1, 4, 1)\n",
    "    plt.scatter(step1_points[:, 0], step1_points[:, 1], alpha=0.7)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"Original Circle\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "    # After V^T transformation\n",
    "    plt.subplot(1, 4, 2)\n",
    "    plt.scatter(step2_points[:, 0], step2_points[:, 1], alpha=0.7)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"After V^T (Rotation)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "    # After Σ transformation\n",
    "    plt.subplot(1, 4, 3)\n",
    "    plt.scatter(step3_points[:, 0], step3_points[:, 1], alpha=0.7)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"After Σ (Scaling)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "    # Final result after U transformation\n",
    "    plt.subplot(1, 4, 4)\n",
    "    plt.scatter(step4_points[:, 0], step4_points[:, 1], alpha=0.7)\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.title(\"After U (Final Rotation)\")\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"y\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the original matrix and its decomposition\n",
    "    print(\"Original Matrix A:\")\n",
    "    print(A.numpy())\n",
    "    print(\"\\nLeft Singular Vectors (U):\")\n",
    "    print(U.numpy())\n",
    "    print(\"\\nSingular Values (S):\")\n",
    "    print(S.numpy())\n",
    "    print(\"\\nRight Singular Vectors (V):\")\n",
    "    print(V.numpy())\n",
    "    \n",
    "    # Visualize the matrix transformation\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Original unit vectors\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.arrow(0, 0, 1, 0, head_width=0.1, head_length=0.1, fc='blue', ec='blue', label='i')\n",
    "    plt.arrow(0, 0, 0, 1, head_width=0.1, head_length=0.1, fc='red', ec='red', label='j')\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(-2, 4)\n",
    "    plt.ylim(-2, 4)\n",
    "    plt.title(\"Original Unit Vectors\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Transformed unit vectors\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.arrow(0, 0, A[0, 0], A[1, 0], head_width=0.1, head_length=0.1, fc='blue', ec='blue', label='A·i')\n",
    "    plt.arrow(0, 0, A[0, 1], A[1, 1], head_width=0.1, head_length=0.1, fc='red', ec='red', label='A·j')\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(-2, 4)\n",
    "    plt.ylim(-2, 4)\n",
    "    plt.title(\"Transformed Unit Vectors (A·i and A·j)\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the principal axes (singular vectors)\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    \n",
    "    # Draw the transformed unit circle (ellipse)\n",
    "    plt.axes().set_aspect('equal')\n",
    "    ellipse = Ellipse((0, 0), 2*S[0], 2*S[1], \n",
    "                     angle=np.degrees(np.arctan2(U[1, 0], U[0, 0])),\n",
    "                     facecolor='none', edgecolor='green', linewidth=2, alpha=0.7)\n",
    "    plt.gca().add_patch(ellipse)\n",
    "    \n",
    "    # Draw the principal axes (singular vectors scaled by singular values)\n",
    "    plt.arrow(0, 0, S[0]*U[0, 0], S[0]*U[1, 0], head_width=0.1, head_length=0.1, \n",
    "             fc='blue', ec='blue', label=f'σ₁·u₁ ({S[0]:.2f})')\n",
    "    plt.arrow(0, 0, S[1]*U[0, 1], S[1]*U[1, 1], head_width=0.1, head_length=0.1, \n",
    "             fc='red', ec='red', label=f'σ₂·u₂ ({S[1]:.2f})')\n",
    "    \n",
    "    # Draw original unit vectors transformed by A\n",
    "    plt.arrow(0, 0, A[0, 0], A[1, 0], head_width=0.1, head_length=0.1, \n",
    "             fc='lightblue', ec='lightblue', linestyle='--', alpha=0.7, label='A·i')\n",
    "    plt.arrow(0, 0, A[0, 1], A[1, 1], head_width=0.1, head_length=0.1, \n",
    "             fc='pink', ec='pink', linestyle='--', alpha=0.7, label='A·j')\n",
    "    \n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim(-4, 4)\n",
    "    plt.ylim(-4, 4)\n",
    "    plt.title(\"Principal Axes of the Transformation\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize SVD in 2D\n",
    "visualize_svd_2d()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eefa704a",
   "metadata": {},
   "source": [
    "## Truncated SVD\n",
    "\n",
    "One of the most powerful aspects of SVD is that we can create a low-rank approximation of a matrix by keeping only the largest singular values and their corresponding singular vectors. This is known as truncated SVD.\n",
    "\n",
    "Given the SVD $A = U \\Sigma V^T$, the best rank-$k$ approximation of $A$ is:\n",
    "\n",
    "$$A_k = U_k \\Sigma_k V_k^T$$\n",
    "\n",
    "where $U_k$, $\\Sigma_k$, and $V_k$ contain only the first $k$ columns of $U$, the $k \\times k$ upper-left block of $\\Sigma$, and the first $k$ columns of $V$, respectively.\n",
    "\n",
    "Let's implement and visualize truncated SVD:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec56fe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def truncated_svd(A, k):\n",
    "    \"\"\"\n",
    "    Compute truncated SVD, keeping only the top k singular values.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        k: Number of singular values to keep\n",
    "        \n",
    "    Returns:\n",
    "        U_k: First k columns of U\n",
    "        S_k: Diagonal matrix with top k singular values\n",
    "        V_k: First k columns of V\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    # Compute full SVD\n",
    "    U, S, V = torch.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    # Keep only the first k components\n",
    "    U_k = U[:, :k]\n",
    "    S_k = torch.zeros(k, k)\n",
    "    for i in range(k):\n",
    "        S_k[i, i] = S[i]\n",
    "    V_k = V[:, :k]\n",
    "    \n",
    "    return U_k, S_k, V_k\n",
    "\n",
    "def visualize_truncated_svd():\n",
    "    \"\"\"Visualize truncated SVD for a larger matrix.\"\"\"\n",
    "    # Create a larger matrix\n",
    "    m, n = 10, 8\n",
    "    A = create_example_matrix(m=m, n=n, method=\"image\")\n",
    "    \n",
    "    # Display the original matrix\n",
    "    plot_matrix(A, \"Original Matrix\")\n",
    "    \n",
    "    # Compute SVD\n",
    "    U, S, V = torch.linalg.svd(A, full_matrices=False)\n",
    "    \n",
    "    # Create a diagonal matrix from S\n",
    "    S_matrix = torch.diag(S)\n",
    "    \n",
    "    # Plot the singular values\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(S.numpy(), 'o-')\n",
    "    plt.title(\"Singular Values\")\n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Value\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(len(S)))\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize different ranks of approximation\n",
    "    ranks = [1, 2, 3, 4, min(m, n)]\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Original matrix\n",
    "    plt.subplot(2, 3, 1)\n",
    "    plt.imshow(A.numpy(), cmap='viridis')\n",
    "    plt.title(f\"Original Matrix (Rank {min(m, n)})\")\n",
    "    plt.colorbar()\n",
    "    \n",
    "    for i, k in enumerate(ranks[:-1]):\n",
    "        # Compute rank-k approximation\n",
    "        U_k, S_k, V_k = truncated_svd(A, k)\n",
    "        A_k = U_k @ S_k @ V_k.T\n",
    "        \n",
    "        # Display the approximation\n",
    "        plt.subplot(2, 3, i+2)\n",
    "        plt.imshow(A_k.numpy(), cmap='viridis')\n",
    "        plt.title(f\"Rank-{k} Approximation\")\n",
    "        plt.colorbar()\n",
    "        \n",
    "        # Calculate and display the approximation error\n",
    "        error = torch.norm(A - A_k).item() / torch.norm(A).item()\n",
    "        plt.xlabel(f\"Relative Error: {error:.4f}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize the energy captured by each singular value\n",
    "    energy = S**2 / torch.sum(S**2)\n",
    "    cumulative_energy = torch.cumsum(energy, 0)\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(len(energy)), energy.numpy())\n",
    "    plt.title(\"Energy Distribution\")\n",
    "    plt.xlabel(\"Singular Value Index\")\n",
    "    plt.ylabel(\"Normalized Energy\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(range(len(energy)))\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(range(len(cumulative_energy)), cumulative_energy.numpy(), 'o-')\n",
    "    plt.axhline(y=0.9, color='r', linestyle='--', label='90%')\n",
    "    plt.axhline(y=0.99, color='g', linestyle='--', label='99%')\n",
    "    plt.title(\"Cumulative Energy\")\n",
    "    plt.xlabel(\"Number of Singular Values\")\n",
    "    plt.ylabel(\"Cumulative Energy\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.xticks(range(len(cumulative_energy)))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find the number of singular values needed to capture 90% and 99% of energy\n",
    "    k_90 = torch.sum(cumulative_energy < 0.9).item() + 1\n",
    "    k_99 = torch.sum(cumulative_energy < 0.99).item() + 1\n",
    "    \n",
    "    print(f\"Number of singular values needed to capture 90% of energy: {k_90}\")\n",
    "    print(f\"Number of singular values needed to capture 99% of energy: {k_99}\")\n",
    "    \n",
    "    return A, U, S, V, cumulative_energy\n",
    "\n",
    "# Visualize truncated SVD\n",
    "A_trunc, U_trunc, S_trunc, V_trunc, energy_trunc = visualize_truncated_svd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95847e71",
   "metadata": {},
   "source": [
    "## Comparing SVD Implementations\n",
    "\n",
    "Let's compare our eigendecomposition-based SVD with built-in implementations in terms of performance and accuracy:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f8e409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_svd_methods(A):\n",
    "    \"\"\"\n",
    "    Compare different SVD implementations.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with timing and accuracy results\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    methods = []\n",
    "    times = []\n",
    "    reconstruction_errors = []\n",
    "    orthogonality_errors_U = []\n",
    "    orthogonality_errors_V = []\n",
    "    \n",
    "    # Method 1: Our eigendecomposition-based SVD\n",
    "    start_time = time.time()\n",
    "    U_eigen, S_eigen, V_eigen = compute_svd_via_eigen(A)\n",
    "    eigen_time = time.time() - start_time\n",
    "    \n",
    "    methods.append(\"Eigendecomposition\")\n",
    "    times.append(eigen_time)\n",
    "    \n",
    "    # Calculate errors\n",
    "    A_reconstructed_eigen = U_eigen @ S_eigen @ V_eigen.T\n",
    "    reconstruction_errors.append(torch.norm(A - A_reconstructed_eigen).item() / torch.norm(A).item())\n",
    "    orthogonality_errors_U.append(torch.norm(U_eigen.T @ U_eigen - torch.eye(U_eigen.shape[1])).item())\n",
    "    orthogonality_errors_V.append(torch.norm(V_eigen.T @ V_eigen - torch.eye(V_eigen.shape[1])).item())\n",
    "    \n",
    "    # Method 2: PyTorch SVD\n",
    "    start_time = time.time()\n",
    "    U_torch, S_torch, V_torch = torch.linalg.svd(A, full_matrices=True)\n",
    "    torch_time = time.time() - start_time\n",
    "    \n",
    "    methods.append(\"PyTorch SVD\")\n",
    "    times.append(torch_time)\n",
    "    \n",
    "    # Create a diagonal matrix from S\n",
    "    m, n = A.shape\n",
    "    S_torch_matrix = torch.zeros(m, n)\n",
    "    for i in range(min(m, n)):\n",
    "        S_torch_matrix[i, i] = S_torch[i]\n",
    "    \n",
    "    # Calculate errors\n",
    "    A_reconstructed_torch = U_torch @ S_torch_matrix @ V_torch\n",
    "    reconstruction_errors.append(torch.norm(A - A_reconstructed_torch).item() / torch.norm(A).item())\n",
    "    orthogonality_errors_U.append(torch.norm(U_torch.T @ U_torch - torch.eye(U_torch.shape[1])).item())\n",
    "    orthogonality_errors_V.append(torch.norm(V_torch.T @ V_torch - torch.eye(V_torch.shape[1])).item())\n",
    "    \n",
    "    # Method 3: NumPy SVD (via SciPy)\n",
    "    A_np = A.numpy()\n",
    "    start_time = time.time()\n",
    "    U_np, S_np, Vt_np = scipy.linalg.svd(A_np, full_matrices=True)\n",
    "    np_time = time.time() - start_time\n",
    "    \n",
    "    methods.append(\"NumPy/SciPy SVD\")\n",
    "    times.append(np_time)\n",
    "    \n",
    "    # Create a diagonal matrix from S\n",
    "    S_np_matrix = np.zeros((m, n))\n",
    "    for i in range(min(m, n)):\n",
    "        S_np_matrix[i, i] = S_np[i]\n",
    "    \n",
    "    # Calculate errors\n",
    "    A_reconstructed_np = U_np @ S_np_matrix @ Vt_np\n",
    "    reconstruction_errors.append(np.linalg.norm(A_np - A_reconstructed_np) / np.linalg.norm(A_np))\n",
    "    orthogonality_errors_U.append(np.linalg.norm(U_np.T @ U_np - np.eye(U_np.shape[1])))\n",
    "    orthogonality_errors_V.append(np.linalg.norm(Vt_np @ Vt_np.T - np.eye(Vt_np.shape[0])))\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Timing comparison\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(methods, times)\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"SVD Computation Time\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reconstruction error\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(methods, reconstruction_errors)\n",
    "    plt.ylabel(\"Relative Reconstruction Error\")\n",
    "    plt.title(\"SVD Accuracy (||A - USV^T||/||A||)\")\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Orthogonality error for U\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(methods, orthogonality_errors_U)\n",
    "    plt.ylabel(\"Orthogonality Error\")\n",
    "    plt.title(\"U Orthogonality (||U^T U - I||)\")\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Orthogonality error for V\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar(methods, orthogonality_errors_V)\n",
    "    plt.ylabel(\"Orthogonality Error\")\n",
    "    plt.title(\"V Orthogonality (||V^T V - I||)\")\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"SVD Method Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Method':<20} {'Time (s)':<15} {'Reconstruction Error':<25} {'U Orthogonality':<20} {'V Orthogonality':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, method in enumerate(methods):\n",
    "        print(f\"{method:<20} {times[i]:<15.6f} {reconstruction_errors[i]:<25.6e} {orthogonality_errors_U[i]:<20.6e} {orthogonality_errors_V[i]:<20.6e}\")\n",
    "    \n",
    "    return {\n",
    "        'methods': methods,\n",
    "        'times': times,\n",
    "        'reconstruction_errors': reconstruction_errors,\n",
    "        'orthogonality_errors_U': orthogonality_errors_U,\n",
    "        'orthogonality_errors_V': orthogonality_errors_V\n",
    "    }\n",
    "\n",
    "# Create a larger matrix for performance testing\n",
    "A_large = create_example_matrix(m=100, n=80, method=\"random\")\n",
    "\n",
    "# Compare SVD methods\n",
    "svd_comparison = compare_svd_methods(A_large)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ffe6bb7",
   "metadata": {},
   "source": [
    "## SVD for Non-Square and Rank-Deficient Matrices\n",
    "\n",
    "One of the strengths of SVD is that it works for any matrix, regardless of shape or rank. Let's explore SVD for non-square and rank-deficient matrices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebf474e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_nonsquare_svd():\n",
    "    \"\"\"Demonstrate SVD for non-square matrices.\"\"\"\n",
    "    # Create a tall matrix (more rows than columns)\n",
    "    A_tall = torch.tensor([\n",
    "        [1.0, 2.0],\n",
    "        [3.0, 4.0],\n",
    "        [5.0, 6.0],\n",
    "        [7.0, 8.0]\n",
    "    ])\n",
    "    \n",
    "    # Create a wide matrix (more columns than rows)\n",
    "    A_wide = torch.tensor([\n",
    "        [1.0, 2.0, 3.0, 4.0],\n",
    "        [5.0, 6.0, 7.0, 8.0]\n",
    "    ])\n",
    "    \n",
    "    # Create a rank-deficient matrix\n",
    "    A_deficient = torch.tensor([\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [2.0, 4.0, 6.0],  # 2 × row 1\n",
    "        [3.0, 6.0, 9.0]   # 3 × row 1\n",
    "    ])\n",
    "    \n",
    "    # Function to visualize SVD\n",
    "    def visualize_svd(A, name):\n",
    "        # Compute SVD\n",
    "        U, S, V = torch.linalg.svd(A, full_matrices=True)\n",
    "        \n",
    "        # Create a diagonal matrix from S\n",
    "        m, n = A.shape\n",
    "        S_matrix = torch.zeros(m, n)\n",
    "        for i in range(min(m, n)):\n",
    "            S_matrix[i, i] = S[i]\n",
    "        \n",
    "        # Display matrices\n",
    "        print(f\"SVD of {name} Matrix {A.shape}:\")\n",
    "        \n",
    "        plot_matrix(A, f\"Original {name} Matrix\")\n",
    "        plot_matrix(U, \"Left Singular Vectors (U)\")\n",
    "        plot_matrix(S_matrix, \"Singular Values (Σ)\")\n",
    "        plot_matrix(V.T, \"Right Singular Vectors (V)\")\n",
    "        \n",
    "        # Reconstruct and check error\n",
    "        A_reconstructed = U @ S_matrix @ V\n",
    "        error = torch.norm(A - A_reconstructed).item() / torch.norm(A).item()\n",
    "        plot_matrix(A_reconstructed, f\"Reconstructed Matrix (Error: {error:.2e})\")\n",
    "        \n",
    "        # Plot singular values\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        plt.plot(S.numpy(), 'o-')\n",
    "        plt.title(f\"Singular Values of {name} Matrix\")\n",
    "        plt.xlabel(\"Index\")\n",
    "        plt.ylabel(\"Value\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        plt.xticks(range(len(S)))\n",
    "        plt.show()\n",
    "        \n",
    "        return U, S, V\n",
    "    \n",
    "    # Visualize SVD for each matrix\n",
    "    results_tall = visualize_svd(A_tall, \"Tall\")\n",
    "    results_wide = visualize_svd(A_wide, \"Wide\")\n",
    "    results_deficient = visualize_svd(A_deficient, \"Rank-Deficient\")\n",
    "    \n",
    "    # For the rank-deficient matrix, demonstrate the nullspace\n",
    "    U_def, S_def, V_def = results_deficient\n",
    "    \n",
    "    # Find the rank (number of non-zero singular values)\n",
    "    rank = torch.sum(S_def > 1e-10).item()\n",
    "    print(f\"Rank of the rank-deficient matrix: {rank}\")\n",
    "    \n",
    "    # The nullspace vectors are the columns of V corresponding to zero singular values\n",
    "    nullspace_vectors = V_def.T[:, rank:]\n",
    "    \n",
    "    if nullspace_vectors.shape[1] > 0:\n",
    "        print(\"Nullspace vectors:\")\n",
    "        print(nullspace_vectors.numpy())\n",
    "        \n",
    "        # Verify that these vectors are in the nullspace\n",
    "        for i in range(nullspace_vectors.shape[1]):\n",
    "            v = nullspace_vectors[:, i]\n",
    "            Av = A_deficient @ v\n",
    "            print(f\"||A·v{i+1}|| = {torch.norm(Av).item():.2e}\")\n",
    "    else:\n",
    "        print(\"No nullspace vectors found (matrix is full rank).\")\n",
    "    \n",
    "    return A_tall, A_wide, A_deficient, results_tall, results_wide, results_deficient\n",
    "\n",
    "# Demonstrate SVD for non-square and rank-deficient matrices\n",
    "A_tall, A_wide, A_deficient, results_tall, results_wide, results_deficient = demonstrate_nonsquare_svd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5b7090",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the Singular Value Decomposition (SVD), a fundamental matrix factorization technique:\n",
    "\n",
    "1. We've implemented SVD from scratch using eigendecomposition of $A^TA$ and $AA^T$.\n",
    "\n",
    "2. We've visualized the geometric interpretation of SVD as a sequence of transformations.\n",
    "\n",
    "3. We've demonstrated truncated SVD for low-rank matrix approximation.\n",
    "\n",
    "4. We've compared different SVD implementations in terms of performance and accuracy.\n",
    "\n",
    "5. We've explored SVD for non-square and rank-deficient matrices.\n",
    "\n",
    "SVD has many important properties:\n",
    "\n",
    "- It works for any matrix, regardless of shape or rank.\n",
    "- It provides orthogonal bases for the four fundamental subspaces (column space, row space, nullspace, and left nullspace).\n",
    "- It allows us to express a matrix as a sum of rank-1 matrices.\n",
    "- It gives the best low-rank approximation to a matrix (via truncated SVD).\n",
    "\n",
    "In the next notebook, we'll explore some of the computational aspects of SVD, including more efficient algorithms and numerical considerations.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
