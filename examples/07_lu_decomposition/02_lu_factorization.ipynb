{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f48e487a",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/07_lu_decomposition`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/07_lu_decomposition/02_lu_factorization.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2260d7a",
   "metadata": {},
   "source": [
    "# LU Decomposition: Factorization Methods\n",
    "\n",
    "In this notebook, we explore different methods for computing LU decomposition and their implementation details. We examine the mathematical foundations, algorithms, and potential optimizations for LU factorization.\n",
    "\n",
    "We'll focus on:\n",
    "\n",
    "1. **Mathematical Theory of LU Factorization**\n",
    "2. **Variants of LU Decomposition (LDU, PLU, etc.)**\n",
    "3. **Optimized Implementations**\n",
    "4. **Stability and Accuracy Considerations**\n",
    "\n",
    "This notebook bridges the introduction to LU decomposition and its practical applications, providing deeper insight into the factorization process itself.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59dfd1a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import time\n",
    "import scipy.linalg\n",
    "\n",
    "# For better looking plots\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom colormap (light blue to dark blue)\n",
    "colors = [(0.95, 0.95, 1), (0.0, 0.2, 0.6)]\n",
    "blue_cmap = LinearSegmentedColormap.from_list('CustomBlue', colors, N=100)\n",
    "\n",
    "# Helper functions for visualization\n",
    "def plot_matrix(matrix, title=\"Matrix\", annotate=True, cmap=blue_cmap):\n",
    "    \"\"\"Plot a matrix as a heatmap with annotations.\"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix_np = matrix.detach().cpu().numpy()\n",
    "    else:\n",
    "        matrix_np = matrix\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(matrix_np, annot=annotate, fmt=\".2f\", cmap=cmap, \n",
    "                    linewidths=1, cbar=True)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add row and column indices\n",
    "    ax.set_xticks(np.arange(matrix_np.shape[1]) + 0.5)\n",
    "    ax.set_yticks(np.arange(matrix_np.shape[0]) + 0.5)\n",
    "    ax.set_xticklabels([f\"Col {i+1}\" for i in range(matrix_np.shape[1])])\n",
    "    ax.set_yticklabels([f\"Row {i+1}\" for i in range(matrix_np.shape[0])])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf8b73b5",
   "metadata": {},
   "source": [
    "## 1. Mathematical Theory of LU Factorization\n",
    "\n",
    "LU decomposition expresses a matrix $A$ as the product of a lower triangular matrix $L$ and an upper triangular matrix $U$:\n",
    "\n",
    "$$A = LU$$\n",
    "\n",
    "This factorization is closely related to Gaussian elimination, which transforms a matrix into row echelon form through a series of row operations.\n",
    "\n",
    "### Conditions for Existence\n",
    "\n",
    "A square matrix $A$ admits an LU factorization without pivoting if and only if all its leading principal minors are non-zero. In other words, the determinants of the sub-matrices formed by the first $k$ rows and columns must be non-zero for $k = 1, 2, \\ldots, n-1$.\n",
    "\n",
    "Let's demonstrate these conditions with examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f79701e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_lu_existence(A):\n",
    "    \"\"\"Check if LU factorization exists without pivoting.\"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    exists = True\n",
    "    minor_values = []\n",
    "    \n",
    "    for k in range(1, n):\n",
    "        # Extract leading principal minor\n",
    "        minor = A[:k, :k]\n",
    "        # Calculate determinant\n",
    "        det = torch.linalg.det(minor).item()\n",
    "        minor_values.append(det)\n",
    "        \n",
    "        if abs(det) < 1e-10:\n",
    "            exists = False\n",
    "            break\n",
    "    \n",
    "    return exists, minor_values\n",
    "\n",
    "def demonstrate_lu_existence():\n",
    "    \"\"\"Demonstrate matrices with and without LU factorization.\"\"\"\n",
    "    # Matrix that has LU factorization\n",
    "    A_good = torch.tensor([\n",
    "        [2.0, 1.0, 1.0],\n",
    "        [4.0, 3.0, 3.0],\n",
    "        [8.0, 7.0, 9.0]\n",
    "    ])\n",
    "    \n",
    "    # Matrix that requires pivoting (first leading minor is zero)\n",
    "    A_bad = torch.tensor([\n",
    "        [0.0, 1.0, 2.0],\n",
    "        [3.0, 4.0, 5.0],\n",
    "        [6.0, 7.0, 8.0]\n",
    "    ])\n",
    "    \n",
    "    # Check existence\n",
    "    exists_good, minors_good = check_lu_existence(A_good)\n",
    "    exists_bad, minors_bad = check_lu_existence(A_bad)\n",
    "    \n",
    "    # Display the matrices\n",
    "    plot_matrix(A_good, \"Matrix with LU Factorization\")\n",
    "    plot_matrix(A_bad, \"Matrix without LU Factorization (Needs Pivoting)\")\n",
    "    \n",
    "    # Show leading principal minors\n",
    "    print(\"Leading Principal Minors for Matrix with LU Factorization:\")\n",
    "    for i, det in enumerate(minors_good):\n",
    "        print(f\"Order {i+1}: {det:.6f}\")\n",
    "    \n",
    "    print(\"\\nLeading Principal Minors for Matrix without LU Factorization:\")\n",
    "    for i, det in enumerate(minors_bad):\n",
    "        print(f\"Order {i+1}: {det:.6f}\")\n",
    "    \n",
    "    # Try to perform LU decomposition\n",
    "    try:\n",
    "        L_good, U_good = scipy.linalg.lu_factor(A_good.numpy())\n",
    "        print(\"\\nLU factorization successful for first matrix\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"\\nLU factorization failed for first matrix\")\n",
    "    \n",
    "    try:\n",
    "        L_bad, U_bad = scipy.linalg.lu_factor(A_bad.numpy())\n",
    "        print(\"LU factorization successful for second matrix (with pivoting)\")\n",
    "    except np.linalg.LinAlgError:\n",
    "        print(\"LU factorization failed for second matrix\")\n",
    "    \n",
    "    return A_good, A_bad\n",
    "\n",
    "# Demonstrate existence conditions for LU factorization\n",
    "A_good, A_bad = demonstrate_lu_existence()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb2b0c45",
   "metadata": {},
   "source": [
    "## 2. Variants of LU Decomposition\n",
    "\n",
    "There are several variants of LU decomposition, each with specific properties and applications:\n",
    "\n",
    "### 2.1 LDU Decomposition\n",
    "\n",
    "LDU decomposition expresses a matrix as $A = LDU$ where:\n",
    "- $L$ is a lower triangular matrix with ones on the diagonal\n",
    "- $D$ is a diagonal matrix\n",
    "- $U$ is an upper triangular matrix with ones on the diagonal\n",
    "\n",
    "This is a more refined form of the LU decomposition that separates the diagonal elements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11c0543b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ldu_decomposition(A):\n",
    "    \"\"\"\n",
    "    Compute LDU decomposition of a matrix A.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "    \n",
    "    Returns:\n",
    "        L: Lower triangular matrix with ones on diagonal\n",
    "        D: Diagonal matrix\n",
    "        U: Upper triangular matrix with ones on diagonal\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Perform LU decomposition first\n",
    "    P, L, U = scipy.linalg.lu(A.numpy())\n",
    "    \n",
    "    # Extract diagonal of U\n",
    "    D = torch.diag(torch.diag(torch.tensor(U, dtype=torch.float64)))\n",
    "    \n",
    "    # Create U with ones on diagonal\n",
    "    U_normalized = torch.tensor(U, dtype=torch.float64) @ torch.linalg.inv(D)\n",
    "    \n",
    "    return torch.tensor(L, dtype=torch.float64), D, U_normalized\n",
    "\n",
    "def demonstrate_ldu():\n",
    "    \"\"\"Demonstrate LDU decomposition.\"\"\"\n",
    "    # Create a simple matrix\n",
    "    A = torch.tensor([\n",
    "        [4.0, 3.0, 2.0],\n",
    "        [3.0, 5.0, 1.0],\n",
    "        [2.0, 1.0, 6.0]\n",
    "    ])\n",
    "    \n",
    "    # Compute LDU decomposition\n",
    "    L, D, U = ldu_decomposition(A)\n",
    "    \n",
    "    # Display the matrices\n",
    "    plot_matrix(A, \"Original Matrix A\")\n",
    "    plot_matrix(L, \"Lower Triangular Matrix L\")\n",
    "    plot_matrix(D, \"Diagonal Matrix D\")\n",
    "    plot_matrix(U, \"Upper Triangular Matrix U\")\n",
    "    \n",
    "    # Verify the decomposition\n",
    "    LDU = L @ D @ U\n",
    "    plot_matrix(LDU, \"Reconstructed Matrix LDU\")\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    error = torch.norm(A - LDU).item()\n",
    "    print(f\"Reconstruction error: {error:.2e}\")\n",
    "    \n",
    "    # Check that L and U have ones on their diagonals\n",
    "    print(\"\\nDiagonal of L:\", torch.diag(L).numpy())\n",
    "    print(\"Diagonal of U:\", torch.diag(U).numpy())\n",
    "    \n",
    "    return A, L, D, U\n",
    "\n",
    "# Demonstrate LDU decomposition\n",
    "A_ldu, L_ldu, D_ldu, U_ldu = demonstrate_ldu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3155232a",
   "metadata": {},
   "source": [
    "### 2.2 PLU Decomposition (with Partial Pivoting)\n",
    "\n",
    "PLU decomposition incorporates row permutations to handle matrices that don't admit a direct LU factorization. It expresses a matrix as:\n",
    "\n",
    "$$PA = LU$$\n",
    "\n",
    "where $P$ is a permutation matrix that reorders the rows of $A$.\n",
    "\n",
    "This variant is more robust and is implemented in most numerical libraries. Let's implement and demonstrate this pivoting process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc9b537",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_with_partial_pivoting(A):\n",
    "    \"\"\"\n",
    "    Perform LU decomposition with partial pivoting.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "    \n",
    "    Returns:\n",
    "        P: Permutation matrix\n",
    "        L: Lower triangular matrix\n",
    "        U: Upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    \n",
    "    # Make a copy to avoid modifying the original matrix\n",
    "    A_work = A.clone()\n",
    "    \n",
    "    # Initialize L as identity matrix\n",
    "    L = torch.eye(n, dtype=A.dtype)\n",
    "    \n",
    "    # Initialize permutation matrix\n",
    "    P = torch.eye(n, dtype=A.dtype)\n",
    "    \n",
    "    # Perform Gaussian elimination\n",
    "    for k in range(n-1):  # Loop through each column\n",
    "        # Find the index of the maximum absolute value in the current column (from k to n)\n",
    "        max_idx = torch.argmax(torch.abs(A_work[k:, k])) + k\n",
    "        \n",
    "        # If the max is not at the current row, swap rows\n",
    "        if max_idx != k:\n",
    "            # Swap rows in A_work\n",
    "            A_work[[k, max_idx], :] = A_work[[max_idx, k], :]\n",
    "            \n",
    "            # Swap rows in P\n",
    "            P[[k, max_idx], :] = P[[max_idx, k], :]\n",
    "            \n",
    "            # Swap rows in L (up to the current column)\n",
    "            if k > 0:\n",
    "                L[[k, max_idx], :k] = L[[max_idx, k], :k]\n",
    "        \n",
    "        # Skip if the current pivot is zero (singular matrix)\n",
    "        if torch.abs(A_work[k, k]) < 1e-12:\n",
    "            continue\n",
    "        \n",
    "        # For each row below the current row\n",
    "        for i in range(k+1, n):\n",
    "            # Compute the multiplier\n",
    "            factor = A_work[i, k] / A_work[k, k]\n",
    "            L[i, k] = factor\n",
    "            \n",
    "            # Update the current row by subtracting the scaled pivot row\n",
    "            A_work[i, k:] -= factor * A_work[k, k:]\n",
    "    \n",
    "    # The resulting A_work is now the upper triangular matrix U\n",
    "    U = A_work\n",
    "    \n",
    "    return P, L, U\n",
    "\n",
    "def demonstrate_plu():\n",
    "    \"\"\"Demonstrate PLU decomposition.\"\"\"\n",
    "    # Use the matrix that requires pivoting\n",
    "    A = torch.tensor([\n",
    "        [0.0, 1.0, 2.0],\n",
    "        [3.0, 4.0, 5.0],\n",
    "        [6.0, 7.0, 8.0]\n",
    "    ])\n",
    "    \n",
    "    # Compute PLU decomposition\n",
    "    P, L, U = lu_with_partial_pivoting(A)\n",
    "    \n",
    "    # Display the matrices\n",
    "    plot_matrix(A, \"Original Matrix A\")\n",
    "    plot_matrix(P, \"Permutation Matrix P\")\n",
    "    plot_matrix(L, \"Lower Triangular Matrix L\")\n",
    "    plot_matrix(U, \"Upper Triangular Matrix U\")\n",
    "    \n",
    "    # Verify that PA = LU\n",
    "    PA = P @ A\n",
    "    LU = L @ U\n",
    "    \n",
    "    plot_matrix(PA, \"PA\")\n",
    "    plot_matrix(LU, \"LU\")\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    error = torch.norm(PA - LU).item()\n",
    "    print(f\"Reconstruction error: {error:.2e}\")\n",
    "    \n",
    "    # Also try with scipy for comparison\n",
    "    P_scipy, L_scipy, U_scipy = scipy.linalg.lu(A.numpy())\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    P_scipy = torch.tensor(P_scipy, dtype=torch.float64)\n",
    "    L_scipy = torch.tensor(L_scipy, dtype=torch.float64)\n",
    "    U_scipy = torch.tensor(U_scipy, dtype=torch.float64)\n",
    "    \n",
    "    # Verify scipy's result\n",
    "    PA_scipy = P_scipy.T @ A\n",
    "    LU_scipy = L_scipy @ U_scipy\n",
    "    \n",
    "    error_scipy = torch.norm(PA_scipy - LU_scipy).item()\n",
    "    print(f\"SciPy reconstruction error: {error_scipy:.2e}\")\n",
    "    \n",
    "    return A, P, L, U\n",
    "\n",
    "# Demonstrate PLU decomposition\n",
    "A_plu, P_plu, L_plu, U_plu = demonstrate_plu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cd3b482",
   "metadata": {},
   "source": [
    "### 2.3 LU Factorization of Rectangular Matrices\n",
    "\n",
    "While we often think of LU decomposition for square matrices, it can be extended to rectangular matrices as well. For an $m \\times n$ matrix with $m \\geq n$, the decomposition is:\n",
    "\n",
    "$$A = LU$$\n",
    "\n",
    "where $L$ is an $m \\times n$ lower triangular matrix, and $U$ is an $n \\times n$ upper triangular matrix.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d6c62e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lu_rectangular(A):\n",
    "    \"\"\"\n",
    "    Compute LU decomposition for a rectangular matrix.\n",
    "    \n",
    "    Args:\n",
    "        A: Input rectangular matrix (m x n) with m >= n\n",
    "        \n",
    "    Returns:\n",
    "        L: m x n lower triangular matrix\n",
    "        U: n x n upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    assert m >= n, \"Matrix must have at least as many rows as columns\"\n",
    "    \n",
    "    # Use existing implementation with partial pivoting\n",
    "    P, L_full, U = lu_with_partial_pivoting(A[:n, :])  # First get square part\n",
    "    \n",
    "    # Now compute the remaining rows of L\n",
    "    if m > n:\n",
    "        L_bottom = torch.zeros((m-n, n), dtype=A.dtype)\n",
    "        A_bottom = P @ A[n:, :]  # Apply permutation to bottom rows\n",
    "        \n",
    "        for i in range(m-n):\n",
    "            for j in range(n):\n",
    "                if j == n-1:\n",
    "                    # Last column just uses the value from A_bottom\n",
    "                    L_bottom[i, j] = A_bottom[i, j]\n",
    "                else:\n",
    "                    # Earlier columns subtract the effect of previous columns\n",
    "                    L_bottom[i, j] = A_bottom[i, j]\n",
    "                    for k in range(j+1, n):\n",
    "                        L_bottom[i, j] -= L_bottom[i, k] * U[j, k]\n",
    "        \n",
    "        # Combine top and bottom parts of L\n",
    "        L = torch.cat([L_full, L_bottom], dim=0)\n",
    "    else:\n",
    "        L = L_full\n",
    "    \n",
    "    return L, U\n",
    "\n",
    "def demonstrate_rectangular_lu():\n",
    "    \"\"\"Demonstrate LU decomposition for rectangular matrices.\"\"\"\n",
    "    # Create a rectangular matrix (more rows than columns)\n",
    "    A = torch.tensor([\n",
    "        [1.0, 2.0, 3.0],\n",
    "        [4.0, 5.0, 6.0],\n",
    "        [7.0, 8.0, 9.0],\n",
    "        [10.0, 11.0, 12.0],\n",
    "        [13.0, 14.0, 15.0]\n",
    "    ])\n",
    "    \n",
    "    # Get dimensions\n",
    "    m, n = A.shape\n",
    "    \n",
    "    # Compute LU decomposition\n",
    "    L, U = lu_rectangular(A)\n",
    "    \n",
    "    # Display the matrices\n",
    "    plot_matrix(A, f\"Original Rectangular Matrix A ({m}x{n})\")\n",
    "    plot_matrix(L, f\"Lower Triangular Matrix L ({m}x{n})\")\n",
    "    plot_matrix(U, f\"Upper Triangular Matrix U ({n}x{n})\")\n",
    "    \n",
    "    # Verify the decomposition\n",
    "    LU = L @ U\n",
    "    plot_matrix(LU, \"Reconstructed Matrix LU\")\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    error = torch.norm(A - LU).item()\n",
    "    print(f\"Reconstruction error: {error:.2e}\")\n",
    "    \n",
    "    return A, L, U\n",
    "\n",
    "# Demonstrate LU decomposition for rectangular matrices\n",
    "A_rect, L_rect, U_rect = demonstrate_rectangular_lu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efeafa9",
   "metadata": {},
   "source": [
    "## 3. Optimized Implementations\n",
    "\n",
    "In practice, LU decomposition is often implemented using optimized techniques for better performance and numerical stability. Let's explore some of these optimizations.\n",
    "\n",
    "### 3.1 Block LU Decomposition\n",
    "\n",
    "Block LU decomposition breaks down a large matrix into smaller blocks, which can be processed more efficiently. For a matrix partitioned as:\n",
    "\n",
    "$$A = \\begin{pmatrix} A_{11} & A_{12} \\\\ A_{21} & A_{22} \\end{pmatrix}$$\n",
    "\n",
    "The block LU decomposition is:\n",
    "\n",
    "$$A = \\begin{pmatrix} L_{11} & 0 \\\\ L_{21} & L_{22} \\end{pmatrix} \\begin{pmatrix} U_{11} & U_{12} \\\\ 0 & U_{22} \\end{pmatrix}$$\n",
    "\n",
    "Let's implement a simple version of block LU decomposition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e95dcae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_lu_decomposition(A, block_size=2):\n",
    "    \"\"\"\n",
    "    Perform block LU decomposition.\n",
    "    \n",
    "    Args:\n",
    "        A: Input square matrix\n",
    "        block_size: Size of the blocks\n",
    "        \n",
    "    Returns:\n",
    "        L: Lower triangular matrix\n",
    "        U: Upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    assert n % block_size == 0, \"Matrix size must be divisible by block_size\"\n",
    "    \n",
    "    # Initialize L and U\n",
    "    L = torch.eye(n, dtype=A.dtype)\n",
    "    U = torch.zeros_like(A)\n",
    "    \n",
    "    # Process the matrix in blocks\n",
    "    for i in range(0, n, block_size):\n",
    "        # Compute U blocks in current block-row\n",
    "        for j in range(0, i, block_size):\n",
    "            U[i:i+block_size, j:j+block_size] = (\n",
    "                A[i:i+block_size, j:j+block_size] - \n",
    "                L[i:i+block_size, :j] @ U[:j, j:j+block_size]\n",
    "            )\n",
    "        \n",
    "        # Compute diagonal U block\n",
    "        U[i:i+block_size, i:i+block_size] = (\n",
    "            A[i:i+block_size, i:i+block_size] - \n",
    "            L[i:i+block_size, :i] @ U[:i, i:i+block_size]\n",
    "        )\n",
    "        \n",
    "        # Compute L blocks in current block-column\n",
    "        for j in range(i+block_size, n, block_size):\n",
    "            L[j:j+block_size, i:i+block_size] = (\n",
    "                A[j:j+block_size, i:i+block_size] - \n",
    "                L[j:j+block_size, :i] @ U[:i, i:i+block_size]\n",
    "            ) @ torch.linalg.inv(U[i:i+block_size, i:i+block_size])\n",
    "        \n",
    "        # Compute U blocks in remaining rows of current block-column\n",
    "        for j in range(i+block_size, n, block_size):\n",
    "            U[i:i+block_size, j:j+block_size] = (\n",
    "                A[i:i+block_size, j:j+block_size] - \n",
    "                L[i:i+block_size, :i] @ U[:i, j:j+block_size]\n",
    "            )\n",
    "    \n",
    "    return L, U\n",
    "\n",
    "def demonstrate_block_lu():\n",
    "    \"\"\"Demonstrate block LU decomposition.\"\"\"\n",
    "    # Create a 4x4 matrix for 2x2 blocks\n",
    "    A = torch.tensor([\n",
    "        [4.0, 1.0, 2.0, 3.0],\n",
    "        [1.0, 5.0, 6.0, 2.0],\n",
    "        [2.0, 6.0, 8.0, 4.0],\n",
    "        [3.0, 2.0, 4.0, 7.0]\n",
    "    ])\n",
    "    \n",
    "    # Compute block LU decomposition\n",
    "    L_block, U_block = block_lu_decomposition(A, block_size=2)\n",
    "    \n",
    "    # For comparison, compute regular LU decomposition\n",
    "    _, L_reg, U_reg = scipy.linalg.lu(A.numpy())\n",
    "    \n",
    "    # Display the matrices\n",
    "    plot_matrix(A, \"Original Matrix A\")\n",
    "    plot_matrix(L_block, \"Block L\")\n",
    "    plot_matrix(U_block, \"Block U\")\n",
    "    \n",
    "    # Verify the decomposition\n",
    "    LU_block = L_block @ U_block\n",
    "    plot_matrix(LU_block, \"Reconstructed Matrix (Block LU)\")\n",
    "    \n",
    "    # Calculate reconstruction error\n",
    "    error_block = torch.norm(A - LU_block).item()\n",
    "    print(f\"Block LU reconstruction error: {error_block:.2e}\")\n",
    "    \n",
    "    # Compare with regular LU\n",
    "    L_reg = torch.tensor(L_reg, dtype=torch.float64)\n",
    "    U_reg = torch.tensor(U_reg, dtype=torch.float64)\n",
    "    LU_reg = L_reg @ U_reg\n",
    "    error_reg = torch.norm(A - LU_reg).item()\n",
    "    print(f\"Regular LU reconstruction error: {error_reg:.2e}\")\n",
    "    \n",
    "    # Compare performance for larger matrices\n",
    "    def time_comparison(n=100, block_size=25):\n",
    "        # Create a larger random matrix\n",
    "        A_large = torch.rand(n, n, dtype=torch.float64)\n",
    "        \n",
    "        # Time block LU\n",
    "        start_time = time.time()\n",
    "        L_block, U_block = block_lu_decomposition(A_large, block_size=block_size)\n",
    "        block_time = time.time() - start_time\n",
    "        \n",
    "        # Time regular LU\n",
    "        start_time = time.time()\n",
    "        _, L_reg, U_reg = scipy.linalg.lu(A_large.numpy())\n",
    "        reg_time = time.time() - start_time\n",
    "        \n",
    "        return block_time, reg_time\n",
    "    \n",
    "    # Compare timing\n",
    "    block_time, reg_time = time_comparison()\n",
    "    print(f\"\\nTiming for 100x100 matrix:\")\n",
    "    print(f\"Block LU time: {block_time:.4f}s\")\n",
    "    print(f\"Regular LU time: {reg_time:.4f}s\")\n",
    "    \n",
    "    return A, L_block, U_block\n",
    "\n",
    "# Demonstrate block LU decomposition\n",
    "A_block, L_block, U_block = demonstrate_block_lu()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95f022a",
   "metadata": {},
   "source": [
    "### 3.2 Memory Layout Optimizations\n",
    "\n",
    "Modern implementations of LU decomposition take advantage of optimized memory layouts for better cache performance. These implementations typically use BLAS (Basic Linear Algebra Subprograms) and LAPACK libraries, which are heavily optimized.\n",
    "\n",
    "Let's discuss the importance of memory layout:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c66fdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_memory_layout():\n",
    "    \"\"\"Demonstrate the importance of memory layout for LU decomposition.\"\"\"\n",
    "    n = 1000\n",
    "    \n",
    "    # Create random matrices in both row-major and column-major layout\n",
    "    A_row_major = np.random.rand(n, n)  # NumPy uses row-major (C-style)\n",
    "    A_col_major = np.asfortranarray(A_row_major)  # Convert to column-major (Fortran-style)\n",
    "    \n",
    "    # Time LU decomposition for row-major layout\n",
    "    start_time = time.time()\n",
    "    LU_row_major = scipy.linalg.lu_factor(A_row_major)\n",
    "    row_major_time = time.time() - start_time\n",
    "    \n",
    "    # Time LU decomposition for column-major layout\n",
    "    start_time = time.time()\n",
    "    LU_col_major = scipy.linalg.lu_factor(A_col_major)\n",
    "    col_major_time = time.time() - start_time\n",
    "    \n",
    "    # Compare timing\n",
    "    print(\"Memory Layout Optimization:\")\n",
    "    print(f\"Row-major (C-style) time: {row_major_time:.4f}s\")\n",
    "    print(f\"Column-major (Fortran-style) time: {col_major_time:.4f}s\")\n",
    "    print(f\"Speedup: {row_major_time / col_major_time:.2f}x\")\n",
    "    \n",
    "    # Plot the timing comparison\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.bar([\"Row-major\", \"Column-major\"], [row_major_time, col_major_time])\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"LU Decomposition Time by Memory Layout\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Note on why column-major is faster for LU\n",
    "    print(\"\\nWhy is column-major layout often faster for LU decomposition?\")\n",
    "    print(\"LU decomposition processes the matrix column by column.\")\n",
    "    print(\"In column-major format, elements in a column are contiguous in memory,\")\n",
    "    print(\"leading to better cache locality and fewer cache misses.\")\n",
    "    \n",
    "    return row_major_time, col_major_time\n",
    "\n",
    "# Skip the actual execution for larger matrices to avoid long computation\n",
    "# But show the conceptual explanation\n",
    "print(\"Memory Layout Optimization:\")\n",
    "print(\"LU decomposition with column-major layout is typically faster\")\n",
    "print(\"because the algorithm processes columns sequentially.\")\n",
    "print(\"When elements in a column are contiguous in memory (column-major),\")\n",
    "print(\"this results in better cache performance.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "673e2236",
   "metadata": {},
   "source": [
    "## 4. Stability and Accuracy Considerations\n",
    "\n",
    "Numerical stability is a critical aspect of LU decomposition. Let's explore some factors that affect the stability and accuracy of the algorithm.\n",
    "\n",
    "### 4.1 Condition Number Analysis\n",
    "\n",
    "The condition number of a matrix affects the numerical stability of LU decomposition and the accuracy of solutions to linear systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "827c4556",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_condition_number():\n",
    "    \"\"\"Analyze the effect of condition number on LU decomposition.\"\"\"\n",
    "    n = 10\n",
    "    \n",
    "    # Create matrices with different condition numbers\n",
    "    condition_numbers = [1e1, 1e3, 1e5, 1e7, 1e9]\n",
    "    errors = []\n",
    "    residuals = []\n",
    "    \n",
    "    for kappa in condition_numbers:\n",
    "        # Create a diagonal matrix with specified condition number\n",
    "        s1 = 1.0\n",
    "        sn = s1 / kappa\n",
    "        S = np.diag(np.linspace(s1, sn, n))\n",
    "        \n",
    "        # Create a random orthogonal matrix\n",
    "        Q, _ = np.linalg.qr(np.random.randn(n, n))\n",
    "        \n",
    "        # Create a matrix with the given condition number\n",
    "        A = Q @ S @ Q.T\n",
    "        \n",
    "        # Create a right-hand side\n",
    "        b = np.random.rand(n)\n",
    "        \n",
    "        # Compute LU decomposition\n",
    "        try:\n",
    "            lu, piv = scipy.linalg.lu_factor(A)\n",
    "            x_lu = scipy.linalg.lu_solve((lu, piv), b)\n",
    "            \n",
    "            # Compute residual ||Ax - b||/||b||\n",
    "            residual = np.linalg.norm(A @ x_lu - b) / np.linalg.norm(b)\n",
    "            residuals.append(residual)\n",
    "            \n",
    "            # Compute error ||x - x_true||/||x_true|| if we know the true solution\n",
    "            # (here we compute x_true using a more accurate method)\n",
    "            x_true = np.linalg.solve(A, b)\n",
    "            error = np.linalg.norm(x_lu - x_true) / np.linalg.norm(x_true)\n",
    "            errors.append(error)\n",
    "        except np.linalg.LinAlgError:\n",
    "            errors.append(np.nan)\n",
    "            residuals.append(np.nan)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.loglog(condition_numbers, errors, 'o-', label='Solution Error')\n",
    "    plt.loglog(condition_numbers, [1e-16 * k for k in condition_numbers], '--', label='Machine Epsilon × Condition Number')\n",
    "    plt.xlabel(\"Condition Number\")\n",
    "    plt.ylabel(\"Relative Error\")\n",
    "    plt.title(\"Solution Error vs. Condition Number\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.loglog(condition_numbers, residuals, 'o-')\n",
    "    plt.xlabel(\"Condition Number\")\n",
    "    plt.ylabel(\"Relative Residual\")\n",
    "    plt.title(\"Residual vs. Condition Number\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Condition Number Analysis:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Condition Number':<20} {'Solution Error':<20} {'Residual':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for kappa, error, residual in zip(condition_numbers, errors, residuals):\n",
    "        print(f\"{kappa:<20.1e} {error:<20.2e} {residual:<20.2e}\")\n",
    "    \n",
    "    print(\"\\nObservation:\")\n",
    "    print(\"The solution error grows approximately linearly with the condition number,\")\n",
    "    print(\"following the theoretical bound: error ≈ machine_epsilon × condition_number.\")\n",
    "\n",
    "# Demonstrate condition number effect\n",
    "analyze_condition_number()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d414df62",
   "metadata": {},
   "source": [
    "### 4.2 Comparison of Pivoting Strategies\n",
    "\n",
    "Different pivoting strategies affect the stability of LU decomposition. Let's compare partial pivoting (which we've used) with complete pivoting:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0264af6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_pivoting_strategies():\n",
    "    \"\"\"Compare different pivoting strategies for LU decomposition.\"\"\"\n",
    "    # Create a challenging matrix\n",
    "    n = 5\n",
    "    A = torch.tensor([\n",
    "        [1e-10, 1.0, 2.0, 3.0, 4.0],\n",
    "        [1.0, 1.0, 2.0, 3.0, 4.0],\n",
    "        [2.0, 2.0, 1.0, 3.0, 4.0],\n",
    "        [3.0, 3.0, 3.0, 1.0, 4.0],\n",
    "        [4.0, 4.0, 4.0, 4.0, 1.0]\n",
    "    ])\n",
    "    \n",
    "    # No pivoting (will likely be unstable)\n",
    "    def lu_no_pivoting(A):\n",
    "        A_np = A.numpy()\n",
    "        n = A_np.shape[0]\n",
    "        L = np.eye(n)\n",
    "        U = A_np.copy()\n",
    "        \n",
    "        for k in range(n-1):\n",
    "            for i in range(k+1, n):\n",
    "                if abs(U[k, k]) < 1e-10:\n",
    "                    return None, None  # Pivot too small\n",
    "                \n",
    "                factor = U[i, k] / U[k, k]\n",
    "                L[i, k] = factor\n",
    "                U[i, k:] -= factor * U[k, k:]\n",
    "        \n",
    "        return L, U\n",
    "    \n",
    "    # Partial pivoting (row exchanges)\n",
    "    P, L_partial, U_partial = scipy.linalg.lu(A.numpy())\n",
    "    \n",
    "    # Calculate growth factor for partial pivoting\n",
    "    growth_partial = np.max(np.abs(L_partial @ U_partial)) / np.max(np.abs(A.numpy()))\n",
    "    \n",
    "    # Try no pivoting\n",
    "    try:\n",
    "        L_no, U_no = lu_no_pivoting(A)\n",
    "        if L_no is None:\n",
    "            print(\"LU without pivoting failed due to a near-zero pivot\")\n",
    "            growth_no = float('inf')\n",
    "        else:\n",
    "            growth_no = np.max(np.abs(L_no @ U_no)) / np.max(np.abs(A.numpy()))\n",
    "    except Exception as e:\n",
    "        print(f\"LU without pivoting failed: {e}\")\n",
    "        growth_no = float('inf')\n",
    "        L_no, U_no = np.eye(n), np.zeros((n, n))\n",
    "    \n",
    "    # Display the matrices\n",
    "    plot_matrix(A, \"Original Matrix A\")\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    try:\n",
    "        sns.heatmap(L_no, annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "        plt.title(\"L (No Pivoting)\")\n",
    "    except:\n",
    "        plt.title(\"L (No Pivoting) - Failed\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    try:\n",
    "        sns.heatmap(U_no, annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "        plt.title(\"U (No Pivoting)\")\n",
    "    except:\n",
    "        plt.title(\"U (No Pivoting) - Failed\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    try:\n",
    "        sns.heatmap(L_no @ U_no, annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "        plt.title(f\"LU (No Pivoting)\\nGrowth Factor: {growth_no:.2e}\")\n",
    "    except:\n",
    "        plt.title(\"LU (No Pivoting) - Failed\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(L_partial, annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "    plt.title(\"L (Partial Pivoting)\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(U_partial, annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "    plt.title(\"U (Partial Pivoting)\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.heatmap(L_partial @ U_partial, annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "    plt.title(f\"LU (Partial Pivoting)\\nGrowth Factor: {growth_partial:.2e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare error in solving linear systems\n",
    "    b = torch.ones(n, dtype=torch.float64)\n",
    "    \n",
    "    try:\n",
    "        # Solve with no pivoting (if it succeeded)\n",
    "        if L_no is not None:\n",
    "            y_no = np.linalg.solve(L_no, b.numpy())\n",
    "            x_no = np.linalg.solve(U_no, y_no)\n",
    "            residual_no = np.linalg.norm(A.numpy() @ x_no - b.numpy()) / np.linalg.norm(b.numpy())\n",
    "        else:\n",
    "            residual_no = float('inf')\n",
    "    except:\n",
    "        residual_no = float('inf')\n",
    "    \n",
    "    # Solve with partial pivoting\n",
    "    lu_partial, piv_partial = scipy.linalg.lu_factor(A.numpy())\n",
    "    x_partial = scipy.linalg.lu_solve((lu_partial, piv_partial), b.numpy())\n",
    "    residual_partial = np.linalg.norm(A.numpy() @ x_partial - b.numpy()) / np.linalg.norm(b.numpy())\n",
    "    \n",
    "    # Print comparison\n",
    "    print(\"Pivoting Strategy Comparison:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Strategy':<20} {'Growth Factor':<20} {'Residual':<20}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'No Pivoting':<20} {growth_no:<20.2e} {residual_no:<20.2e}\")\n",
    "    print(f\"{'Partial Pivoting':<20} {growth_partial:<20.2e} {residual_partial:<20.2e}\")\n",
    "    \n",
    "    print(\"\\nObservation:\")\n",
    "    print(\"Partial pivoting significantly improves numerical stability,\")\n",
    "    print(\"especially for matrices with small pivots in the diagonal.\")\n",
    "    \n",
    "    return A, growth_no, growth_partial, residual_no, residual_partial\n",
    "\n",
    "# Compare pivoting strategies\n",
    "A_piv, growth_no, growth_partial, res_no, res_partial = compare_pivoting_strategies()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a347e4ec",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored the mathematical theory and practical implementations of LU decomposition:\n",
    "\n",
    "1. We examined the conditions for the existence of LU factorization without pivoting.\n",
    "\n",
    "2. We implemented and demonstrated various variants of LU decomposition:\n",
    "   - LDU decomposition (separating the diagonal elements)\n",
    "   - PLU decomposition (with partial pivoting)\n",
    "   - LU factorization for rectangular matrices\n",
    "\n",
    "3. We discussed optimization techniques for LU decomposition:\n",
    "   - Block LU decomposition for better performance\n",
    "   - Memory layout considerations (column-major vs. row-major)\n",
    "\n",
    "4. We analyzed numerical stability aspects:\n",
    "   - The effect of condition number on solution accuracy\n",
    "   - Comparison of different pivoting strategies\n",
    "\n",
    "These insights provide a deeper understanding of LU decomposition beyond the basic algorithm, helping to choose the appropriate variant and implementation for specific applications.\n",
    "\n",
    "In the next notebook, we'll explore practical applications of LU decomposition in various domains.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
