{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d57ba8af",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/11_tensor_operations_einsum`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/11_tensor_operations_einsum/02_advanced_einsum.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff7b79f9",
   "metadata": {},
   "source": [
    "# Advanced Tensor Operations with Einstein Notation\n",
    "\n",
    "In this notebook, we'll explore more advanced tensor operations using Einstein notation. Building on the fundamentals covered in the introduction, we'll dive into complex operations that demonstrate the power and flexibility of Einstein notation for tensor algebra.\n",
    "\n",
    "We'll cover:\n",
    "1. Advanced tensor contractions\n",
    "2. Batch operations with tensors\n",
    "3. Tensor reshaping and dimension manipulation\n",
    "4. Optimizing tensor operations\n",
    "5. Real-world examples of einsum usage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2faf1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "\n",
    "# Set the style for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Helper function to display tensors as heatmaps\n",
    "def plot_tensor_heatmap(tensor, title, is_3d=False):\n",
    "    if is_3d:\n",
    "        fig, axes = plt.subplots(1, tensor.shape[0], figsize=(15, 5))\n",
    "        for i in range(tensor.shape[0]):\n",
    "            im = axes[i].imshow(tensor[i].numpy(), cmap='Blues')\n",
    "            axes[i].set_title(f'Slice {i}')\n",
    "            plt.colorbar(im, ax=axes[i])\n",
    "        fig.suptitle(title, fontsize=16)\n",
    "    else:\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        im = sns.heatmap(tensor.numpy(), annot=True, cmap='Blues', fmt=\".1f\",\n",
    "                cbar_kws={'label': 'Value'})\n",
    "        plt.title(title)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6ec295",
   "metadata": {},
   "source": [
    "## Advanced Tensor Contractions\n",
    "\n",
    "Tensor contraction is the generalization of matrix multiplication to higher-order tensors. When we contract tensors, we sum over one or more pairs of indices.\n",
    "\n",
    "Let's start with some examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab8319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create some tensors for our examples\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                 [4, 5, 6]], dtype=torch.float32)  # Shape (2, 3)\n",
    "\n",
    "B = torch.tensor([[[1, 2], [3, 4], [5, 6]], \n",
    "                 [[7, 8], [9, 10], [11, 12]]], dtype=torch.float32)  # Shape (2, 3, 2)\n",
    "\n",
    "# Print shapes\n",
    "print(\"Tensor A shape:\", A.shape)\n",
    "print(\"Tensor B shape:\", B.shape)\n",
    "\n",
    "# Visualize the tensors\n",
    "plot_tensor_heatmap(A, \"Tensor A (Shape 2x3)\")\n",
    "\n",
    "# For the 3D tensor, plot each slice\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "for i in range(B.shape[0]):\n",
    "    im = axes[i].imshow(B[i].numpy(), cmap='Blues')\n",
    "    axes[i].set_title(f'Tensor B - Slice {i}')\n",
    "    plt.colorbar(im, ax=axes[i])\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0624659d",
   "metadata": {},
   "source": [
    "### Example 1: Contracting a 2D Tensor with a 3D Tensor\n",
    "\n",
    "Let's contract the last dimension of tensor A with the middle dimension of tensor B.\n",
    "In Einstein notation, this would be: $C_{ik} = A_{ij} B_{jik}$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "188c1af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional approach using loops\n",
    "C_traditional = torch.zeros((2, 2), dtype=torch.float32)\n",
    "for i in range(2):\n",
    "    for j in range(3):\n",
    "        for k in range(2):\n",
    "            C_traditional[i, k] += A[i, j] * B[i, j, k]\n",
    "\n",
    "print(\"Traditional result shape:\", C_traditional.shape)\n",
    "print(\"Traditional result:\\n\", C_traditional)\n",
    "\n",
    "# Using einsum\n",
    "C_einsum = torch.einsum('ij,ijk->ik', A, B)\n",
    "print(\"\\nEinsum result shape:\", C_einsum.shape)\n",
    "print(\"Einsum result:\\n\", C_einsum)\n",
    "\n",
    "# Check if they're the same\n",
    "print(\"\\nAre they equal?\", torch.allclose(C_traditional, C_einsum))\n",
    "\n",
    "# Visualize result\n",
    "plot_tensor_heatmap(C_einsum, \"Contraction Result (C_ik = A_ij B_ijk)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0eb411a7",
   "metadata": {},
   "source": [
    "### Example 2: Tensor Trace\n",
    "\n",
    "The trace of a matrix is the sum of its diagonal elements. We can generalize this to higher-order tensors using Einstein notation.\n",
    "\n",
    "For a 2D tensor, the trace is: $\\text{Tr}(A) = A_{ii}$ (summing over repeated index i)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3aaeb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a square tensor\n",
    "square_tensor = torch.tensor([[1, 2, 3], \n",
    "                             [4, 5, 6],\n",
    "                             [7, 8, 9]])\n",
    "\n",
    "# Calculate trace using traditional method\n",
    "trace_traditional = torch.trace(square_tensor)\n",
    "print(\"Traditional trace:\", trace_traditional.item())\n",
    "\n",
    "# Calculate trace using einsum\n",
    "trace_einsum = torch.einsum('ii->', square_tensor)\n",
    "print(\"Einsum trace:\", trace_einsum.item())\n",
    "\n",
    "print(\"Are they equal?\", torch.isclose(trace_traditional, trace_einsum))\n",
    "\n",
    "# Visualize the tensor and highlight the diagonal elements\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(square_tensor.numpy(), annot=True, cmap='Blues', fmt=\".0f\")\n",
    "\n",
    "# Highlight the diagonal\n",
    "mask = np.zeros_like(square_tensor.numpy(), dtype=bool)\n",
    "np.fill_diagonal(mask, True)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(square_tensor.numpy(), annot=True, cmap='Blues', fmt=\".0f\", \n",
    "            mask=~mask, cbar_kws={'label': 'Value'})\n",
    "plt.title('Trace - Sum of Diagonal Elements')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf905f7",
   "metadata": {},
   "source": [
    "### Example 3: Higher-Order Contractions\n",
    "\n",
    "Let's create a 4D tensor and perform a double contraction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58475a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 4D tensor with shape (2, 3, 3, 2)\n",
    "tensor_4d = torch.randint(1, 10, (2, 3, 3, 2)).float()\n",
    "print(\"4D tensor shape:\", tensor_4d.shape)\n",
    "\n",
    "# Display a slice of the 4D tensor\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(121)\n",
    "sns.heatmap(tensor_4d[0, :, :, 0].numpy(), annot=True, cmap='Blues', fmt=\".0f\")\n",
    "plt.title('4D Tensor - Slice [0,:,:,0]')\n",
    "plt.subplot(122)\n",
    "sns.heatmap(tensor_4d[1, :, :, 1].numpy(), annot=True, cmap='Blues', fmt=\".0f\")\n",
    "plt.title('4D Tensor - Slice [1,:,:,1]')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a compatible second tensor\n",
    "tensor_4d_2 = torch.randint(1, 10, (3, 2, 4, 5)).float()\n",
    "print(\"Second 4D tensor shape:\", tensor_4d_2.shape)\n",
    "\n",
    "# Double contraction (contract over the middle two dimensions)\n",
    "result_double_contraction = torch.einsum('ijkl,klmn->ijmn', tensor_4d, tensor_4d_2)\n",
    "print(\"Double contraction result shape:\", result_double_contraction.shape)\n",
    "\n",
    "# Display a slice of the result\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(result_double_contraction[0, 0].numpy(), annot=True, cmap='Blues', fmt=\".0f\")\n",
    "plt.title('Double Contraction Result - Slice [0,0,:,:]')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "854fbd81",
   "metadata": {},
   "source": [
    "## Batch Operations with Einstein Notation\n",
    "\n",
    "One of the strengths of einsum is handling batch operations efficiently. Let's explore some examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca26bf66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of matrices (batch_size, rows, cols)\n",
    "batch_size = 5\n",
    "batch_matrices = torch.randint(1, 10, (batch_size, 3, 4)).float()\n",
    "print(\"Batch matrices shape:\", batch_matrices.shape)\n",
    "\n",
    "# Create another batch with compatible dimensions for matrix multiplication\n",
    "batch_matrices2 = torch.randint(1, 10, (batch_size, 4, 2)).float()\n",
    "print(\"Second batch shape:\", batch_matrices2.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d577e2",
   "metadata": {},
   "source": [
    "### Batch Matrix Multiplication\n",
    "\n",
    "We can use einsum to perform matrix multiplication on every pair of matrices in the batches:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f89c38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Traditional batch matmul using torch.bmm\n",
    "bmm_result = torch.bmm(batch_matrices, batch_matrices2)\n",
    "print(\"BMM result shape:\", bmm_result.shape)\n",
    "\n",
    "# Using einsum for batch matrix multiplication\n",
    "einsum_bmm = torch.einsum('bij,bjk->bik', batch_matrices, batch_matrices2)\n",
    "print(\"Einsum BMM result shape:\", einsum_bmm.shape)\n",
    "\n",
    "# Check if they're the same\n",
    "print(\"Are they equal?\", torch.allclose(bmm_result, einsum_bmm))\n",
    "\n",
    "# Visualize a single example from the batch\n",
    "idx = 0  # Choose the first matrix from the batch\n",
    "print(f\"\\nExample from batch (index {idx}):\")\n",
    "print(\"Input matrix 1:\\n\", batch_matrices[idx])\n",
    "print(\"Input matrix 2:\\n\", batch_matrices2[idx])\n",
    "print(\"Result matrix:\\n\", einsum_bmm[idx])\n",
    "\n",
    "# Visualize the full batch operation\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "axes[0].imshow(batch_matrices[idx].numpy(), cmap='Blues')\n",
    "axes[0].set_title(f'Input Matrix 1\\nBatch {idx}')\n",
    "axes[1].imshow(batch_matrices2[idx].numpy(), cmap='Blues')\n",
    "axes[1].set_title(f'Input Matrix 2\\nBatch {idx}')\n",
    "axes[2].imshow(einsum_bmm[idx].numpy(), cmap='Blues')\n",
    "axes[2].set_title(f'Result\\nBatch {idx}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3859903e",
   "metadata": {},
   "source": [
    "### Batch Diagonal Extraction\n",
    "\n",
    "We can use einsum to extract diagonals from a batch of matrices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce78c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of square matrices\n",
    "batch_square = torch.randint(1, 10, (batch_size, 3, 3)).float()\n",
    "print(\"Batch square matrices shape:\", batch_square.shape)\n",
    "\n",
    "# Extract diagonal for each matrix in the batch\n",
    "batch_diag_traditional = torch.diagonal(batch_square, dim1=1, dim2=2)\n",
    "print(\"Traditional batch diagonal shape:\", batch_diag_traditional.shape)\n",
    "print(\"Traditional batch diagonal:\\n\", batch_diag_traditional)\n",
    "\n",
    "# Using einsum to extract diagonals\n",
    "batch_diag_einsum = torch.einsum('bii->bi', batch_square)\n",
    "print(\"\\nEinsum batch diagonal shape:\", batch_diag_einsum.shape)\n",
    "print(\"Einsum batch diagonal:\\n\", batch_diag_einsum)\n",
    "\n",
    "# Check if they're the same\n",
    "print(\"\\nAre they equal?\", torch.allclose(batch_diag_traditional, batch_diag_einsum))\n",
    "\n",
    "# Visualize diagonal extraction for one matrix in the batch\n",
    "idx = 2  # Choose third matrix from batch\n",
    "fig, ax = plt.subplots(figsize=(8, 6))\n",
    "sns.heatmap(batch_square[idx].numpy(), annot=True, cmap='Blues', fmt=\".0f\")\n",
    "plt.title(f'Matrix from Batch {idx}')\n",
    "\n",
    "# Highlight the diagonal\n",
    "mask = np.zeros_like(batch_square[idx].numpy(), dtype=bool)\n",
    "np.fill_diagonal(mask, True)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(batch_square[idx].numpy(), annot=True, cmap='Blues', fmt=\".0f\", \n",
    "            mask=~mask, cbar_kws={'label': 'Value'})\n",
    "plt.title(f'Diagonal Elements of Matrix {idx}')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e11f297f",
   "metadata": {},
   "source": [
    "## Tensor Reshaping with Einstein Notation\n",
    "\n",
    "Einstein notation can also help with tensor reshaping operations. While PyTorch has dedicated functions like `view` and `permute`, einsum can combine reshaping with computation in a single operation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde25327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor\n",
    "tensor_3d = torch.tensor([[[1, 2], [3, 4]], \n",
    "                         [[5, 6], [7, 8]]])  # Shape (2, 2, 2)\n",
    "print(\"3D tensor shape:\", tensor_3d.shape)\n",
    "print(\"3D tensor:\\n\", tensor_3d)\n",
    "\n",
    "# Traditional transpose using permute\n",
    "transpose_trad = tensor_3d.permute(2, 0, 1)\n",
    "print(\"\\nTransposed tensor using permute, shape:\", transpose_trad.shape)\n",
    "print(\"Transposed tensor:\\n\", transpose_trad)\n",
    "\n",
    "# Using einsum to transpose\n",
    "transpose_einsum = torch.einsum('ijk->kij', tensor_3d)\n",
    "print(\"\\nTransposed tensor using einsum, shape:\", transpose_einsum.shape)\n",
    "print(\"Transposed tensor:\\n\", transpose_einsum)\n",
    "\n",
    "# Check if they're the same\n",
    "print(\"\\nAre they equal?\", torch.allclose(transpose_trad, transpose_einsum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f70c495b",
   "metadata": {},
   "source": [
    "### Flattening Dimensions\n",
    "\n",
    "We can use einsum to flatten specific dimensions of a tensor:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14866a95",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a 3D tensor\n",
    "tensor_3d = torch.tensor([[[1, 2, 3], [4, 5, 6]], \n",
    "                        [[7, 8, 9], [10, 11, 12]]])  # Shape (2, 2, 3)\n",
    "print(\"Original tensor shape:\", tensor_3d.shape)\n",
    "\n",
    "# Flatten last two dimensions using traditional method\n",
    "flatten_trad = tensor_3d.reshape(2, -1)\n",
    "print(\"Traditional flatten shape:\", flatten_trad.shape)\n",
    "print(\"Traditional flatten:\\n\", flatten_trad)\n",
    "\n",
    "# Using view operation as alternative\n",
    "flatten_view = tensor_3d.view(2, -1)\n",
    "print(\"\\nView flatten shape:\", flatten_view.shape)\n",
    "print(\"View flatten:\\n\", flatten_view)\n",
    "\n",
    "print(\"\\nAre they equal?\", torch.allclose(flatten_trad, flatten_view))\n",
    "\n",
    "# Visualize the flattening process\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Original 3D tensor visualization (2 slices)\n",
    "for i in range(tensor_3d.shape[0]):\n",
    "    axes[i].imshow(tensor_3d[i].numpy(), cmap='Blues')\n",
    "    axes[i].set_title(f'Original Tensor - Slice {i}')\n",
    "    \n",
    "    # Add text annotations\n",
    "    for j in range(tensor_3d.shape[1]):\n",
    "        for k in range(tensor_3d.shape[2]):\n",
    "            axes[i].text(k, j, f'{tensor_3d[i,j,k].item():.0f}', \n",
    "                      ha='center', va='center', color='black')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Flattened 2D tensor visualization\n",
    "plt.figure(figsize=(10, 4))\n",
    "sns.heatmap(flatten_view.numpy(), annot=True, cmap='Blues', fmt=\".0f\")\n",
    "plt.title('Flattened Tensor')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f390ad0",
   "metadata": {},
   "source": [
    "## Optimizing Tensor Operations with Einstein Notation\n",
    "\n",
    "One advantage of using einsum is that operations can be more efficient than chaining multiple operations. Let's compare the performance of traditional methods versus einsum:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aac141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create large tensors for performance testing\n",
    "n = 100\n",
    "A = torch.randn(n, n)\n",
    "B = torch.randn(n, n)\n",
    "C = torch.randn(n, n)\n",
    "\n",
    "# Measure time for traditional matrix chain multiplication\n",
    "start_time = time.time()\n",
    "result_traditional = torch.matmul(torch.matmul(A, B), C)\n",
    "traditional_time = time.time() - start_time\n",
    "\n",
    "# Measure time for einsum\n",
    "start_time = time.time()\n",
    "result_einsum = torch.einsum('ij,jk,kl->il', A, B, C)\n",
    "einsum_time = time.time() - start_time\n",
    "\n",
    "print(f\"Traditional chain multiplication time: {traditional_time:.6f} seconds\")\n",
    "print(f\"Einsum time: {einsum_time:.6f} seconds\")\n",
    "print(f\"Speedup: {traditional_time / einsum_time:.2f}x\")\n",
    "print(\"Are results equal?\", torch.allclose(result_traditional, result_einsum, rtol=1e-5))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466832c8",
   "metadata": {},
   "source": [
    "Let's benchmark a few more operations to compare performance:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fed76e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create tensors for benchmarking\n",
    "batch_size = 32\n",
    "A = torch.randn(batch_size, 50, 50)\n",
    "B = torch.randn(batch_size, 50, 50)\n",
    "\n",
    "# 1. Batch diagonal extraction\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    diag_trad = torch.diagonal(A, dim1=1, dim2=2)\n",
    "trad_diag_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    diag_einsum = torch.einsum('bii->bi', A)\n",
    "einsum_diag_time = time.time() - start_time\n",
    "\n",
    "print(\"Diagonal extraction:\")\n",
    "print(f\"Traditional time: {trad_diag_time:.6f} seconds\")\n",
    "print(f\"Einsum time: {einsum_diag_time:.6f} seconds\")\n",
    "print(f\"Ratio: {trad_diag_time / einsum_diag_time:.2f}x\")\n",
    "print()\n",
    "\n",
    "# 2. Batch matrix multiplication\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    result_bmm = torch.bmm(A, B)\n",
    "bmm_time = time.time() - start_time\n",
    "\n",
    "start_time = time.time()\n",
    "for _ in range(100):\n",
    "    result_einsum = torch.einsum('bij,bjk->bik', A, B)\n",
    "einsum_bmm_time = time.time() - start_time\n",
    "\n",
    "print(\"Batch matrix multiplication:\")\n",
    "print(f\"torch.bmm time: {bmm_time:.6f} seconds\")\n",
    "print(f\"Einsum time: {einsum_bmm_time:.6f} seconds\")\n",
    "print(f\"Ratio: {bmm_time / einsum_bmm_time:.2f}x\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad5b78e7",
   "metadata": {},
   "source": [
    "## Real-World Examples of Einstein Notation\n",
    "\n",
    "Now let's explore some real-world applications where Einstein notation is particularly useful.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a23e22a",
   "metadata": {},
   "source": [
    "### Example 1: Attention Mechanism in Transformers\n",
    "\n",
    "Attention mechanisms in transformers involve multiple matrix multiplications. Let's implement a simplified self-attention calculation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c57062fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define tensor dimensions\n",
    "batch_size = 2\n",
    "seq_len = 4\n",
    "d_model = 8\n",
    "num_heads = 2\n",
    "d_head = d_model // num_heads\n",
    "\n",
    "# Create query, key, and value tensors\n",
    "# Shape: (batch_size, seq_len, d_model)\n",
    "queries = torch.randn(batch_size, seq_len, d_model)\n",
    "keys = torch.randn(batch_size, seq_len, d_model)\n",
    "values = torch.randn(batch_size, seq_len, d_model)\n",
    "\n",
    "# Reshape to (batch_size, num_heads, seq_len, d_head)\n",
    "q_reshaped = queries.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "k_reshaped = keys.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "v_reshaped = values.view(batch_size, seq_len, num_heads, d_head).transpose(1, 2)\n",
    "\n",
    "print(\"Q reshaped shape:\", q_reshaped.shape)\n",
    "print(\"K reshaped shape:\", k_reshaped.shape)\n",
    "print(\"V reshaped shape:\", v_reshaped.shape)\n",
    "\n",
    "# Calculate attention scores traditional way\n",
    "# Shape: (batch_size, num_heads, seq_len, seq_len)\n",
    "scores = torch.matmul(q_reshaped, k_reshaped.transpose(-2, -1)) / (d_head ** 0.5)\n",
    "print(\"Attention scores shape:\", scores.shape)\n",
    "\n",
    "# Apply softmax\n",
    "attn_weights = torch.softmax(scores, dim=-1)\n",
    "print(\"Attention weights shape:\", attn_weights.shape)\n",
    "\n",
    "# Apply attention weights to values\n",
    "# Shape: (batch_size, num_heads, seq_len, d_head)\n",
    "attn_output_trad = torch.matmul(attn_weights, v_reshaped)\n",
    "print(\"Attention output shape:\", attn_output_trad.shape)\n",
    "\n",
    "# Reshape back to (batch_size, seq_len, d_model)\n",
    "attn_output_trad = attn_output_trad.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "print(\"Final output shape:\", attn_output_trad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664649a6",
   "metadata": {},
   "source": [
    "Now let's implement the same operation using einsum:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499179ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Einsum approach for self-attention\n",
    "# 1. Calculate attention scores\n",
    "# b=batch, h=head, s/t=sequence, d=dimension\n",
    "scores_einsum = torch.einsum('bhsd,bhtd->bhst', q_reshaped, k_reshaped) / (d_head ** 0.5)\n",
    "print(\"Einsum scores shape:\", scores_einsum.shape)\n",
    "\n",
    "# Apply softmax (no einsum equivalent)\n",
    "attn_weights_einsum = torch.softmax(scores_einsum, dim=-1)\n",
    "\n",
    "# 2. Apply attention weights to values\n",
    "attn_output_einsum = torch.einsum('bhst,bhtd->bhsd', attn_weights_einsum, v_reshaped)\n",
    "print(\"Einsum attention output shape:\", attn_output_einsum.shape)\n",
    "\n",
    "# 3. Reshape back (could use einsum, but view is clearer)\n",
    "attn_output_einsum = attn_output_einsum.transpose(1, 2).contiguous().view(batch_size, seq_len, d_model)\n",
    "print(\"Einsum final output shape:\", attn_output_einsum.shape)\n",
    "\n",
    "# Check if results match\n",
    "print(\"Results match?\", torch.allclose(attn_output_trad, attn_output_einsum, rtol=1e-5))\n",
    "\n",
    "# Visualize attention weights for one head\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(attn_weights[0, 0].detach().numpy(), annot=True, cmap='Blues', fmt=\".2f\", \n",
    "            cbar_kws={'label': 'Attention Weight'})\n",
    "plt.title('Attention Weights for Batch 0, Head 0')\n",
    "plt.xlabel('Key Sequence Position')\n",
    "plt.ylabel('Query Sequence Position')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361f0ed1",
   "metadata": {},
   "source": [
    "### Example 2: Convolutional Neural Networks\n",
    "\n",
    "Convolutions can also be expressed using Einstein notation. Let's implement a basic 2D convolution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010d057f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a batch of 2D images\n",
    "batch_size = 2\n",
    "in_channels = 3\n",
    "height = 5\n",
    "width = 5\n",
    "kernel_size = 3\n",
    "out_channels = 2\n",
    "\n",
    "# Image tensor: [batch, channels, height, width]\n",
    "images = torch.randn(batch_size, in_channels, height, width)\n",
    "print(\"Images shape:\", images.shape)\n",
    "\n",
    "# Kernel tensor: [out_channels, in_channels, kernel_height, kernel_width]\n",
    "kernels = torch.randn(out_channels, in_channels, kernel_size, kernel_size)\n",
    "print(\"Kernels shape:\", kernels.shape)\n",
    "\n",
    "# Traditional 2D convolution using torch.nn.functional\n",
    "import torch.nn.functional as F\n",
    "conv_output_trad = F.conv2d(images, kernels, padding=0)\n",
    "print(\"Traditional conv output shape:\", conv_output_trad.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa23457d",
   "metadata": {},
   "source": [
    "Implementing the same convolution manually for illustration:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abcf44e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual implementation of convolution using for loops\n",
    "\n",
    "# 1. Extract patches from the image\n",
    "patches = torch.zeros(batch_size, in_channels, height-kernel_size+1, width-kernel_size+1, \n",
    "                      kernel_size, kernel_size)\n",
    "\n",
    "# Fill the patches\n",
    "for i in range(height-kernel_size+1):\n",
    "    for j in range(width-kernel_size+1):\n",
    "        patches[:, :, i, j] = images[:, :, i:i+kernel_size, j:j+kernel_size]\n",
    "\n",
    "print(\"Patches shape:\", patches.shape)\n",
    "\n",
    "# 2. Implement manual convolution using loops\n",
    "manual_output = torch.zeros_like(conv_output_trad)\n",
    "for b in range(batch_size):\n",
    "    for o in range(out_channels):\n",
    "        for i in range(height-kernel_size+1):\n",
    "            for j in range(width-kernel_size+1):\n",
    "                for c in range(in_channels):\n",
    "                    for kh in range(kernel_size):\n",
    "                        for kw in range(kernel_size):\n",
    "                            manual_output[b, o, i, j] += images[b, c, i+kh, j+kw] * kernels[o, c, kh, kw]\n",
    "\n",
    "print(\"Manual output shape:\", manual_output.shape)\n",
    "print(\"Manual and traditional results match?\", torch.allclose(conv_output_trad, manual_output, rtol=1e-5))\n",
    "\n",
    "# Visualize one filter and its output\n",
    "filter_idx = 0\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Original image (first channel)\n",
    "plt.subplot(131)\n",
    "plt.imshow(images[0, 0].detach().numpy(), cmap='Blues')\n",
    "plt.title(f'Original Image\\nBatch 0, Channel 0')\n",
    "plt.colorbar()\n",
    "\n",
    "# Filter\n",
    "plt.subplot(132)\n",
    "plt.imshow(kernels[filter_idx, 0].detach().numpy(), cmap='Blues')\n",
    "plt.title(f'Filter {filter_idx}\\nChannel 0')\n",
    "plt.colorbar()\n",
    "\n",
    "# Output feature map\n",
    "plt.subplot(133)\n",
    "plt.imshow(conv_output_trad[0, filter_idx].detach().numpy(), cmap='Blues')\n",
    "plt.title(f'Output Feature Map\\nBatch 0, Filter {filter_idx}')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e58a003f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this notebook, we've explored advanced tensor operations using Einstein notation:\n",
    "\n",
    "1. **Advanced Tensor Contractions**: We've seen how to contract higher-order tensors along multiple dimensions.\n",
    "\n",
    "2. **Batch Operations**: We've used einsum to efficiently process batches of data at once.\n",
    "\n",
    "3. **Tensor Reshaping**: We've demonstrated how einsum can combine reshaping with computation.\n",
    "\n",
    "4. **Performance Optimization**: We've benchmarked einsum against traditional operations.\n",
    "\n",
    "5. **Real-World Applications**: We've implemented attention mechanisms and convolutions using einsum.\n",
    "\n",
    "Einstein notation provides a powerful, concise, and sometimes more efficient way to express complex tensor operations. The ability to clearly specify which dimensions are being operated on makes code easier to read once you're familiar with the notation.\n",
    "\n",
    "In the next notebook, we'll explore practical applications of tensor operations and Einstein notation in various fields such as machine learning, physics, and computer graphics."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
