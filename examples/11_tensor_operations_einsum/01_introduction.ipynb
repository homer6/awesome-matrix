{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df1397e1",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/11_tensor_operations_einsum`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/11_tensor_operations_einsum/01_introduction.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ccc1f0a",
   "metadata": {},
   "source": [
    "# Introduction to Tensor Operations and Einstein Notation\n",
    "\n",
    "In this notebook, we'll explore tensor operations and Einstein notation, which provides a powerful and concise way to express complex tensor operations. Einstein notation allows us to write complex tensor operations in a compact form by implicitly summing over repeated indices.\n",
    "\n",
    "## What are Tensors?\n",
    "\n",
    "Tensors are generalizations of vectors and matrices to higher dimensions:\n",
    "- Scalars are 0-rank tensors (single values)\n",
    "- Vectors are 1-rank tensors (arrays with 1 dimension)\n",
    "- Matrices are 2-rank tensors (arrays with 2 dimensions)\n",
    "- Higher-rank tensors have 3+ dimensions\n",
    "\n",
    "Tensor operations become increasingly complex as dimensionality grows, which is why Einstein notation is so valuable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57fbfe32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style for plots\n",
    "plt.style.use('ggplot')\n",
    "sns.set(style=\"whitegrid\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f4fc17",
   "metadata": {},
   "source": [
    "## Visualizing Tensors of Different Ranks\n",
    "\n",
    "Let's visualize tensors of different ranks to build intuition:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57cd473",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scalar (rank 0)\n",
    "scalar = torch.tensor(5.0)\n",
    "print(f\"Scalar (rank 0):\")\n",
    "print(f\"Value: {scalar.item()}\")\n",
    "print(f\"Shape: {scalar.shape}\")\n",
    "print()\n",
    "\n",
    "# Vector (rank 1)\n",
    "vector = torch.tensor([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "print(f\"Vector (rank 1):\")\n",
    "print(f\"Values: {vector}\")\n",
    "print(f\"Shape: {vector.shape}\")\n",
    "print()\n",
    "\n",
    "# Matrix (rank 2)\n",
    "matrix = torch.tensor([[1.0, 2.0, 3.0], \n",
    "                       [4.0, 5.0, 6.0],\n",
    "                       [7.0, 8.0, 9.0]])\n",
    "print(f\"Matrix (rank 2):\")\n",
    "print(f\"Values:\\n{matrix}\")\n",
    "print(f\"Shape: {matrix.shape}\")\n",
    "print()\n",
    "\n",
    "# 3D tensor (rank 3)\n",
    "tensor_3d = torch.tensor([[[1.0, 2.0], [3.0, 4.0]],\n",
    "                         [[5.0, 6.0], [7.0, 8.0]]])\n",
    "print(f\"3D Tensor (rank 3):\")\n",
    "print(f\"Values:\\n{tensor_3d}\")\n",
    "print(f\"Shape: {tensor_3d.shape}\")\n",
    "print()\n",
    "\n",
    "# 4D tensor (rank 4) - Mini-batch of images (batch, channels, height, width)\n",
    "tensor_4d = torch.rand(2, 3, 4, 4)  # 2 samples, 3 channels, 4x4 images\n",
    "print(f\"4D Tensor (rank 4):\")\n",
    "print(f\"Shape: {tensor_4d.shape}\")\n",
    "print(f\"Number of elements: {tensor_4d.numel()}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b984e2",
   "metadata": {},
   "source": [
    "Let's visualize our vector, matrix, and 3D tensor to better understand their structure:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c47207",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot vector\n",
    "plt.figure(figsize=(10, 3))\n",
    "plt.bar(range(len(vector)), vector.numpy(), color='skyblue')\n",
    "plt.xticks(range(len(vector)))\n",
    "plt.title('Vector (Rank 1 Tensor)')\n",
    "plt.xlabel('Index')\n",
    "plt.ylabel('Value')\n",
    "plt.show()\n",
    "\n",
    "# Plot matrix as heatmap\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(matrix.numpy(), annot=True, cmap='Blues', fmt=\".1f\", \n",
    "            cbar_kws={'label': 'Value'})\n",
    "plt.title('Matrix (Rank 2 Tensor)')\n",
    "plt.xlabel('Column Index')\n",
    "plt.ylabel('Row Index')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot 3D tensor\n",
    "fig = plt.figure(figsize=(12, 5))\n",
    "\n",
    "# First slice\n",
    "ax1 = fig.add_subplot(121)\n",
    "sns.heatmap(tensor_3d[0].numpy(), annot=True, cmap='Blues', fmt=\".1f\",\n",
    "            cbar_kws={'label': 'Value'})\n",
    "ax1.set_title('3D Tensor - Slice 0')\n",
    "ax1.set_xlabel('Column Index')\n",
    "ax1.set_ylabel('Row Index')\n",
    "\n",
    "# Second slice\n",
    "ax2 = fig.add_subplot(122)\n",
    "sns.heatmap(tensor_3d[1].numpy(), annot=True, cmap='Blues', fmt=\".1f\",\n",
    "            cbar_kws={'label': 'Value'})\n",
    "ax2.set_title('3D Tensor - Slice 1')\n",
    "ax2.set_xlabel('Column Index')\n",
    "ax2.set_ylabel('Row Index')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd71c2f9",
   "metadata": {},
   "source": [
    "## Tensor Indexing and Slicing\n",
    "\n",
    "Understanding how to access elements in tensors is fundamental. Let's practice with some examples:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37cf3fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using our previously defined tensors\n",
    "print(\"Vector element at index 2:\", vector[2])\n",
    "print(\"Matrix element at row 1, col 2:\", matrix[1, 2])\n",
    "print(\"3D tensor element at indices (1,0,1):\", tensor_3d[1, 0, 1])\n",
    "print()\n",
    "\n",
    "# Slicing examples\n",
    "print(\"Vector slice [1:4]:\", vector[1:4])\n",
    "print()\n",
    "\n",
    "print(\"Matrix row 1:\", matrix[1])\n",
    "print(\"Matrix column 1:\", matrix[:, 1])\n",
    "print(\"Matrix slice [0:2, 1:3]:\\n\", matrix[0:2, 1:3])\n",
    "print()\n",
    "\n",
    "print(\"3D tensor slice [0, :, :]:\\n\", tensor_3d[0, :, :])\n",
    "print(\"3D tensor slice [:, 1, :]:\\n\", tensor_3d[:, 1, :])\n",
    "print(\"3D tensor slice [:, :, 0]:\\n\", tensor_3d[:, :, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a0659a",
   "metadata": {},
   "source": [
    "## Basic Tensor Operations\n",
    "\n",
    "Before we dive into Einstein notation, let's review basic tensor operations that we can express more concisely with Einstein notation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a5cb31",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Element-wise operations\n",
    "a = torch.tensor([1, 2, 3, 4])\n",
    "b = torch.tensor([5, 6, 7, 8])\n",
    "\n",
    "# Addition\n",
    "print(\"Element-wise addition:\\n\", a + b)\n",
    "\n",
    "# Multiplication (Hadamard product)\n",
    "print(\"Element-wise multiplication:\\n\", a * b)\n",
    "\n",
    "# Matrix multiplication\n",
    "A = torch.tensor([[1, 2], [3, 4]])\n",
    "B = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "# Matrix multiplication using matmul\n",
    "C = torch.matmul(A, B)\n",
    "print(\"\\nMatrix multiplication (A @ B):\\n\", C)\n",
    "\n",
    "# Matrix-vector multiplication\n",
    "v = torch.tensor([1, 2])\n",
    "result = torch.matmul(A, v)\n",
    "print(\"\\nMatrix-vector multiplication (A @ v):\\n\", result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "633efe06",
   "metadata": {},
   "source": [
    "## Introduction to Einstein Notation\n",
    "\n",
    "Einstein notation (or Einstein summation) is a concise way to express operations on multi-dimensional arrays. The key idea is that repeated indices in a term imply summation over that index.\n",
    "\n",
    "### Basic Rules:\n",
    "\n",
    "1. Repeated indices are implicitly summed over\n",
    "2. Each index can appear at most twice in any term\n",
    "3. Each un-repeated index appears on both sides of the equation\n",
    "\n",
    "Let's see some examples of how traditional operations can be expressed in Einstein notation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63e84b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's first define some tensors for our examples\n",
    "x = torch.tensor([1, 2, 3])\n",
    "y = torch.tensor([4, 5, 6])\n",
    "\n",
    "A = torch.tensor([[1, 2, 3], \n",
    "                 [4, 5, 6],\n",
    "                 [7, 8, 9]])\n",
    "\n",
    "B = torch.tensor([[9, 8, 7], \n",
    "                 [6, 5, 4],\n",
    "                 [3, 2, 1]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f271d45e",
   "metadata": {},
   "source": [
    "### Example 1: Vector Dot Product\n",
    "\n",
    "A dot product between vectors x and y can be written as:\n",
    "\n",
    "Traditional notation: $\\sum_{i} x_i y_i$\n",
    "\n",
    "Einstein notation: $x_i y_i$ (the repeated index i implies summation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615b5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vector dot product using traditional method\n",
    "dot_product_traditional = torch.dot(x, y)\n",
    "print(\"Dot product (traditional):\", dot_product_traditional.item())\n",
    "\n",
    "# Using Einstein notation with torch.einsum\n",
    "dot_product_einsum = torch.einsum('i,i->', x, y)\n",
    "print(\"Dot product (einsum): \", dot_product_einsum.item())\n",
    "\n",
    "# Verify they're the same\n",
    "print(\"Are they equal?\", torch.isclose(dot_product_traditional, dot_product_einsum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c933c6a5",
   "metadata": {},
   "source": [
    "### Example 2: Matrix-Vector Multiplication\n",
    "\n",
    "Matrix-vector multiplication can be written as:\n",
    "\n",
    "Traditional notation: $\\sum_{j} A_{ij} x_j$\n",
    "\n",
    "Einstein notation: $A_{ij} x_j$ (the repeated index j implies summation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0025af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix-vector multiplication using traditional method\n",
    "mv_traditional = torch.matmul(A, x)\n",
    "print(\"Matrix-vector product (traditional):\\n\", mv_traditional)\n",
    "\n",
    "# Using Einstein notation\n",
    "mv_einsum = torch.einsum('ij,j->i', A, x)\n",
    "print(\"\\nMatrix-vector product (einsum):\\n\", mv_einsum)\n",
    "\n",
    "# Verify they're the same\n",
    "print(\"\\nAre they equal?\", torch.allclose(mv_traditional, mv_einsum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8703afc",
   "metadata": {},
   "source": [
    "### Example 3: Matrix Multiplication\n",
    "\n",
    "Matrix multiplication can be written as:\n",
    "\n",
    "Traditional notation: $C_{ik} = \\sum_{j} A_{ij} B_{jk}$\n",
    "\n",
    "Einstein notation: $C_{ik} = A_{ij} B_{jk}$ (the repeated index j implies summation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26addc13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrix multiplication using traditional method\n",
    "mm_traditional = torch.matmul(A, B)\n",
    "print(\"Matrix multiplication (traditional):\\n\", mm_traditional)\n",
    "\n",
    "# Using Einstein notation\n",
    "mm_einsum = torch.einsum('ij,jk->ik', A, B)\n",
    "print(\"\\nMatrix multiplication (einsum):\\n\", mm_einsum)\n",
    "\n",
    "# Verify they're the same\n",
    "print(\"\\nAre they equal?\", torch.allclose(mm_traditional, mm_einsum))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b252cbb1",
   "metadata": {},
   "source": [
    "## Visualizing Einstein Notation Operations\n",
    "\n",
    "To build intuition, let's visualize what happens in Einstein notation for matrix multiplication:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14ae41d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a visual explanation of matrix multiplication with einsum\n",
    "fig, axs = plt.subplots(1, 3, figsize=(18, 6))\n",
    "\n",
    "# Visualize matrix A\n",
    "im1 = axs[0].imshow(A.numpy(), cmap='Blues')\n",
    "axs[0].set_title('Matrix A (ij)')\n",
    "axs[0].set_xlabel('j')\n",
    "axs[0].set_ylabel('i')\n",
    "plt.colorbar(im1, ax=axs[0])\n",
    "\n",
    "for i in range(A.shape[0]):\n",
    "    for j in range(A.shape[1]):\n",
    "        axs[0].text(j, i, f'{A[i, j].item():.0f}', \n",
    "                   ha='center', va='center', color='black')\n",
    "\n",
    "# Visualize matrix B\n",
    "im2 = axs[1].imshow(B.numpy(), cmap='Blues')\n",
    "axs[1].set_title('Matrix B (jk)')\n",
    "axs[1].set_xlabel('k')\n",
    "axs[1].set_ylabel('j')\n",
    "plt.colorbar(im2, ax=axs[1])\n",
    "\n",
    "for j in range(B.shape[0]):\n",
    "    for k in range(B.shape[1]):\n",
    "        axs[1].text(k, j, f'{B[j, k].item():.0f}', \n",
    "                   ha='center', va='center', color='black')\n",
    "\n",
    "# Visualize result C = A @ B\n",
    "C = torch.matmul(A, B)\n",
    "im3 = axs[2].imshow(C.numpy(), cmap='Blues')\n",
    "axs[2].set_title('Result C (ik): A(ij) B(jk) -> C(ik)')\n",
    "axs[2].set_xlabel('k')\n",
    "axs[2].set_ylabel('i')\n",
    "plt.colorbar(im3, ax=axs[2])\n",
    "\n",
    "for i in range(C.shape[0]):\n",
    "    for k in range(C.shape[1]):\n",
    "        axs[2].text(k, i, f'{C[i, k].item():.0f}', \n",
    "                   ha='center', va='center', color='black')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7098b557",
   "metadata": {},
   "source": [
    "## Understanding einsum Notation\n",
    "\n",
    "In `torch.einsum`, the notation works as follows:\n",
    "\n",
    "`torch.einsum(equation, *operands)` where:\n",
    "- `equation` is a string describing the operation\n",
    "- `operands` are the tensors involved in the operation\n",
    "\n",
    "The equation has the format: `subscripts->output` where:\n",
    "- `subscripts` describes the dimensions of the input tensors\n",
    "- `->` separates inputs from output\n",
    "- `output` describes the dimensions of the output tensor\n",
    "\n",
    "For example, in `ij,jk->ik`:\n",
    "- `ij` represents the dimensions of the first tensor (a matrix)\n",
    "- `jk` represents the dimensions of the second tensor (another matrix)\n",
    "- `->ik` indicates that the output will have dimensions corresponding to i and k\n",
    "- The repeated index `j` is summed over\n",
    "\n",
    "Let's visualize one more example to reinforce understanding:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5dffda1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outer product example\n",
    "outer_product = torch.einsum('i,j->ij', x, y)\n",
    "print(\"Outer product shape:\", outer_product.shape)\n",
    "print(\"Outer product:\\n\", outer_product)\n",
    "\n",
    "# Visualize outer product\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(outer_product.numpy(), annot=True, cmap='Blues', fmt=\".0f\",\n",
    "            cbar_kws={'label': 'Value'})\n",
    "plt.title('Outer Product (x_i y_j -> result_ij)')\n",
    "plt.xlabel('j (index of y)')\n",
    "plt.ylabel('i (index of x)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "804cdb80",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "In this introduction, we've:\n",
    "\n",
    "1. Explored tensors of different ranks and their visualizations\n",
    "2. Learned about basic tensor operations and indexing\n",
    "3. Introduced Einstein notation and its basic rules\n",
    "4. Demonstrated how to use `torch.einsum` for various operations\n",
    "5. Visualized operations to build intuition\n",
    "\n",
    "In the next notebook, we'll dive deeper into more complex tensor operations using Einstein notation and explore real-world applications."
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
