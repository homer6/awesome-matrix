{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ddd7bf23",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/08_qr_decomposition`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/08_qr_decomposition/01_introduction.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818c4b29",
   "metadata": {},
   "source": [
    "# QR Decomposition: Introduction\n",
    "\n",
    "QR decomposition is a fundamental matrix factorization method that decomposes a matrix into the product of an orthogonal matrix (Q) and an upper triangular matrix (R). This decomposition has numerous applications in numerical linear algebra, particularly for solving linear systems, finding eigenvalues, and least squares problems.\n",
    "\n",
    "In this notebook, we will:\n",
    "\n",
    "1. Understand the concept of QR decomposition\n",
    "2. Implement QR decomposition using the Gram-Schmidt process\n",
    "3. Visualize the decomposition geometrically\n",
    "4. Compare with built-in QR decomposition functions\n",
    "5. Explore the numerical stability of different QR algorithms\n",
    "\n",
    "QR decomposition is particularly valuable because the orthogonality of Q preserves vector norms and angles, making it numerically stable for many applications.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1de8434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.linalg\n",
    "import time\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# For better looking plots\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom colormap (light blue to dark blue)\n",
    "colors = [(0.95, 0.95, 1), (0.0, 0.2, 0.6)]\n",
    "blue_cmap = LinearSegmentedColormap.from_list('CustomBlue', colors, N=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc02dec9",
   "metadata": {},
   "source": [
    "## Basic Concept of QR Decomposition\n",
    "\n",
    "For a matrix $A \\in \\mathbb{R}^{m \\times n}$ with $m \\geq n$, the QR decomposition finds matrices $Q$ and $R$ such that:\n",
    "\n",
    "$A = QR$\n",
    "\n",
    "where:\n",
    "- $Q \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix ($Q^T Q = I$)\n",
    "- $R \\in \\mathbb{R}^{m \\times n}$ is an upper triangular matrix with zeros below the diagonal\n",
    "\n",
    "In the case where $m > n$, we often use the \"economy\" or \"reduced\" QR decomposition, where:\n",
    "- $Q \\in \\mathbb{R}^{m \\times n}$ has orthonormal columns\n",
    "- $R \\in \\mathbb{R}^{n \\times n}$ is square upper triangular\n",
    "\n",
    "Let's start by creating an example matrix to decompose:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8ff1fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_example_matrix(m=4, n=3, method=\"random\"):\n",
    "    \"\"\"Create a matrix for QR decomposition demonstration.\"\"\"\n",
    "    if method == \"random\":\n",
    "        # Create a random matrix\n",
    "        A = torch.rand(m, n) * 10 - 5  # Values between -5 and 5\n",
    "    elif method == \"simple\":\n",
    "        # Simple predefined matrix for clear demonstration\n",
    "        if m == 3 and n == 2:\n",
    "            A = torch.tensor([\n",
    "                [12.0, 6.0],\n",
    "                [3.0, 4.0],\n",
    "                [4.0, 8.0]\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(f\"Simple method doesn't support dimensions {m}x{n}\")\n",
    "    else:\n",
    "        raise ValueError(\"Unknown method\")\n",
    "    \n",
    "    return A\n",
    "\n",
    "# Create a simple example matrix\n",
    "A_simple = create_example_matrix(m=3, n=2, method=\"simple\")\n",
    "\n",
    "# Display the matrix\n",
    "def plot_matrix(matrix, title=\"Matrix\", annotate=True, cmap=blue_cmap):\n",
    "    \"\"\"Plot a matrix as a heatmap with annotations.\"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix_np = matrix.numpy()\n",
    "    else:\n",
    "        matrix_np = matrix\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(matrix_np, annot=annotate, fmt=\".2f\", cmap=cmap, \n",
    "                    linewidths=1, cbar=True)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add row and column indices\n",
    "    ax.set_xticks(np.arange(matrix_np.shape[1]) + 0.5)\n",
    "    ax.set_yticks(np.arange(matrix_np.shape[0]) + 0.5)\n",
    "    ax.set_xticklabels([f\"Col {i+1}\" for i in range(matrix_np.shape[1])])\n",
    "    ax.set_yticklabels([f\"Row {i+1}\" for i in range(matrix_np.shape[0])])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "plot_matrix(A_simple, \"Example Matrix A (3×2)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be65e235",
   "metadata": {},
   "source": [
    "## QR Decomposition using Gram-Schmidt Process\n",
    "\n",
    "The classic method for computing QR decomposition is the Gram-Schmidt orthogonalization process, which transforms a set of linearly independent vectors into an orthogonal or orthonormal set.\n",
    "\n",
    "Here's how it works:\n",
    "\n",
    "1. Start with columns of matrix $A$: $\\{a_1, a_2, ..., a_n\\}$\n",
    "2. Compute orthogonal vectors $\\{u_1, u_2, ..., u_n\\}$:\n",
    "   - $u_1 = a_1$\n",
    "   - $u_2 = a_2 - \\text{proj}_{u_1}(a_2)$\n",
    "   - $u_3 = a_3 - \\text{proj}_{u_1}(a_3) - \\text{proj}_{u_2}(a_3)$\n",
    "   - ...\n",
    "3. Normalize to get orthonormal vectors $\\{q_1, q_2, ..., q_n\\}$:\n",
    "   - $q_i = \\frac{u_i}{||u_i||}$\n",
    "4. Form the matrix $Q = [q_1, q_2, ..., q_n]$\n",
    "5. Compute $R$ such that $A = QR$, which gives us:\n",
    "   - $R_{ij} = q_i^T a_j$ for $i \\leq j$\n",
    "   - $R_{ij} = 0$ for $i > j$\n",
    "\n",
    "Let's implement this process:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8c15145",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    Compute QR decomposition using the Gram-Schmidt process.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "    R = torch.zeros((n, n), dtype=A.dtype)\n",
    "    \n",
    "    for j in range(n):\n",
    "        # Get the j-th column vector\n",
    "        v = A[:, j].clone()\n",
    "        \n",
    "        # Orthogonalize with respect to previous columns of Q\n",
    "        for i in range(j):\n",
    "            # Project onto previous orthogonal vectors\n",
    "            R[i, j] = torch.dot(Q[:, i], A[:, j])\n",
    "            v = v - R[i, j] * Q[:, i]\n",
    "        \n",
    "        # Compute the norm of the orthogonalized vector\n",
    "        R[j, j] = torch.norm(v)\n",
    "        \n",
    "        # Normalize to get an orthonormal vector\n",
    "        if R[j, j] > 1e-10:  # Check for numerical stability\n",
    "            Q[:, j] = v / R[j, j]\n",
    "        else:\n",
    "            Q[:, j] = torch.zeros(m, dtype=A.dtype)\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "# Test the QR decomposition on our example matrix\n",
    "Q_simple, R_simple = qr_gram_schmidt(A_simple)\n",
    "\n",
    "# Display the results\n",
    "plot_matrix(Q_simple, \"Orthogonal Matrix Q\")\n",
    "plot_matrix(R_simple, \"Upper Triangular Matrix R\")\n",
    "\n",
    "# Check that A = QR\n",
    "A_reconstructed = Q_simple @ R_simple\n",
    "plot_matrix(A_reconstructed, \"Reconstructed Matrix (Q×R)\")\n",
    "\n",
    "# Calculate reconstruction error\n",
    "reconstruction_error = torch.norm(A_simple - A_reconstructed).item()\n",
    "print(f\"Reconstruction error: {reconstruction_error:.2e}\")\n",
    "\n",
    "# Verify that Q is orthogonal (Q^T Q = I)\n",
    "Q_orthogonality = Q_simple.T @ Q_simple\n",
    "plot_matrix(Q_orthogonality, \"Q^T Q (should be identity)\")\n",
    "\n",
    "orthogonality_error = torch.norm(Q_orthogonality - torch.eye(Q_orthogonality.shape[0])).item()\n",
    "print(f\"Orthogonality error: {orthogonality_error:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca127376",
   "metadata": {},
   "source": [
    "### Visualizing Gram-Schmidt Orthogonalization\n",
    "\n",
    "To better understand how the Gram-Schmidt process works, let's visualize it in a 2D or 3D space. We'll show how each column of A gets transformed into orthogonal vectors:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc453b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gram_schmidt_2d():\n",
    "    \"\"\"Visualize Gram-Schmidt process in 2D.\"\"\"\n",
    "    # Create a simple 2×2 matrix\n",
    "    A = torch.tensor([[3.0, 2.0],\n",
    "                      [1.0, 2.0]])\n",
    "    \n",
    "    # Extract columns\n",
    "    a1 = A[:, 0].numpy()\n",
    "    a2 = A[:, 1].numpy()\n",
    "    \n",
    "    # Apply Gram-Schmidt\n",
    "    u1 = a1.copy()  # First vector is unchanged\n",
    "    q1 = u1 / np.linalg.norm(u1)  # Normalize\n",
    "    \n",
    "    # Orthogonalize second vector\n",
    "    proj = np.dot(a2, q1) * q1\n",
    "    u2 = a2 - proj\n",
    "    q2 = u2 / np.linalg.norm(u2)\n",
    "    \n",
    "    # Compute R matrix components\n",
    "    r11 = np.linalg.norm(u1)\n",
    "    r12 = np.dot(q1, a2)\n",
    "    r22 = np.linalg.norm(u2)\n",
    "    \n",
    "    # Create the plot\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Original vectors\n",
    "    plt.arrow(0, 0, a1[0], a1[1], head_width=0.2, head_length=0.3, fc='blue', ec='blue', label='a1')\n",
    "    plt.arrow(0, 0, a2[0], a2[1], head_width=0.2, head_length=0.3, fc='red', ec='red', label='a2')\n",
    "    \n",
    "    # Orthogonal vectors\n",
    "    plt.arrow(0, 0, q1[0], q1[1], head_width=0.2, head_length=0.3, fc='green', ec='green', label='q1')\n",
    "    plt.arrow(0, 0, q2[0], q2[1], head_width=0.2, head_length=0.3, fc='purple', ec='purple', label='q2')\n",
    "    \n",
    "    # Projection\n",
    "    plt.arrow(0, 0, proj[0], proj[1], head_width=0.2, head_length=0.3, fc='orange', ec='orange', \n",
    "              linestyle='dashed', label='proj_q1(a2)')\n",
    "    \n",
    "    # Draw the projection line\n",
    "    plt.plot([a2[0], proj[0]], [a2[1], proj[1]], 'k--', alpha=0.7)\n",
    "    \n",
    "    plt.axis('equal')\n",
    "    plt.xlim(-1, 4)\n",
    "    plt.ylim(-1, 4)\n",
    "    plt.title('Gram-Schmidt Process in 2D')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Show the resulting matrices\n",
    "    Q = np.column_stack([q1, q2])\n",
    "    R = np.array([[r11, r12],\n",
    "                  [0, r22]])\n",
    "    \n",
    "    print(\"Original Matrix A:\")\n",
    "    print(A.numpy())\n",
    "    print(\"\\nOrthogonal Matrix Q:\")\n",
    "    print(Q)\n",
    "    print(\"\\nUpper Triangular Matrix R:\")\n",
    "    print(R)\n",
    "    print(\"\\nReconstruction A = QR:\")\n",
    "    print(Q @ R)\n",
    "    \n",
    "    return A.numpy(), Q, R\n",
    "\n",
    "# Visualize Gram-Schmidt in 2D\n",
    "A_2d, Q_2d, R_2d = visualize_gram_schmidt_2d()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de805d22",
   "metadata": {},
   "source": [
    "### Visualizing Gram-Schmidt in 3D\n",
    "\n",
    "Now let's visualize the Gram-Schmidt process for a 3×3 matrix in 3D space:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a537334b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_gram_schmidt_3d():\n",
    "    \"\"\"Visualize Gram-Schmidt process in 3D.\"\"\"\n",
    "    # Create a simple 3×3 matrix\n",
    "    A = torch.tensor([[3.0, 1.0, 1.0],\n",
    "                      [1.0, 2.0, 0.0],\n",
    "                      [1.0, 1.0, 2.0]])\n",
    "    \n",
    "    # Extract columns\n",
    "    a1 = A[:, 0].numpy()\n",
    "    a2 = A[:, 1].numpy()\n",
    "    a3 = A[:, 2].numpy()\n",
    "    \n",
    "    # Apply Gram-Schmidt\n",
    "    u1 = a1.copy()\n",
    "    q1 = u1 / np.linalg.norm(u1)\n",
    "    \n",
    "    # Second vector\n",
    "    proj1 = np.dot(a2, q1) * q1\n",
    "    u2 = a2 - proj1\n",
    "    q2 = u2 / np.linalg.norm(u2)\n",
    "    \n",
    "    # Third vector\n",
    "    proj1_3 = np.dot(a3, q1) * q1\n",
    "    proj2_3 = np.dot(a3, q2) * q2\n",
    "    u3 = a3 - proj1_3 - proj2_3\n",
    "    q3 = u3 / np.linalg.norm(u3)\n",
    "    \n",
    "    # Create 3D plot\n",
    "    fig = plt.figure(figsize=(12, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Draw original vectors\n",
    "    ax.quiver(0, 0, 0, a1[0], a1[1], a1[2], color='blue', label='a1')\n",
    "    ax.quiver(0, 0, 0, a2[0], a2[1], a2[2], color='red', label='a2')\n",
    "    ax.quiver(0, 0, 0, a3[0], a3[1], a3[2], color='darkred', label='a3')\n",
    "    \n",
    "    # Draw orthogonal vectors\n",
    "    ax.quiver(0, 0, 0, q1[0], q1[1], q1[2], color='green', label='q1')\n",
    "    ax.quiver(0, 0, 0, q2[0], q2[1], q2[2], color='purple', label='q2')\n",
    "    ax.quiver(0, 0, 0, q3[0], q3[1], q3[2], color='orange', label='q3')\n",
    "    \n",
    "    ax.set_xlim([-1, 3])\n",
    "    ax.set_ylim([-1, 3])\n",
    "    ax.set_zlim([-1, 3])\n",
    "    ax.set_xlabel('X')\n",
    "    ax.set_ylabel('Y')\n",
    "    ax.set_zlabel('Z')\n",
    "    ax.set_title('Gram-Schmidt Process in 3D')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compute R matrix\n",
    "    r11 = np.linalg.norm(u1)\n",
    "    r12 = np.dot(q1, a2)\n",
    "    r13 = np.dot(q1, a3)\n",
    "    r22 = np.linalg.norm(u2)\n",
    "    r23 = np.dot(q2, a3)\n",
    "    r33 = np.linalg.norm(u3)\n",
    "    \n",
    "    # Show the resulting matrices\n",
    "    Q = np.column_stack([q1, q2, q3])\n",
    "    R = np.array([[r11, r12, r13],\n",
    "                  [0, r22, r23],\n",
    "                  [0, 0, r33]])\n",
    "    \n",
    "    print(\"Original Matrix A:\")\n",
    "    print(A.numpy())\n",
    "    print(\"\\nOrthogonal Matrix Q:\")\n",
    "    print(Q)\n",
    "    print(\"\\nUpper Triangular Matrix R:\")\n",
    "    print(R)\n",
    "    print(\"\\nReconstruction A = QR:\")\n",
    "    print(Q @ R)\n",
    "    \n",
    "    # Calculate orthogonality of Q\n",
    "    QTQ = Q.T @ Q\n",
    "    print(\"\\nQ^T Q (should be identity):\")\n",
    "    print(QTQ)\n",
    "    print(f\"Orthogonality error: {np.linalg.norm(QTQ - np.eye(3)):.2e}\")\n",
    "    \n",
    "    return A.numpy(), Q, R\n",
    "\n",
    "# Visualize Gram-Schmidt in 3D\n",
    "A_3d, Q_3d, R_3d = visualize_gram_schmidt_3d()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b289a",
   "metadata": {},
   "source": [
    "## Modified Gram-Schmidt Algorithm\n",
    "\n",
    "The classic Gram-Schmidt process can suffer from numerical instability due to floating-point errors. The modified Gram-Schmidt algorithm provides better numerical stability by orthogonalizing vectors one by one, rather than all at once.\n",
    "\n",
    "Here's the implementation:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91db579e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_modified_gram_schmidt(A):\n",
    "    \"\"\"\n",
    "    Compute QR decomposition using the modified Gram-Schmidt process.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "    R = torch.zeros((n, n), dtype=A.dtype)\n",
    "    \n",
    "    # Initialize U as a copy of A\n",
    "    U = A.clone()\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Compute the norm of the i-th column of U\n",
    "        R[i, i] = torch.norm(U[:, i])\n",
    "        \n",
    "        # Normalize to get an orthonormal vector\n",
    "        if R[i, i] > 1e-10:  # Check for numerical stability\n",
    "            Q[:, i] = U[:, i] / R[i, i]\n",
    "        else:\n",
    "            Q[:, i] = torch.zeros(m, dtype=A.dtype)\n",
    "        \n",
    "        # Orthogonalize remaining columns with respect to the i-th column of Q\n",
    "        for j in range(i+1, n):\n",
    "            R[i, j] = torch.dot(Q[:, i], U[:, j])\n",
    "            U[:, j] = U[:, j] - R[i, j] * Q[:, i]\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "# Test the modified Gram-Schmidt on our example matrix\n",
    "Q_modified, R_modified = qr_modified_gram_schmidt(A_simple)\n",
    "\n",
    "# Display the results\n",
    "plot_matrix(Q_modified, \"Orthogonal Matrix Q (Modified G-S)\")\n",
    "plot_matrix(R_modified, \"Upper Triangular Matrix R (Modified G-S)\")\n",
    "\n",
    "# Check that A = QR\n",
    "A_reconstructed_modified = Q_modified @ R_modified\n",
    "plot_matrix(A_reconstructed_modified, \"Reconstructed Matrix (Q×R)\")\n",
    "\n",
    "# Calculate reconstruction error\n",
    "reconstruction_error_modified = torch.norm(A_simple - A_reconstructed_modified).item()\n",
    "print(f\"Reconstruction error (Modified G-S): {reconstruction_error_modified:.2e}\")\n",
    "\n",
    "# Verify that Q is orthogonal (Q^T Q = I)\n",
    "Q_orthogonality_modified = Q_modified.T @ Q_modified\n",
    "plot_matrix(Q_orthogonality_modified, \"Q^T Q (should be identity)\")\n",
    "\n",
    "orthogonality_error_modified = torch.norm(Q_orthogonality_modified - torch.eye(Q_orthogonality_modified.shape[0])).item()\n",
    "print(f\"Orthogonality error (Modified G-S): {orthogonality_error_modified:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10b021d9",
   "metadata": {},
   "source": [
    "## Householder QR Decomposition\n",
    "\n",
    "The Householder QR algorithm is another approach for computing QR decomposition. Instead of the Gram-Schmidt process, it uses Householder reflections to progressively zero out elements below the diagonal.\n",
    "\n",
    "A Householder reflection is a transformation that reflects a vector about a hyperplane. It's defined by a vector $v$ such that the reflection matrix is:\n",
    "\n",
    "$H = I - 2 \\frac{vv^T}{v^T v}$\n",
    "\n",
    "Let's implement this approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a46b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_householder(A):\n",
    "    \"\"\"\n",
    "    Compute QR decomposition using Householder reflections.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    R = A.clone()\n",
    "    Q = torch.eye(m, dtype=A.dtype)\n",
    "    \n",
    "    for k in range(min(m-1, n)):\n",
    "        # Extract the column we want to transform\n",
    "        x = R[k:, k]\n",
    "        \n",
    "        # Construct the Householder vector\n",
    "        e1 = torch.zeros_like(x)\n",
    "        e1[0] = 1.0\n",
    "        \n",
    "        alpha = torch.norm(x)\n",
    "        # Ensure proper sign of alpha to avoid cancellation\n",
    "        if x[0] < 0:\n",
    "            alpha = -alpha\n",
    "        \n",
    "        u = x - alpha * e1\n",
    "        v = u / torch.norm(u)  # Normalize\n",
    "        \n",
    "        # Apply the Householder reflection to R\n",
    "        R[k:, k:] = R[k:, k:] - 2.0 * torch.outer(v, torch.matmul(v, R[k:, k:]))\n",
    "        \n",
    "        # Apply the Householder reflection to Q\n",
    "        Q[:, k:] = Q[:, k:] - 2.0 * torch.matmul(Q[:, k:], torch.outer(v, v))\n",
    "    \n",
    "    # Q should be the product of all Householder reflections\n",
    "    Q = Q.T  # Transpose because we've been applying reflections from the right\n",
    "    \n",
    "    # Ensure the diagonal of R is positive\n",
    "    for i in range(min(m, n)):\n",
    "        if R[i, i] < 0:\n",
    "            R[i, i:] = -R[i, i:]\n",
    "            Q[:, i] = -Q[:, i]\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "# Test the Householder QR on our example matrix\n",
    "Q_house, R_house = qr_householder(A_simple)\n",
    "\n",
    "# Display the results\n",
    "plot_matrix(Q_house, \"Orthogonal Matrix Q (Householder)\")\n",
    "plot_matrix(R_house, \"Upper Triangular Matrix R (Householder)\")\n",
    "\n",
    "# Check that A = QR\n",
    "A_reconstructed_house = Q_house @ R_house\n",
    "plot_matrix(A_reconstructed_house, \"Reconstructed Matrix (Q×R)\")\n",
    "\n",
    "# Calculate reconstruction error\n",
    "reconstruction_error_house = torch.norm(A_simple - A_reconstructed_house).item()\n",
    "print(f\"Reconstruction error (Householder): {reconstruction_error_house:.2e}\")\n",
    "\n",
    "# Verify that Q is orthogonal (Q^T Q = I)\n",
    "Q_orthogonality_house = Q_house.T @ Q_house\n",
    "plot_matrix(Q_orthogonality_house, \"Q^T Q (should be identity)\")\n",
    "\n",
    "orthogonality_error_house = torch.norm(Q_orthogonality_house - torch.eye(Q_orthogonality_house.shape[0])).item()\n",
    "print(f\"Orthogonality error (Householder): {orthogonality_error_house:.2e}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3d241bc",
   "metadata": {},
   "source": [
    "## Comparing QR Decomposition Methods\n",
    "\n",
    "Now let's compare the different QR decomposition methods we've implemented with the built-in functions in terms of:\n",
    "1. Numerical accuracy\n",
    "2. Computational performance\n",
    "3. Orthogonality of Q\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466d9043",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_qr_methods(A):\n",
    "    \"\"\"Compare different QR decomposition methods.\"\"\"\n",
    "    # Make a copy for consistent results\n",
    "    A = A.clone()\n",
    "    \n",
    "    # Our implementations\n",
    "    methods = []\n",
    "    q_matrices = []\n",
    "    r_matrices = []\n",
    "    times = []\n",
    "    reconstruction_errors = []\n",
    "    orthogonality_errors = []\n",
    "    \n",
    "    # Method 1: Classic Gram-Schmidt\n",
    "    start_time = time.time()\n",
    "    Q_gs, R_gs = qr_gram_schmidt(A)\n",
    "    gs_time = time.time() - start_time\n",
    "    methods.append(\"Classic Gram-Schmidt\")\n",
    "    q_matrices.append(Q_gs)\n",
    "    r_matrices.append(R_gs)\n",
    "    times.append(gs_time)\n",
    "    \n",
    "    # Method 2: Modified Gram-Schmidt\n",
    "    start_time = time.time()\n",
    "    Q_mgs, R_mgs = qr_modified_gram_schmidt(A)\n",
    "    mgs_time = time.time() - start_time\n",
    "    methods.append(\"Modified Gram-Schmidt\")\n",
    "    q_matrices.append(Q_mgs)\n",
    "    r_matrices.append(R_mgs)\n",
    "    times.append(mgs_time)\n",
    "    \n",
    "    # Method 3: Householder\n",
    "    start_time = time.time()\n",
    "    Q_house, R_house = qr_householder(A)\n",
    "    house_time = time.time() - start_time\n",
    "    methods.append(\"Householder\")\n",
    "    q_matrices.append(Q_house)\n",
    "    r_matrices.append(R_house)\n",
    "    times.append(house_time)\n",
    "    \n",
    "    # Method 4: NumPy QR (via SciPy)\n",
    "    A_np = A.numpy()\n",
    "    start_time = time.time()\n",
    "    Q_np, R_np = scipy.linalg.qr(A_np, mode='economic')\n",
    "    np_time = time.time() - start_time\n",
    "    methods.append(\"SciPy QR\")\n",
    "    q_matrices.append(torch.from_numpy(Q_np))\n",
    "    r_matrices.append(torch.from_numpy(R_np))\n",
    "    times.append(np_time)\n",
    "    \n",
    "    # Method 5: PyTorch QR\n",
    "    start_time = time.time()\n",
    "    Q_torch, R_torch = torch.linalg.qr(A, mode='reduced')\n",
    "    torch_time = time.time() - start_time\n",
    "    methods.append(\"PyTorch QR\")\n",
    "    q_matrices.append(Q_torch)\n",
    "    r_matrices.append(R_torch)\n",
    "    times.append(torch_time)\n",
    "    \n",
    "    # Calculate errors\n",
    "    for i, (method, Q, R) in enumerate(zip(methods, q_matrices, r_matrices)):\n",
    "        # Reconstruction error\n",
    "        reconstructed = Q @ R\n",
    "        reconstruction_errors.append(torch.norm(A - reconstructed).item())\n",
    "        \n",
    "        # Orthogonality error\n",
    "        Q_orth = Q.T @ Q\n",
    "        orthogonality_errors.append(torch.norm(Q_orth - torch.eye(Q_orth.shape[0])).item())\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    # Computation time\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.bar(methods, times)\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"Computation Time\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Reconstruction error\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(methods, reconstruction_errors)\n",
    "    plt.ylabel(\"Reconstruction Error\")\n",
    "    plt.title(\"A - QR (Frobenius norm)\")\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Orthogonality error\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(methods, orthogonality_errors)\n",
    "    plt.ylabel(\"Orthogonality Error\")\n",
    "    plt.title(\"Q^T Q - I (Frobenius norm)\")\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the performance comparison\n",
    "    print(\"QR Decomposition Method Comparison:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Method':<25} {'Time (s)':<15} {'Reconstruction Error':<25} {'Orthogonality Error':<25}\")\n",
    "    print(\"-\" * 100)\n",
    "    for method, t, rec_err, orth_err in zip(methods, times, reconstruction_errors, orthogonality_errors):\n",
    "        print(f\"{method:<25} {t:<15.6f} {rec_err:<25.2e} {orth_err:<25.2e}\")\n",
    "    \n",
    "    return methods, times, reconstruction_errors, orthogonality_errors\n",
    "\n",
    "# Test on our example matrix\n",
    "methods, times, rec_errors, orth_errors = compare_qr_methods(A_simple)\n",
    "\n",
    "# Create a larger random matrix to better highlight performance differences\n",
    "A_random = create_example_matrix(m=100, n=50, method=\"random\")\n",
    "methods_large, times_large, rec_errors_large, orth_errors_large = compare_qr_methods(A_random)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b93bf4",
   "metadata": {},
   "source": [
    "### Numerical Stability Demonstration\n",
    "\n",
    "Let's create an example that specifically demonstrates the numerical instability of the classic Gram-Schmidt process compared to the modified version:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29161bf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def demonstrate_numerical_stability():\n",
    "    \"\"\"Demonstrate numerical instability of classical Gram-Schmidt.\"\"\"\n",
    "    # Create a matrix with nearly linearly dependent columns\n",
    "    m, n = 5, 3\n",
    "    \n",
    "    # Start with orthogonal columns\n",
    "    Q = torch.eye(m, n)\n",
    "    \n",
    "    # Apply a condition number to make it ill-conditioned\n",
    "    kappa = 1e5  # Condition number\n",
    "    S = torch.diag(torch.tensor([1.0, 1.0/np.sqrt(kappa), 1.0/kappa]))\n",
    "    \n",
    "    # Create a matrix with columns that are nearly linearly dependent\n",
    "    A = Q @ S\n",
    "    \n",
    "    # Add a small random perturbation\n",
    "    A = A + 1e-10 * torch.randn(m, n)\n",
    "    \n",
    "    # Apply different QR methods\n",
    "    Q_gs, R_gs = qr_gram_schmidt(A)\n",
    "    Q_mgs, R_mgs = qr_modified_gram_schmidt(A)\n",
    "    Q_house, R_house = qr_householder(A)\n",
    "    \n",
    "    # Calculate orthogonality errors\n",
    "    orth_err_gs = torch.norm(Q_gs.T @ Q_gs - torch.eye(n)).item()\n",
    "    orth_err_mgs = torch.norm(Q_mgs.T @ Q_mgs - torch.eye(n)).item()\n",
    "    orth_err_house = torch.norm(Q_house.T @ Q_house - torch.eye(n)).item()\n",
    "    \n",
    "    # Calculate reconstruction errors\n",
    "    rec_err_gs = torch.norm(A - Q_gs @ R_gs).item()\n",
    "    rec_err_mgs = torch.norm(A - Q_mgs @ R_mgs).item()\n",
    "    rec_err_house = torch.norm(A - Q_house @ R_house).item()\n",
    "    \n",
    "    # Print results\n",
    "    print(\"Numerical Stability Comparison:\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"Condition number of test matrix: {kappa:.1e}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Method':<25} {'Orthogonality Error':<25} {'Reconstruction Error':<25}\")\n",
    "    print(\"-\" * 70)\n",
    "    print(f\"{'Classic Gram-Schmidt':<25} {orth_err_gs:<25.2e} {rec_err_gs:<25.2e}\")\n",
    "    print(f\"{'Modified Gram-Schmidt':<25} {orth_err_mgs:<25.2e} {rec_err_mgs:<25.2e}\")\n",
    "    print(f\"{'Householder':<25} {orth_err_house:<25.2e} {rec_err_house:<25.2e}\")\n",
    "    \n",
    "    # Plot the orthogonality of Q matrices\n",
    "    methods = [\"Classic G-S\", \"Modified G-S\", \"Householder\"]\n",
    "    orth_errors = [orth_err_gs, orth_err_mgs, orth_err_house]\n",
    "    rec_errors = [rec_err_gs, rec_err_mgs, rec_err_house]\n",
    "    \n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(methods, orth_errors)\n",
    "    plt.title(\"Orthogonality Error (Q^T Q - I)\")\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(methods, rec_errors)\n",
    "    plt.title(\"Reconstruction Error (A - QR)\")\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize orthogonality matrices\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(Q_gs.T @ Q_gs, annot=True, fmt=\".3f\", cmap=blue_cmap)\n",
    "    plt.title(f\"Q^T Q (Classic G-S)\\nError: {orth_err_gs:.2e}\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(Q_mgs.T @ Q_mgs, annot=True, fmt=\".3f\", cmap=blue_cmap)\n",
    "    plt.title(f\"Q^T Q (Modified G-S)\\nError: {orth_err_mgs:.2e}\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.heatmap(Q_house.T @ Q_house, annot=True, fmt=\".3f\", cmap=blue_cmap)\n",
    "    plt.title(f\"Q^T Q (Householder)\\nError: {orth_err_house:.2e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return A, [Q_gs, Q_mgs, Q_house], [R_gs, R_mgs, R_house]\n",
    "\n",
    "# Demonstrate numerical stability\n",
    "A_ill, Q_methods, R_methods = demonstrate_numerical_stability()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c062ea59",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we have explored QR decomposition, a fundamental matrix factorization technique:\n",
    "\n",
    "1. We implemented and compared three different QR decomposition algorithms:\n",
    "   - Classic Gram-Schmidt process\n",
    "   - Modified Gram-Schmidt process\n",
    "   - Householder reflections\n",
    "\n",
    "2. We visualized the Gram-Schmidt process geometrically in 2D and 3D to build intuition.\n",
    "\n",
    "3. We demonstrated the numerical stability advantages of the modified Gram-Schmidt and Householder methods over the classic Gram-Schmidt process.\n",
    "\n",
    "4. We compared our implementations with built-in functions from SciPy and PyTorch.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- Classic Gram-Schmidt is conceptually simple but can suffer from numerical instability\n",
    "- Modified Gram-Schmidt provides better numerical stability with the same computational complexity\n",
    "- Householder reflections offer the best numerical stability and are commonly used in production libraries\n",
    "- Built-in implementations (SciPy, PyTorch) are typically faster and more robust for practical use\n",
    "\n",
    "In the next notebook, we'll explore applications of QR decomposition in solving linear systems, least squares problems, and eigenvalue computations.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
