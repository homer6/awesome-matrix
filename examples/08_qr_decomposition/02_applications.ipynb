{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee4d6492",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/08_qr_decomposition`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/08_qr_decomposition/02_applications.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fce25e92",
   "metadata": {},
   "source": [
    "# QR Decomposition: Applications\n",
    "\n",
    "This notebook explores practical applications of QR decomposition in computational linear algebra and beyond, demonstrating why this factorization is so valuable in various fields.\n",
    "\n",
    "We'll investigate the following applications:\n",
    "\n",
    "1. **Solving Linear Systems**\n",
    "2. **Least Squares Problems**\n",
    "3. **QR Algorithm for Eigenvalues**\n",
    "4. **Singular Value Decomposition (SVD)**\n",
    "5. **Applications in Data Science and Machine Learning**\n",
    "\n",
    "Each application demonstrates the power and versatility of QR decomposition and provides insights into how this matrix factorization technique is used in practice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff563c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import time\n",
    "import scipy\n",
    "import scipy.linalg\n",
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as spalg\n",
    "import matplotlib.patches as patches\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# For better looking plots\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom colormap (light blue to dark blue)\n",
    "colors = [(0.95, 0.95, 1), (0.0, 0.2, 0.6)]\n",
    "blue_cmap = LinearSegmentedColormap.from_list('CustomBlue', colors, N=100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b761359",
   "metadata": {},
   "source": [
    "First, let's implement a reusable QR decomposition function and visualization helpers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eabce88d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_decomposition(A, method=\"householder\"):\n",
    "    \"\"\"\n",
    "    Compute QR decomposition of a matrix A.\n",
    "    \n",
    "    Args:\n",
    "        A: Matrix to decompose (numpy array or PyTorch tensor)\n",
    "        method: 'gram_schmidt', 'modified_gram_schmidt', 'householder', or 'builtin'\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    if method == \"gram_schmidt\":\n",
    "        # Classical Gram-Schmidt\n",
    "        Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "        R = torch.zeros((n, n), dtype=A.dtype)\n",
    "        \n",
    "        for j in range(n):\n",
    "            v = A[:, j].clone()\n",
    "            \n",
    "            for i in range(j):\n",
    "                R[i, j] = torch.dot(Q[:, i], A[:, j])\n",
    "                v = v - R[i, j] * Q[:, i]\n",
    "            \n",
    "            R[j, j] = torch.norm(v)\n",
    "            if R[j, j] > 1e-10:\n",
    "                Q[:, j] = v / R[j, j]\n",
    "            else:\n",
    "                Q[:, j] = torch.zeros(m, dtype=A.dtype)\n",
    "                \n",
    "        return Q, R\n",
    "        \n",
    "    elif method == \"modified_gram_schmidt\":\n",
    "        # Modified Gram-Schmidt\n",
    "        Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "        R = torch.zeros((n, n), dtype=A.dtype)\n",
    "        \n",
    "        U = A.clone()\n",
    "        \n",
    "        for i in range(n):\n",
    "            R[i, i] = torch.norm(U[:, i])\n",
    "            \n",
    "            if R[i, i] > 1e-10:\n",
    "                Q[:, i] = U[:, i] / R[i, i]\n",
    "            else:\n",
    "                Q[:, i] = torch.zeros(m, dtype=A.dtype)\n",
    "            \n",
    "            for j in range(i+1, n):\n",
    "                R[i, j] = torch.dot(Q[:, i], U[:, j])\n",
    "                U[:, j] = U[:, j] - R[i, j] * Q[:, i]\n",
    "                \n",
    "        return Q, R\n",
    "        \n",
    "    elif method == \"householder\":\n",
    "        # Householder reflections\n",
    "        R = A.clone()\n",
    "        Q = torch.eye(m, dtype=A.dtype)\n",
    "        \n",
    "        for k in range(min(m-1, n)):\n",
    "            x = R[k:, k]\n",
    "            \n",
    "            e1 = torch.zeros_like(x)\n",
    "            e1[0] = 1.0\n",
    "            \n",
    "            alpha = torch.norm(x)\n",
    "            if x[0] < 0:\n",
    "                alpha = -alpha\n",
    "                \n",
    "            u = x - alpha * e1\n",
    "            v = u / torch.norm(u)\n",
    "            \n",
    "            R[k:, k:] = R[k:, k:] - 2.0 * torch.outer(v, torch.matmul(v, R[k:, k:]))\n",
    "            Q[:, k:] = Q[:, k:] - 2.0 * torch.matmul(Q[:, k:], torch.outer(v, v))\n",
    "        \n",
    "        Q = Q.T\n",
    "        \n",
    "        # Ensure positive diagonal for R\n",
    "        for i in range(min(m, n)):\n",
    "            if R[i, i] < 0:\n",
    "                R[i, i:] = -R[i, i:]\n",
    "                Q[:, i] = -Q[:, i]\n",
    "                \n",
    "        return Q, R\n",
    "        \n",
    "    elif method == \"builtin\":\n",
    "        # Use PyTorch's built-in QR decomposition\n",
    "        return torch.linalg.qr(A, mode='reduced')\n",
    "        \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown method: {method}\")\n",
    "\n",
    "def plot_matrix(matrix, title=\"Matrix\", annotate=True, cmap=blue_cmap):\n",
    "    \"\"\"Plot a matrix as a heatmap with annotations.\"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix_np = matrix.detach().cpu().numpy()\n",
    "    else:\n",
    "        matrix_np = matrix\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(matrix_np, annot=annotate, fmt=\".2f\", cmap=cmap, \n",
    "                    linewidths=1, cbar=True)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add row and column indices\n",
    "    ax.set_xticks(np.arange(matrix_np.shape[1]) + 0.5)\n",
    "    ax.set_yticks(np.arange(matrix_np.shape[0]) + 0.5)\n",
    "    ax.set_xticklabels([f\"Col {i+1}\" for i in range(matrix_np.shape[1])])\n",
    "    ax.set_yticklabels([f\"Row {i+1}\" for i in range(matrix_np.shape[0])])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9c362e",
   "metadata": {},
   "source": [
    "## 1. Solving Linear Systems using QR Decomposition\n",
    "\n",
    "One of the primary applications of QR decomposition is solving linear systems of equations. If we have a system $Ax = b$, we can use QR decomposition to solve it efficiently.\n",
    "\n",
    "The process works as follows:\n",
    "1. Decompose $A = QR$\n",
    "2. Rewrite the system as $QRx = b$\n",
    "3. Multiply both sides by $Q^T$ to get $Rx = Q^T b$ (using the fact that $Q^T Q = I$)\n",
    "4. Solve the upper triangular system $Rx = Q^T b$ using back substitution\n",
    "\n",
    "Let's implement this approach and compare it with other methods for solving linear systems:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dd43c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_with_qr(A, b):\n",
    "    \"\"\"Solve a linear system Ax = b using QR decomposition.\"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    if isinstance(b, np.ndarray):\n",
    "        b = torch.tensor(b, dtype=torch.float64)\n",
    "    \n",
    "    # Perform QR decomposition\n",
    "    Q, R = qr_decomposition(A, method=\"householder\")\n",
    "    \n",
    "    # Compute Q^T b\n",
    "    y = Q.T @ b\n",
    "    \n",
    "    # Solve the upper triangular system Rx = y using back substitution\n",
    "    n = R.shape[0]\n",
    "    x = torch.zeros_like(y)\n",
    "    \n",
    "    for i in range(n-1, -1, -1):\n",
    "        x[i] = y[i]\n",
    "        for j in range(i+1, n):\n",
    "            x[i] -= R[i, j] * x[j]\n",
    "        x[i] /= R[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "def compare_linear_system_solvers():\n",
    "    \"\"\"Compare different methods for solving linear systems.\"\"\"\n",
    "    # Create a well-conditioned random matrix\n",
    "    n = 100\n",
    "    np.random.seed(42)\n",
    "    A = np.random.rand(n, n)\n",
    "    \n",
    "    # Make it diagonally dominant for better conditioning\n",
    "    for i in range(n):\n",
    "        A[i, i] = np.sum(np.abs(A[i, :])) + 1.0\n",
    "    \n",
    "    # Create a random right-hand side\n",
    "    b = np.random.rand(n)\n",
    "    \n",
    "    # Convert to PyTorch tensors\n",
    "    A_torch = torch.tensor(A, dtype=torch.float64)\n",
    "    b_torch = torch.tensor(b, dtype=torch.float64)\n",
    "    \n",
    "    # Method 1: Our QR implementation\n",
    "    start_time = time.time()\n",
    "    x_qr = solve_with_qr(A_torch, b_torch)\n",
    "    qr_time = time.time() - start_time\n",
    "    \n",
    "    # Method 2: Direct solve using PyTorch\n",
    "    start_time = time.time()\n",
    "    x_direct = torch.linalg.solve(A_torch, b_torch)\n",
    "    direct_time = time.time() - start_time\n",
    "    \n",
    "    # Method 3: LU decomposition\n",
    "    start_time = time.time()\n",
    "    P, L, U = scipy.linalg.lu(A)\n",
    "    y = scipy.linalg.solve_triangular(L, P @ b, lower=True)\n",
    "    x_lu = scipy.linalg.solve_triangular(U, y, lower=False)\n",
    "    lu_time = time.time() - start_time\n",
    "    \n",
    "    # Method 4: SciPy's QR solver\n",
    "    start_time = time.time()\n",
    "    Q_scipy, R_scipy = scipy.linalg.qr(A)\n",
    "    y_scipy = Q_scipy.T @ b\n",
    "    x_scipy = scipy.linalg.solve_triangular(R_scipy, y_scipy, lower=False)\n",
    "    scipy_qr_time = time.time() - start_time\n",
    "    \n",
    "    # Calculate residuals\n",
    "    qr_residual = np.linalg.norm(A @ x_qr.numpy() - b) / np.linalg.norm(b)\n",
    "    direct_residual = np.linalg.norm(A @ x_direct.numpy() - b) / np.linalg.norm(b)\n",
    "    lu_residual = np.linalg.norm(A @ x_lu - b) / np.linalg.norm(b)\n",
    "    scipy_qr_residual = np.linalg.norm(A @ x_scipy - b) / np.linalg.norm(b)\n",
    "    \n",
    "    # Compare methods\n",
    "    methods = [\"QR (Ours)\", \"Direct Solve\", \"LU Decomposition\", \"QR (SciPy)\"]\n",
    "    times = [qr_time, direct_time, lu_time, scipy_qr_time]\n",
    "    residuals = [qr_residual, direct_residual, lu_residual, scipy_qr_residual]\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(methods, times)\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"Solving Linear System Ax = b\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate with time values\n",
    "    for i, t in enumerate(times):\n",
    "        plt.text(i, t + 0.001, f\"{t:.4f}s\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.bar(methods, residuals)\n",
    "    plt.ylabel(\"Relative Residual\")\n",
    "    plt.title(\"Solution Accuracy\")\n",
    "    plt.yscale('log')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate with residual values\n",
    "    for i, r in enumerate(residuals):\n",
    "        plt.text(i, r * 1.1, f\"{r:.2e}\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Linear System Solver Comparison:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Method':<20} {'Time (s)':<15} {'Relative Residual':<20} {'Speedup':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Calculate speedup relative to our QR implementation\n",
    "    reference_time = qr_time\n",
    "    for method, t, r in zip(methods, times, residuals):\n",
    "        speedup = reference_time / t\n",
    "        print(f\"{method:<20} {t:<15.6f} {r:<20.2e} {speedup:<10.2f}\")\n",
    "    \n",
    "    # Return the solutions for further analysis\n",
    "    return {\n",
    "        \"QR (Ours)\": x_qr.numpy(),\n",
    "        \"Direct Solve\": x_direct.numpy(),\n",
    "        \"LU Decomposition\": x_lu,\n",
    "        \"QR (SciPy)\": x_scipy\n",
    "    }\n",
    "\n",
    "# Compare different linear system solvers\n",
    "solution_comparison = compare_linear_system_solvers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3641ad1",
   "metadata": {},
   "source": [
    "### Effect of Matrix Condition Number\n",
    "\n",
    "The condition number of a matrix affects the numerical stability of linear system solvers. Let's explore how different methods perform with varying condition numbers:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfca0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_solvers_with_condition_numbers():\n",
    "    \"\"\"Compare solver performance with different matrix condition numbers.\"\"\"\n",
    "    # Define a range of condition numbers to test\n",
    "    condition_numbers = [1e1, 1e3, 1e5, 1e7, 1e9]\n",
    "    \n",
    "    n = 50  # Matrix size\n",
    "    \n",
    "    # Store results\n",
    "    qr_residuals = []\n",
    "    lu_residuals = []\n",
    "    direct_residuals = []\n",
    "    qr_times = []\n",
    "    lu_times = []\n",
    "    direct_times = []\n",
    "    \n",
    "    for kappa in condition_numbers:\n",
    "        # Create a matrix with specified condition number\n",
    "        # Start with a random orthogonal matrix\n",
    "        X = np.random.randn(n, n)\n",
    "        Q, _ = np.linalg.qr(X)\n",
    "        \n",
    "        # Create a diagonal matrix with desired condition number\n",
    "        s1 = 1.0\n",
    "        sn = s1 / kappa\n",
    "        S = np.diag(np.linspace(s1, sn, n))\n",
    "        \n",
    "        # Form the test matrix\n",
    "        A = Q @ S @ Q.T\n",
    "        \n",
    "        # Create a random right-hand side\n",
    "        b = np.random.rand(n)\n",
    "        \n",
    "        # Convert to PyTorch tensors\n",
    "        A_torch = torch.tensor(A, dtype=torch.float64)\n",
    "        b_torch = torch.tensor(b, dtype=torch.float64)\n",
    "        \n",
    "        # Method 1: QR decomposition\n",
    "        start_time = time.time()\n",
    "        x_qr = solve_with_qr(A_torch, b_torch)\n",
    "        qr_time = time.time() - start_time\n",
    "        qr_residual = np.linalg.norm(A @ x_qr.numpy() - b) / np.linalg.norm(b)\n",
    "        \n",
    "        # Method 2: LU decomposition\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            P, L, U = scipy.linalg.lu(A)\n",
    "            y = scipy.linalg.solve_triangular(L, P @ b, lower=True)\n",
    "            x_lu = scipy.linalg.solve_triangular(U, y, lower=False)\n",
    "            lu_residual = np.linalg.norm(A @ x_lu - b) / np.linalg.norm(b)\n",
    "        except np.linalg.LinAlgError:\n",
    "            x_lu = np.full_like(b, np.nan)\n",
    "            lu_residual = np.nan\n",
    "        lu_time = time.time() - start_time\n",
    "        \n",
    "        # Method 3: Direct solve\n",
    "        start_time = time.time()\n",
    "        try:\n",
    "            x_direct = torch.linalg.solve(A_torch, b_torch)\n",
    "            direct_residual = np.linalg.norm(A @ x_direct.numpy() - b) / np.linalg.norm(b)\n",
    "        except RuntimeError:\n",
    "            x_direct = torch.full_like(b_torch, float('nan'))\n",
    "            direct_residual = float('nan')\n",
    "        direct_time = time.time() - start_time\n",
    "        \n",
    "        # Store results\n",
    "        qr_residuals.append(qr_residual)\n",
    "        lu_residuals.append(lu_residual)\n",
    "        direct_residuals.append(direct_residual)\n",
    "        qr_times.append(qr_time)\n",
    "        lu_times.append(lu_time)\n",
    "        direct_times.append(direct_time)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Residuals\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.loglog(condition_numbers, qr_residuals, 'o-', label='QR')\n",
    "    plt.loglog(condition_numbers, lu_residuals, 's-', label='LU')\n",
    "    plt.loglog(condition_numbers, direct_residuals, '^-', label='Direct')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel(\"Condition Number\")\n",
    "    plt.ylabel(\"Relative Residual\")\n",
    "    plt.title(\"Solver Accuracy vs. Matrix Condition Number\")\n",
    "    plt.legend()\n",
    "    \n",
    "    # Computation time\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.semilogx(condition_numbers, qr_times, 'o-', label='QR')\n",
    "    plt.semilogx(condition_numbers, lu_times, 's-', label='LU')\n",
    "    plt.semilogx(condition_numbers, direct_times, '^-', label='Direct')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xlabel(\"Condition Number\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"Solver Performance vs. Matrix Condition Number\")\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Solver Performance with Different Condition Numbers:\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Condition Number':<20} {'QR Residual':<15} {'LU Residual':<15} {'Direct Residual':<15}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for kappa, qr_res, lu_res, direct_res in zip(condition_numbers, qr_residuals, lu_residuals, direct_residuals):\n",
    "        print(f\"{kappa:<20.1e} {qr_res:<15.2e} {lu_res:<15.2e} {direct_res:<15.2e}\")\n",
    "    \n",
    "    return condition_numbers, qr_residuals, lu_residuals, direct_residuals\n",
    "\n",
    "# Compare solvers with different condition numbers\n",
    "cond_numbers, qr_res, lu_res, direct_res = compare_solvers_with_condition_numbers()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d205d1",
   "metadata": {},
   "source": [
    "## 2. Least Squares Problems\n",
    "\n",
    "QR decomposition is particularly well-suited for solving least squares problems. Given an overdetermined system (more equations than unknowns), we want to find $x$ that minimizes $||Ax - b||_2$.\n",
    "\n",
    "The solution is given by the normal equations: $A^T A x = A^T b$. However, forming $A^T A$ explicitly can lead to numerical issues. Instead, we can use QR decomposition:\n",
    "\n",
    "1. Decompose $A = QR$\n",
    "2. The least squares solution is then $x = R^{-1} Q^T b$, or equivalently, $Rx = Q^T b$\n",
    "\n",
    "Let's implement and demonstrate this approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a395938c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def solve_least_squares_with_qr(A, b):\n",
    "    \"\"\"Solve a least squares problem using QR decomposition.\"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    if isinstance(b, np.ndarray):\n",
    "        b = torch.tensor(b, dtype=torch.float64)\n",
    "    \n",
    "    # Perform QR decomposition\n",
    "    Q, R = qr_decomposition(A, method=\"householder\")\n",
    "    \n",
    "    # Compute Q^T b\n",
    "    y = Q.T @ b\n",
    "    \n",
    "    # Solve the upper triangular system Rx = y\n",
    "    n = R.shape[1]  # Number of columns\n",
    "    x = torch.zeros(n, dtype=A.dtype)\n",
    "    \n",
    "    for i in range(n-1, -1, -1):\n",
    "        x[i] = y[i]\n",
    "        for j in range(i+1, n):\n",
    "            x[i] -= R[i, j] * x[j]\n",
    "        x[i] /= R[i, i]\n",
    "    \n",
    "    return x\n",
    "\n",
    "def demonstrate_least_squares():\n",
    "    \"\"\"Demonstrate solving a least squares problem with QR decomposition.\"\"\"\n",
    "    # Create a simple linear regression problem\n",
    "    np.random.seed(42)\n",
    "    n = 100  # Number of data points\n",
    "    p = 3    # Number of parameters (including intercept)\n",
    "    \n",
    "    # Create design matrix (features)\n",
    "    X = np.random.rand(n, p-1)  # Random features\n",
    "    X = np.column_stack([np.ones(n), X])  # Add intercept column\n",
    "    \n",
    "    # True parameters\n",
    "    beta_true = np.array([2.5, -1.8, 3.7])\n",
    "    \n",
    "    # Generate noisy observations\n",
    "    noise_level = 0.5\n",
    "    y = X @ beta_true + noise_level * np.random.randn(n)\n",
    "    \n",
    "    # Solve using different methods\n",
    "    methods = []\n",
    "    params = []\n",
    "    residuals = []\n",
    "    times = []\n",
    "    \n",
    "    # Method 1: Our QR implementation\n",
    "    start_time = time.time()\n",
    "    beta_qr = solve_least_squares_with_qr(X, y)\n",
    "    qr_time = time.time() - start_time\n",
    "    y_pred_qr = X @ beta_qr.numpy()\n",
    "    qr_residual = np.linalg.norm(y - y_pred_qr) / np.linalg.norm(y)\n",
    "    \n",
    "    methods.append(\"QR (Ours)\")\n",
    "    params.append(beta_qr.numpy())\n",
    "    residuals.append(qr_residual)\n",
    "    times.append(qr_time)\n",
    "    \n",
    "    # Method 2: Normal equations\n",
    "    start_time = time.time()\n",
    "    beta_normal = np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "    normal_time = time.time() - start_time\n",
    "    y_pred_normal = X @ beta_normal\n",
    "    normal_residual = np.linalg.norm(y - y_pred_normal) / np.linalg.norm(y)\n",
    "    \n",
    "    methods.append(\"Normal Equations\")\n",
    "    params.append(beta_normal)\n",
    "    residuals.append(normal_residual)\n",
    "    times.append(normal_time)\n",
    "    \n",
    "    # Method 3: SciPy's QR solver\n",
    "    start_time = time.time()\n",
    "    Q_scipy, R_scipy = scipy.linalg.qr(X)\n",
    "    z = Q_scipy.T @ y\n",
    "    beta_scipy = scipy.linalg.solve_triangular(R_scipy[:p, :], z[:p], lower=False)\n",
    "    scipy_qr_time = time.time() - start_time\n",
    "    y_pred_scipy = X @ beta_scipy\n",
    "    scipy_qr_residual = np.linalg.norm(y - y_pred_scipy) / np.linalg.norm(y)\n",
    "    \n",
    "    methods.append(\"QR (SciPy)\")\n",
    "    params.append(beta_scipy)\n",
    "    residuals.append(scipy_qr_residual)\n",
    "    times.append(scipy_qr_time)\n",
    "    \n",
    "    # Method 4: NumPy's least squares solver\n",
    "    start_time = time.time()\n",
    "    beta_np, residuals_np, rank_np, s_np = np.linalg.lstsq(X, y, rcond=None)\n",
    "    np_time = time.time() - start_time\n",
    "    y_pred_np = X @ beta_np\n",
    "    np_residual = np.linalg.norm(y - y_pred_np) / np.linalg.norm(y)\n",
    "    \n",
    "    methods.append(\"np.linalg.lstsq\")\n",
    "    params.append(beta_np)\n",
    "    residuals.append(np_residual)\n",
    "    times.append(np_time)\n",
    "    \n",
    "    # Plot the results\n",
    "    # Compare parameters\n",
    "    plt.figure(figsize=(15, 12))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    param_names = [\"Intercept\", \"Coefficient 1\", \"Coefficient 2\"]\n",
    "    x_pos = np.arange(len(param_names))\n",
    "    width = 0.2\n",
    "    \n",
    "    # Plot bars for each method\n",
    "    for i, (method, beta) in enumerate(zip(methods, params)):\n",
    "        plt.bar(x_pos + i*width - 0.3, beta, width, label=method)\n",
    "    \n",
    "    # Add true parameter values\n",
    "    plt.plot(x_pos, beta_true, 'ro', markersize=8, label='True Values')\n",
    "    \n",
    "    plt.xticks(x_pos, param_names)\n",
    "    plt.ylabel(\"Parameter Value\")\n",
    "    plt.title(\"Estimated Parameters\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot computation time\n",
    "    plt.subplot(2, 2, 2)\n",
    "    plt.bar(methods, times)\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"Computation Time\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Annotate with time values\n",
    "    for i, t in enumerate(times):\n",
    "        plt.text(i, t + 0.0001, f\"{t:.6f}s\", ha='center', va='bottom', fontsize=10)\n",
    "    \n",
    "    # Plot residuals\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.bar(methods, residuals)\n",
    "    plt.ylabel(\"Relative Residual\")\n",
    "    plt.title(\"Solution Accuracy\")\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Scatter plot of actual vs. predicted\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(y, y_pred_qr, alpha=0.5, label='Predictions')\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], 'r--', label='Perfect Fit')\n",
    "    plt.xlabel(\"Actual Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f\"Actual vs. Predicted (QR Method)\\nR² = {1 - qr_residual**2:.4f}\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print parameter estimates\n",
    "    print(\"Least Squares Parameter Estimates:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Parameter':<15} {'True Value':<15} \" + \" \".join([f\"{method:<15}\" for method in methods]))\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i, name in enumerate(param_names):\n",
    "        param_values = [p[i] for p in params]\n",
    "        print(f\"{name:<15} {beta_true[i]:<15.4f} \" + \" \".join([f\"{val:<15.4f}\" for val in param_values]))\n",
    "    \n",
    "    print(\"\\nRelative Residuals:\")\n",
    "    print(\"-\" * 50)\n",
    "    for method, residual in zip(methods, residuals):\n",
    "        print(f\"{method:<20} {residual:<15.2e}\")\n",
    "    \n",
    "    return X, y, params\n",
    "\n",
    "# Demonstrate least squares regression\n",
    "X_data, y_data, beta_estimates = demonstrate_least_squares()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11fb04a2",
   "metadata": {},
   "source": [
    "### Geometric Interpretation of Least Squares with QR\n",
    "\n",
    "Let's visualize how QR decomposition helps in solving least squares problems geometrically. In particular, we'll see how the projection of $b$ onto the column space of $A$ gives us the least squares solution:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cccac98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_least_squares_2d():\n",
    "    \"\"\"Visualize least squares solution in 2D using QR decomposition.\"\"\"\n",
    "    # Create a simple 2D problem\n",
    "    np.random.seed(42)\n",
    "    n = 20  # Number of data points\n",
    "    \n",
    "    # Create a single feature and add intercept\n",
    "    x = np.random.rand(n)\n",
    "    X = np.column_stack([np.ones(n), x])\n",
    "    \n",
    "    # True parameters and noisy observations\n",
    "    beta_true = np.array([2.0, 1.5])\n",
    "    noise_level = 0.3\n",
    "    y = X @ beta_true + noise_level * np.random.randn(n)\n",
    "    \n",
    "    # Solve using QR decomposition\n",
    "    Q, R = scipy.linalg.qr(X)\n",
    "    z = Q.T @ y\n",
    "    beta_qr = scipy.linalg.solve_triangular(R[:2, :], z[:2], lower=False)\n",
    "    y_pred = X @ beta_qr\n",
    "    \n",
    "    # Calculate projections\n",
    "    projection = Q[:, :2] @ z[:2]  # Projection of y onto col(X)\n",
    "    residual_vector = y - projection  # Orthogonal to col(X)\n",
    "    \n",
    "    # Create scatter plot of data and regression line\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(x, y, label='Data Points')\n",
    "    \n",
    "    # Plot the regression line\n",
    "    x_line = np.linspace(0, 1, 100)\n",
    "    y_line = beta_qr[0] + beta_qr[1] * x_line\n",
    "    plt.plot(x_line, y_line, 'r-', label='Regression Line')\n",
    "    \n",
    "    # Plot residuals as vertical lines\n",
    "    for i in range(n):\n",
    "        plt.plot([x[i], x[i]], [y[i], y_pred[i]], 'g--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel('x')\n",
    "    plt.ylabel('y')\n",
    "    plt.title('Least Squares Regression')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.show()\n",
    "    \n",
    "    # Now visualize in the column space of X and its orthogonal complement\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Project data into the Q space (transformed coordinates)\n",
    "    y_Q = Q.T @ y\n",
    "    \n",
    "    # In Q space, the first two coordinates represent the column space of X\n",
    "    # and the remaining coordinates represent the orthogonal complement\n",
    "    \n",
    "    # Plot in the column space of X (first two dimensions of Q)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter([0], [0], color='black', s=50, marker='o', label='Origin')\n",
    "    plt.scatter([0], [y_Q[0]], color='blue', s=50, marker='s', label='y_Q[0]')\n",
    "    plt.scatter([y_Q[1]], [0], color='green', s=50, marker='^', label='y_Q[1]')\n",
    "    plt.scatter([y_Q[1]], [y_Q[0]], color='red', s=100, marker='*', label='Projection')\n",
    "    plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.axis('equal')\n",
    "    plt.xlabel('Q₁')\n",
    "    plt.ylabel('Q₂')\n",
    "    plt.title('Projection in Column Space of X')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot original vector and its projection\n",
    "    plt.subplot(2, 2, 2)\n",
    "    \n",
    "    # We'll use the first 3 dimensions for visualization\n",
    "    fig = plt.figure(figsize=(8, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    # Plot the original vector y in the Q-basis\n",
    "    ax.quiver(0, 0, 0, y_Q[0], y_Q[1], y_Q[2], color='blue', label='y')\n",
    "    \n",
    "    # Plot the projection onto the first two dimensions\n",
    "    ax.quiver(0, 0, 0, y_Q[0], y_Q[1], 0, color='red', label='Projection')\n",
    "    \n",
    "    # Plot the residual (orthogonal to the column space)\n",
    "    ax.quiver(y_Q[0], y_Q[1], 0, 0, 0, y_Q[2], color='green', label='Residual')\n",
    "    \n",
    "    # Plot the coordinate axes\n",
    "    length = max(abs(y_Q[:3])) * 1.2\n",
    "    ax.quiver(0, 0, 0, length, 0, 0, color='k', alpha=0.5)\n",
    "    ax.quiver(0, 0, 0, 0, length, 0, color='k', alpha=0.5)\n",
    "    ax.quiver(0, 0, 0, 0, 0, length, color='k', alpha=0.5)\n",
    "    \n",
    "    ax.set_xlabel('Q₁')\n",
    "    ax.set_ylabel('Q₂')\n",
    "    ax.set_zlabel('Q₃')\n",
    "    ax.set_title('Decomposition of y in Q-basis')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Least Squares Solution:\")\n",
    "    print(f\"True parameters: {beta_true}\")\n",
    "    print(f\"QR solution: {beta_qr}\")\n",
    "    print(f\"Residual norm: {np.linalg.norm(y - y_pred):.4f}\")\n",
    "    print(f\"Q^T y: {y_Q[:5]} (first 5 components)\")\n",
    "    print(f\"Norm of residual components: {np.linalg.norm(y_Q[2:]):.4f}\")\n",
    "    \n",
    "    return X, y, beta_qr, Q, R\n",
    "\n",
    "# Visualize least squares solution in 2D\n",
    "X_2d, y_2d, beta_2d, Q_2d, R_2d = visualize_least_squares_2d()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b90ebdfe",
   "metadata": {},
   "source": [
    "## 3. QR Algorithm for Computing Eigenvalues\n",
    "\n",
    "The QR algorithm is one of the most important methods for computing eigenvalues of matrices. It's an iterative method that works as follows:\n",
    "\n",
    "1. Start with a matrix $A_0 = A$\n",
    "2. For $k = 0, 1, 2, ...$:\n",
    "   - Compute the QR decomposition: $A_k = Q_k R_k$\n",
    "   - Form the next iteration: $A_{k+1} = R_k Q_k$\n",
    "3. As $k$ increases, $A_k$ converges to a Schur form (upper triangular or block upper triangular with 2×2 blocks on the diagonal for real matrices)\n",
    "\n",
    "Let's implement and visualize this algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0de009c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_algorithm_for_eigenvalues(A, max_iter=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Compute eigenvalues using the basic QR algorithm.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix (torch tensor or numpy array)\n",
    "        max_iter: Maximum number of iterations\n",
    "        tol: Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        eigenvalues: Estimated eigenvalues\n",
    "        iterations: Number of iterations performed\n",
    "        A_final: Final matrix after iterations\n",
    "        convergence: List of convergence metrics per iteration\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    A_k = A.clone()\n",
    "    \n",
    "    # For tracking convergence\n",
    "    convergence = []\n",
    "    \n",
    "    # Store matrices for visualization\n",
    "    matrices = [A_k.clone()]\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        # Compute QR decomposition\n",
    "        Q, R = torch.linalg.qr(A_k)\n",
    "        \n",
    "        # Form the next iteration\n",
    "        A_next = R @ Q\n",
    "        \n",
    "        # Track convergence (sum of off-diagonal elements)\n",
    "        off_diag_sum = torch.sum(torch.abs(A_next - torch.diag(torch.diag(A_next)))).item()\n",
    "        convergence.append(off_diag_sum)\n",
    "        \n",
    "        # Store the matrix\n",
    "        matrices.append(A_next.clone())\n",
    "        \n",
    "        # Check for convergence\n",
    "        if off_diag_sum < tol:\n",
    "            break\n",
    "            \n",
    "        A_k = A_next\n",
    "    \n",
    "    # Extract eigenvalues from the diagonal\n",
    "    eigenvalues = torch.diag(A_k)\n",
    "    \n",
    "    return eigenvalues, k+1, A_k, convergence, matrices\n",
    "\n",
    "def demonstrate_qr_algorithm():\n",
    "    \"\"\"Demonstrate the QR algorithm for eigenvalue computation.\"\"\"\n",
    "    # Create a matrix with known eigenvalues\n",
    "    n = 5\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Start with diagonal matrix with known eigenvalues\n",
    "    true_eigenvalues = np.array([1.0, 2.0, 3.0, 4.0, 5.0])\n",
    "    D = np.diag(true_eigenvalues)\n",
    "    \n",
    "    # Create a random orthogonal matrix\n",
    "    X = np.random.randn(n, n)\n",
    "    Q, _ = np.linalg.qr(X)\n",
    "    \n",
    "    # Form a matrix with the same eigenvalues\n",
    "    A = Q @ D @ Q.T\n",
    "    \n",
    "    # Apply QR algorithm\n",
    "    computed_eigs, iterations, A_final, convergence, matrices = qr_algorithm_for_eigenvalues(A)\n",
    "    \n",
    "    # Also compute eigenvalues with NumPy for comparison\n",
    "    np_eigs = np.linalg.eigvals(A)\n",
    "    \n",
    "    # Sort eigenvalues for comparison\n",
    "    true_eigenvalues = np.sort(true_eigenvalues)\n",
    "    computed_eigs = torch.sort(computed_eigs)[0].numpy()\n",
    "    np_eigs = np.sort(np_eigs)\n",
    "    \n",
    "    # Visualize convergence\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.semilogy(convergence, 'o-')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Off-diagonal Sum\")\n",
    "    plt.title(\"QR Algorithm Convergence\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Compare eigenvalues\n",
    "    plt.subplot(1, 2, 2)\n",
    "    width = 0.25\n",
    "    x = np.arange(n)\n",
    "    \n",
    "    plt.bar(x - width, true_eigenvalues, width, label='True')\n",
    "    plt.bar(x, computed_eigs, width, label='QR Algorithm')\n",
    "    plt.bar(x + width, np_eigs.real, width, label='NumPy')\n",
    "    \n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(\"Eigenvalue Comparison\")\n",
    "    plt.xticks(x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Visualize matrix evolution\n",
    "    # Show a few selected iterations\n",
    "    num_matrices = len(matrices)\n",
    "    iterations_to_show = [0, 1, 2, 5, 10, num_matrices-1]\n",
    "    iterations_to_show = [min(i, num_matrices-1) for i in iterations_to_show]\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    for i, iter_idx in enumerate(iterations_to_show):\n",
    "        plt.subplot(2, 3, i+1)\n",
    "        matrix = matrices[iter_idx].numpy()\n",
    "        sns.heatmap(matrix, annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "        plt.title(f\"Iteration {iter_idx}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"QR Algorithm for Eigenvalues:\")\n",
    "    print(f\"Number of iterations: {iterations}\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"{'Index':<10} {'True':<15} {'QR Algorithm':<15} {'NumPy':<15} {'Error':<15}\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for i in range(n):\n",
    "        qr_error = abs(computed_eigs[i] - true_eigenvalues[i])\n",
    "        print(f\"{i:<10} {true_eigenvalues[i]:<15.6f} {computed_eigs[i]:<15.6f} {np_eigs[i].real:<15.6f} {qr_error:<15.2e}\")\n",
    "    \n",
    "    return A, true_eigenvalues, computed_eigs, matrices\n",
    "\n",
    "# Demonstrate QR algorithm for eigenvalues\n",
    "A_eigen, true_eigs, qr_eigs, matrix_sequence = demonstrate_qr_algorithm()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29bba3d2",
   "metadata": {},
   "source": [
    "### Shifted QR Algorithm\n",
    "\n",
    "The basic QR algorithm can be slow to converge, especially for matrices with clustered eigenvalues. A common enhancement is the \"shifted QR algorithm\", which accelerates convergence by incorporating shifts:\n",
    "\n",
    "1. Start with a matrix $A_0 = A$\n",
    "2. For $k = 0, 1, 2, ...$:\n",
    "   - Choose a shift $\\mu_k$ (often the bottom-right element of $A_k$)\n",
    "   - Compute the QR decomposition: $A_k - \\mu_k I = Q_k R_k$\n",
    "   - Form the next iteration: $A_{k+1} = R_k Q_k + \\mu_k I$\n",
    "\n",
    "Let's implement and compare this with the basic QR algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58baebc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def shifted_qr_algorithm(A, max_iter=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Compute eigenvalues using the shifted QR algorithm.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix (torch tensor or numpy array)\n",
    "        max_iter: Maximum number of iterations\n",
    "        tol: Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        eigenvalues: Estimated eigenvalues\n",
    "        iterations: Number of iterations performed\n",
    "        A_final: Final matrix after iterations\n",
    "        convergence: List of convergence metrics per iteration\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    n = A.shape[0]\n",
    "    A_k = A.clone()\n",
    "    \n",
    "    # For tracking convergence\n",
    "    convergence = []\n",
    "    \n",
    "    # Store matrices for visualization\n",
    "    matrices = [A_k.clone()]\n",
    "    \n",
    "    for k in range(max_iter):\n",
    "        # Choose a shift (bottom-right element)\n",
    "        mu = A_k[-1, -1].item()\n",
    "        \n",
    "        # Compute shifted QR decomposition\n",
    "        A_shifted = A_k - mu * torch.eye(n, dtype=A.dtype)\n",
    "        Q, R = torch.linalg.qr(A_shifted)\n",
    "        \n",
    "        # Form the next iteration\n",
    "        A_next = R @ Q + mu * torch.eye(n, dtype=A.dtype)\n",
    "        \n",
    "        # Track convergence (sum of off-diagonal elements)\n",
    "        off_diag_sum = torch.sum(torch.abs(A_next - torch.diag(torch.diag(A_next)))).item()\n",
    "        convergence.append(off_diag_sum)\n",
    "        \n",
    "        # Store the matrix\n",
    "        matrices.append(A_next.clone())\n",
    "        \n",
    "        # Check for convergence\n",
    "        if off_diag_sum < tol:\n",
    "            break\n",
    "            \n",
    "        A_k = A_next\n",
    "    \n",
    "    # Extract eigenvalues from the diagonal\n",
    "    eigenvalues = torch.diag(A_k)\n",
    "    \n",
    "    return eigenvalues, k+1, A_k, convergence, matrices\n",
    "\n",
    "def compare_qr_algorithms():\n",
    "    \"\"\"Compare basic and shifted QR algorithms.\"\"\"\n",
    "    # Create a matrix with clustered eigenvalues\n",
    "    n = 5\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Clustered eigenvalues\n",
    "    true_eigenvalues = np.array([1.0, 1.1, 3.0, 3.1, 5.0])\n",
    "    D = np.diag(true_eigenvalues)\n",
    "    \n",
    "    # Create a random orthogonal matrix\n",
    "    X = np.random.randn(n, n)\n",
    "    Q, _ = np.linalg.qr(X)\n",
    "    \n",
    "    # Form a matrix with the same eigenvalues\n",
    "    A = Q @ D @ Q.T\n",
    "    \n",
    "    # Apply basic QR algorithm\n",
    "    basic_eigs, basic_iters, _, basic_conv, _ = qr_algorithm_for_eigenvalues(A)\n",
    "    \n",
    "    # Apply shifted QR algorithm\n",
    "    shifted_eigs, shifted_iters, _, shifted_conv, _ = shifted_qr_algorithm(A)\n",
    "    \n",
    "    # Also compute eigenvalues with NumPy for comparison\n",
    "    np_eigs = np.linalg.eigvals(A)\n",
    "    \n",
    "    # Sort eigenvalues for comparison\n",
    "    true_eigenvalues = np.sort(true_eigenvalues)\n",
    "    basic_eigs = torch.sort(basic_eigs)[0].numpy()\n",
    "    shifted_eigs = torch.sort(shifted_eigs)[0].numpy()\n",
    "    np_eigs = np.sort(np_eigs)\n",
    "    \n",
    "    # Visualize convergence\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.semilogy(basic_conv, 'o-', label='Basic QR')\n",
    "    plt.semilogy(shifted_conv, 's-', label='Shifted QR')\n",
    "    plt.xlabel(\"Iteration\")\n",
    "    plt.ylabel(\"Off-diagonal Sum\")\n",
    "    plt.title(\"Convergence Comparison\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Compare eigenvalues\n",
    "    plt.subplot(2, 2, 2)\n",
    "    width = 0.2\n",
    "    x = np.arange(n)\n",
    "    \n",
    "    plt.bar(x - 1.5*width, true_eigenvalues, width, label='True')\n",
    "    plt.bar(x - 0.5*width, basic_eigs, width, label='Basic QR')\n",
    "    plt.bar(x + 0.5*width, shifted_eigs, width, label='Shifted QR')\n",
    "    plt.bar(x + 1.5*width, np_eigs.real, width, label='NumPy')\n",
    "    \n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Eigenvalue\")\n",
    "    plt.title(\"Eigenvalue Comparison\")\n",
    "    plt.xticks(x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot errors\n",
    "    plt.subplot(2, 2, 3)\n",
    "    basic_errors = [abs(basic_eigs[i] - true_eigenvalues[i]) for i in range(n)]\n",
    "    shifted_errors = [abs(shifted_eigs[i] - true_eigenvalues[i]) for i in range(n)]\n",
    "    numpy_errors = [abs(np_eigs[i].real - true_eigenvalues[i]) for i in range(n)]\n",
    "    \n",
    "    plt.bar(x - width, basic_errors, width, label='Basic QR')\n",
    "    plt.bar(x, shifted_errors, width, label='Shifted QR')\n",
    "    plt.bar(x + width, numpy_errors, width, label='NumPy')\n",
    "    \n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Absolute Error\")\n",
    "    plt.title(\"Eigenvalue Error Comparison\")\n",
    "    plt.xticks(x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.yscale('log')\n",
    "    \n",
    "    # Iterations and timing\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.bar([\"Basic QR\", \"Shifted QR\"], [basic_iters, shifted_iters])\n",
    "    plt.ylabel(\"Number of Iterations\")\n",
    "    plt.title(\"Convergence Speed\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"QR Algorithm Comparison:\")\n",
    "    print(f\"Basic QR iterations: {basic_iters}\")\n",
    "    print(f\"Shifted QR iterations: {shifted_iters}\")\n",
    "    print(\"-\" * 100)\n",
    "    print(f\"{'Index':<6} {'True':<10} {'Basic QR':<15} {'Shifted QR':<15} {'NumPy':<15} {'Basic Error':<15} {'Shifted Error':<15}\")\n",
    "    print(\"-\" * 100)\n",
    "    \n",
    "    for i in range(n):\n",
    "        basic_error = abs(basic_eigs[i] - true_eigenvalues[i])\n",
    "        shifted_error = abs(shifted_eigs[i] - true_eigenvalues[i])\n",
    "        print(f\"{i:<6} {true_eigenvalues[i]:<10.6f} {basic_eigs[i]:<15.6f} {shifted_eigs[i]:<15.6f} {np_eigs[i].real:<15.6f} {basic_error:<15.2e} {shifted_error:<15.2e}\")\n",
    "    \n",
    "    return A, true_eigenvalues, basic_eigs, shifted_eigs\n",
    "\n",
    "# Compare basic and shifted QR algorithms\n",
    "A_clustered, true_clustered_eigs, basic_eigs_clustered, shifted_eigs_clustered = compare_qr_algorithms()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9c891a",
   "metadata": {},
   "source": [
    "## 4. Computing the Singular Value Decomposition (SVD) via QR\n",
    "\n",
    "The Singular Value Decomposition (SVD) is another fundamental matrix factorization that can be computed using QR decomposition. One approach is to first compute the eigendecomposition of $A^T A$ or $AA^T$ using the QR algorithm, and then derive the SVD from there.\n",
    "\n",
    "For a matrix $A \\in \\mathbb{R}^{m \\times n}$, the SVD is $A = USV^T$, where:\n",
    "- $U \\in \\mathbb{R}^{m \\times m}$ is an orthogonal matrix\n",
    "- $S \\in \\mathbb{R}^{m \\times n}$ is a diagonal matrix of singular values\n",
    "- $V \\in \\mathbb{R}^{n \\times n}$ is an orthogonal matrix\n",
    "\n",
    "Let's implement a simplified version of this algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8982189c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def svd_via_qr(A, max_iter=100, tol=1e-8):\n",
    "    \"\"\"\n",
    "    Compute the SVD using QR algorithm for eigendecomposition.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix (torch tensor or numpy array)\n",
    "        max_iter: Maximum number of iterations for QR algorithm\n",
    "        tol: Convergence tolerance\n",
    "        \n",
    "    Returns:\n",
    "        U: Left singular vectors\n",
    "        S: Singular values\n",
    "        V: Right singular vectors\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    # Compute A^T A and AA^T\n",
    "    ATA = A.T @ A\n",
    "    AAT = A @ A.T\n",
    "    \n",
    "    # Find eigenvectors of A^T A (right singular vectors)\n",
    "    ATA_eigs, ATA_iters, ATA_final, _, _ = shifted_qr_algorithm(ATA, max_iter, tol)\n",
    "    \n",
    "    # Extract V and singular values\n",
    "    # Note: This simplified version assumes ATA_final is diagonal (may not be exactly true)\n",
    "    S = torch.sqrt(torch.abs(torch.diag(ATA_final)))\n",
    "    \n",
    "    # Sort singular values in descending order\n",
    "    S_sorted, indices = torch.sort(S, descending=True)\n",
    "    \n",
    "    # Compute V via eigenvectors of A^T A\n",
    "    # In practice, we'd need to use a more refined approach to get V exactly\n",
    "    # This is a simplified approximation\n",
    "    V = torch.zeros((n, n), dtype=A.dtype)\n",
    "    for i, idx in enumerate(indices):\n",
    "        v = torch.zeros(n, dtype=A.dtype)\n",
    "        v[idx] = 1.0\n",
    "        \n",
    "        # Apply inverse iteration to get a better eigenvector\n",
    "        for _ in range(5):\n",
    "            v = torch.linalg.solve(ATA - ATA_final[idx, idx] * torch.eye(n, dtype=A.dtype) + 1e-10 * torch.eye(n, dtype=A.dtype), v)\n",
    "            v = v / torch.norm(v)\n",
    "        \n",
    "        V[:, i] = v\n",
    "    \n",
    "    # Compute U via the relation A = USV^T\n",
    "    U = torch.zeros((m, m), dtype=A.dtype)\n",
    "    for i in range(min(m, n)):\n",
    "        if S_sorted[i] > 1e-10:\n",
    "            U[:, i] = (A @ V[:, i]) / S_sorted[i]\n",
    "    \n",
    "    # Complete U to be orthogonal if m > n\n",
    "    if m > n:\n",
    "        # Find a basis for the nullspace of A^T\n",
    "        Q, _ = torch.linalg.qr(U[:, :n], mode='complete')\n",
    "        U = Q\n",
    "    \n",
    "    return U, S_sorted, V\n",
    "\n",
    "def demonstrate_svd():\n",
    "    \"\"\"Demonstrate SVD computation using QR algorithm.\"\"\"\n",
    "    # Create a matrix for SVD\n",
    "    m, n = 5, 3  # More rows than columns\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create a matrix with known singular values\n",
    "    true_singular_values = np.array([10.0, 5.0, 1.0])\n",
    "    \n",
    "    # Create random orthogonal matrices\n",
    "    U_true = scipy.linalg.orth(np.random.randn(m, m))\n",
    "    V_true = scipy.linalg.orth(np.random.randn(n, n))\n",
    "    \n",
    "    # Create S\n",
    "    S_true = np.zeros((m, n))\n",
    "    for i in range(min(m, n)):\n",
    "        S_true[i, i] = true_singular_values[i]\n",
    "    \n",
    "    # Form the matrix\n",
    "    A = U_true @ S_true @ V_true.T\n",
    "    \n",
    "    # Compute SVD using our QR-based method\n",
    "    U, S, V = svd_via_qr(A)\n",
    "    \n",
    "    # Also compute SVD with NumPy for comparison\n",
    "    U_np, S_np, VT_np = np.linalg.svd(A, full_matrices=True)\n",
    "    \n",
    "    # Visualize the results\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    \n",
    "    # Compare singular values\n",
    "    plt.subplot(2, 2, 1)\n",
    "    width = 0.3\n",
    "    x = np.arange(min(m, n))\n",
    "    \n",
    "    plt.bar(x - width/2, true_singular_values, width, label='True')\n",
    "    plt.bar(x + width/2, S.numpy()[:min(m, n)], width, label='QR-based SVD')\n",
    "    \n",
    "    plt.xlabel(\"Index\")\n",
    "    plt.ylabel(\"Singular Value\")\n",
    "    plt.title(\"Singular Value Comparison\")\n",
    "    plt.xticks(x)\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show the original matrix and its reconstruction\n",
    "    plt.subplot(2, 2, 2)\n",
    "    S_diag = torch.zeros((m, n), dtype=A.dtype)\n",
    "    for i in range(min(m, n)):\n",
    "        S_diag[i, i] = S[i]\n",
    "    A_reconstructed = U @ S_diag @ V.T\n",
    "    \n",
    "    reconstruction_error = torch.norm(torch.tensor(A, dtype=torch.float64) - A_reconstructed).item()\n",
    "    \n",
    "    sns.heatmap(A, annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "    plt.title(\"Original Matrix A\")\n",
    "    \n",
    "    plt.subplot(2, 2, 3)\n",
    "    sns.heatmap(A_reconstructed.numpy(), annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "    plt.title(f\"Reconstructed A = USV^T\\nError: {reconstruction_error:.2e}\")\n",
    "    \n",
    "    # Orthogonality check for U and V\n",
    "    plt.subplot(2, 2, 4)\n",
    "    UTU = U.T @ U\n",
    "    VTV = V.T @ V\n",
    "    \n",
    "    ortho_error_U = torch.norm(UTU - torch.eye(m, dtype=A.dtype)).item()\n",
    "    ortho_error_V = torch.norm(VTV - torch.eye(n, dtype=A.dtype)).item()\n",
    "    \n",
    "    plt.bar([\"U^T U - I\", \"V^T V - I\"], [ortho_error_U, ortho_error_V])\n",
    "    plt.ylabel(\"Frobenius Norm\")\n",
    "    plt.title(\"Orthogonality Check\")\n",
    "    plt.yscale('log')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"SVD via QR Algorithm:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Index':<10} {'True S':<15} {'QR-based S':<15} {'NumPy S':<15} {'Error':<15}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    for i in range(min(m, n)):\n",
    "        error = abs(S.numpy()[i] - true_singular_values[i])\n",
    "        print(f\"{i:<10} {true_singular_values[i]:<15.6f} {S.numpy()[i]:<15.6f} {S_np[i]:<15.6f} {error:<15.2e}\")\n",
    "    \n",
    "    print(f\"\\nReconstruction error: {reconstruction_error:.2e}\")\n",
    "    print(f\"Orthogonality error (U): {ortho_error_U:.2e}\")\n",
    "    print(f\"Orthogonality error (V): {ortho_error_V:.2e}\")\n",
    "    \n",
    "    return A, U, S, V, true_singular_values\n",
    "\n",
    "# Demonstrate SVD computation using QR algorithm\n",
    "A_svd, U_svd, S_svd, V_svd, true_S = demonstrate_svd()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5379874",
   "metadata": {},
   "source": [
    "## 5. Applications in Data Science and Machine Learning\n",
    "\n",
    "QR decomposition has numerous applications in data science and machine learning. Let's explore a few of these applications.\n",
    "\n",
    "### 5.1 Principal Component Analysis (PCA) using QR\n",
    "\n",
    "PCA is typically performed using SVD, but it can also be computed using QR decomposition with column pivoting. Let's implement and demonstrate this approach:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b515bb18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def qr_with_column_pivoting(A):\n",
    "    \"\"\"Compute QR decomposition with column pivoting.\"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    P = torch.eye(n, dtype=A.dtype)\n",
    "    \n",
    "    # Use SciPy's implementation for simplicity\n",
    "    Q, R, P_indices = scipy.linalg.qr(A.numpy(), pivoting=True)\n",
    "    \n",
    "    # Convert P indices to a permutation matrix\n",
    "    P_matrix = torch.zeros((n, n), dtype=A.dtype)\n",
    "    for i, p in enumerate(P_indices):\n",
    "        P_matrix[p, i] = 1.0\n",
    "    \n",
    "    return torch.tensor(Q), torch.tensor(R), P_matrix\n",
    "\n",
    "def pca_with_qr(X, k=2):\n",
    "    \"\"\"\n",
    "    Perform PCA using QR decomposition with column pivoting.\n",
    "    \n",
    "    Args:\n",
    "        X: Data matrix (n_samples, n_features)\n",
    "        k: Number of principal components\n",
    "        \n",
    "    Returns:\n",
    "        principal_components: Top k principal components\n",
    "        projected_data: Data projected onto principal components\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.tensor(X, dtype=torch.float64)\n",
    "    \n",
    "    # Center the data\n",
    "    X_centered = X - X.mean(dim=0)\n",
    "    \n",
    "    # Compute the covariance matrix\n",
    "    n = X.shape[0]\n",
    "    cov = (X_centered.T @ X_centered) / (n - 1)\n",
    "    \n",
    "    # Use QR with column pivoting on the covariance matrix\n",
    "    Q, R, P = qr_with_column_pivoting(cov)\n",
    "    \n",
    "    # The first k columns of Q*P are approximations of the principal components\n",
    "    principal_components = (Q @ P)[:, :k]\n",
    "    \n",
    "    # Project the data onto the principal components\n",
    "    projected_data = X_centered @ principal_components\n",
    "    \n",
    "    return principal_components, projected_data\n",
    "\n",
    "def demonstrate_pca():\n",
    "    \"\"\"Demonstrate PCA using QR decomposition.\"\"\"\n",
    "    # Generate a 2D dataset with correlation\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    \n",
    "    # Create correlated features\n",
    "    x1 = np.random.normal(0, 1, n_samples)\n",
    "    x2 = 0.8 * x1 + 0.2 * np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    # Add some noise in a different direction\n",
    "    x3 = 0.1 * x1 + 0.1 * x2 + 0.2 * np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    # Create a data matrix\n",
    "    X = np.column_stack([x1, x2, x3])\n",
    "    \n",
    "    # Perform PCA using our QR-based method\n",
    "    n_components = 2\n",
    "    principal_components, projected_data = pca_with_qr(X, k=n_components)\n",
    "    \n",
    "    # Also compute PCA with sklearn for comparison\n",
    "    from sklearn.decomposition import PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    projected_data_sklearn = pca.fit_transform(X)\n",
    "    \n",
    "    # Visualize the results\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Original data (first two dimensions)\n",
    "    plt.subplot(2, 2, 1)\n",
    "    plt.scatter(X[:, 0], X[:, 1], alpha=0.5)\n",
    "    plt.xlabel(\"Feature 1\")\n",
    "    plt.ylabel(\"Feature 2\")\n",
    "    plt.title(\"Original Data (First 2 Dimensions)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Original data in 3D\n",
    "    ax = plt.subplot(2, 2, 2, projection='3d')\n",
    "    ax.scatter(X[:, 0], X[:, 1], X[:, 2], alpha=0.5)\n",
    "    ax.set_xlabel(\"Feature 1\")\n",
    "    ax.set_ylabel(\"Feature 2\")\n",
    "    ax.set_zlabel(\"Feature 3\")\n",
    "    ax.set_title(\"Original Data (3D)\")\n",
    "    \n",
    "    # Projected data (QR-based PCA)\n",
    "    plt.subplot(2, 2, 3)\n",
    "    plt.scatter(projected_data.numpy()[:, 0], projected_data.numpy()[:, 1], alpha=0.5)\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.title(\"QR-based PCA\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Projected data (sklearn PCA)\n",
    "    plt.subplot(2, 2, 4)\n",
    "    plt.scatter(projected_data_sklearn[:, 0], projected_data_sklearn[:, 1], alpha=0.5)\n",
    "    plt.xlabel(\"PC 1\")\n",
    "    plt.ylabel(\"PC 2\")\n",
    "    plt.title(\"sklearn PCA\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Compare principal components\n",
    "    print(\"QR-based Principal Components:\")\n",
    "    print(principal_components.numpy())\n",
    "    print(\"\\nsklearn Principal Components:\")\n",
    "    print(pca.components_.T)\n",
    "    \n",
    "    # Note: PCA components may differ in sign but should span the same space\n",
    "    \n",
    "    return X, principal_components, projected_data\n",
    "\n",
    "# Demonstrate PCA using QR decomposition\n",
    "X_pca, pc_qr, projected_qr = demonstrate_pca()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8572ae32",
   "metadata": {},
   "source": [
    "### 5.2 QR for Feature Selection\n",
    "\n",
    "QR decomposition with column pivoting can also be used for feature selection. The idea is that the pivoting strategy selects the most linearly independent columns, which can be used to identify the most important features:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b4bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_selection_with_qr(X, k):\n",
    "    \"\"\"\n",
    "    Select k most important features using QR decomposition with column pivoting.\n",
    "    \n",
    "    Args:\n",
    "        X: Data matrix (n_samples, n_features)\n",
    "        k: Number of features to select\n",
    "        \n",
    "    Returns:\n",
    "        selected_indices: Indices of selected features\n",
    "        X_selected: Data with only selected features\n",
    "    \"\"\"\n",
    "    if isinstance(X, np.ndarray):\n",
    "        X = torch.tensor(X, dtype=torch.float64)\n",
    "    \n",
    "    # Center and scale the data\n",
    "    X_centered = X - X.mean(dim=0)\n",
    "    X_scaled = X_centered / X_centered.std(dim=0)\n",
    "    \n",
    "    # Compute QR decomposition with column pivoting\n",
    "    Q, R, P = scipy.linalg.qr(X_scaled.numpy(), pivoting=True)\n",
    "    \n",
    "    # The first k columns selected by the pivoting are the most important\n",
    "    selected_indices = P[:k]\n",
    "    X_selected = X[:, selected_indices]\n",
    "    \n",
    "    return selected_indices, X_selected\n",
    "\n",
    "def demonstrate_feature_selection():\n",
    "    \"\"\"Demonstrate feature selection using QR decomposition.\"\"\"\n",
    "    # Generate a dataset with some informative and some noise features\n",
    "    np.random.seed(42)\n",
    "    n_samples = 200\n",
    "    n_features = 10\n",
    "    \n",
    "    # Create a few important features\n",
    "    x1 = np.random.normal(0, 1, n_samples)\n",
    "    x2 = np.random.normal(0, 1, n_samples)\n",
    "    x3 = np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    # Target is a function of the important features\n",
    "    y = 2*x1 + 0.5*x2 - 1.5*x3 + 0.1*np.random.normal(0, 1, n_samples)\n",
    "    \n",
    "    # Create noise features\n",
    "    X_noise = np.random.normal(0, 1, (n_samples, n_features-3))\n",
    "    \n",
    "    # Combine important and noise features\n",
    "    X = np.column_stack([x1, x2, x3, X_noise])\n",
    "    \n",
    "    # Create feature names\n",
    "    feature_names = [\"Important 1\", \"Important 2\", \"Important 3\"] + [f\"Noise {i+1}\" for i in range(n_features-3)]\n",
    "    \n",
    "    # Select features using QR decomposition\n",
    "    k = 5  # Number of features to select\n",
    "    selected_indices, X_selected = feature_selection_with_qr(X, k)\n",
    "    \n",
    "    # For comparison, use a linear model with L1 regularization (Lasso)\n",
    "    from sklearn.linear_model import Lasso\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    lasso = Lasso(alpha=0.1)\n",
    "    lasso.fit(X_scaled, y)\n",
    "    \n",
    "    # Get feature importance from Lasso\n",
    "    lasso_importance = np.abs(lasso.coef_)\n",
    "    lasso_selected = np.argsort(lasso_importance)[::-1][:k]\n",
    "    \n",
    "    # Visualize the results\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    \n",
    "    # Show which features were selected\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.bar(range(n_features), np.zeros(n_features), alpha=0.1)  # Placeholder bars\n",
    "    plt.bar(selected_indices, np.ones(k), alpha=0.7, label='Selected by QR')\n",
    "    \n",
    "    # Highlight the truly important features\n",
    "    for i in range(3):\n",
    "        plt.axvline(x=i, color='r', linestyle='--', alpha=0.5)\n",
    "    \n",
    "    plt.xlabel(\"Feature Index\")\n",
    "    plt.title(f\"QR-based Feature Selection\\nSelected: {', '.join([feature_names[i] for i in selected_indices])}\")\n",
    "    plt.xticks(range(n_features), feature_names, rotation=90)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Show Lasso feature importance\n",
    "    plt.subplot(1, 2, 2)\n",
    "    sorted_idx = np.argsort(lasso_importance)[::-1]\n",
    "    plt.bar(range(n_features), lasso_importance[sorted_idx])\n",
    "    \n",
    "    plt.xlabel(\"Feature\")\n",
    "    plt.title(\"Lasso Feature Importance\")\n",
    "    plt.xticks(range(n_features), [feature_names[i] for i in sorted_idx], rotation=90)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Evaluate feature selection by training a simple model\n",
    "    from sklearn.linear_model import LinearRegression\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.metrics import mean_squared_error, r2_score\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Train on all features\n",
    "    reg_all = LinearRegression()\n",
    "    reg_all.fit(X_train, y_train)\n",
    "    y_pred_all = reg_all.predict(X_test)\n",
    "    rmse_all = np.sqrt(mean_squared_error(y_test, y_pred_all))\n",
    "    r2_all = r2_score(y_test, y_pred_all)\n",
    "    \n",
    "    # Train on QR-selected features\n",
    "    reg_qr = LinearRegression()\n",
    "    reg_qr.fit(X_train[:, selected_indices], y_train)\n",
    "    y_pred_qr = reg_qr.predict(X_test[:, selected_indices])\n",
    "    rmse_qr = np.sqrt(mean_squared_error(y_test, y_pred_qr))\n",
    "    r2_qr = r2_score(y_test, y_pred_qr)\n",
    "    \n",
    "    # Train on Lasso-selected features\n",
    "    reg_lasso = LinearRegression()\n",
    "    reg_lasso.fit(X_train[:, lasso_selected], y_train)\n",
    "    y_pred_lasso = reg_lasso.predict(X_test[:, lasso_selected])\n",
    "    rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "    r2_lasso = r2_score(y_test, y_pred_lasso)\n",
    "    \n",
    "    # Train on only the true important features\n",
    "    reg_true = LinearRegression()\n",
    "    reg_true.fit(X_train[:, :3], y_train)\n",
    "    y_pred_true = reg_true.predict(X_test[:, :3])\n",
    "    rmse_true = np.sqrt(mean_squared_error(y_test, y_pred_true))\n",
    "    r2_true = r2_score(y_test, y_pred_true)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"Feature Selection Results:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Method':<20} {'Features':<40} {'RMSE':<10} {'R²':<10}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"{'All Features':<20} {'All 10 features':<40} {rmse_all:<10.4f} {r2_all:<10.4f}\")\n",
    "    print(f\"{'QR Selection':<20} {', '.join([feature_names[i] for i in selected_indices]):<40} {rmse_qr:<10.4f} {r2_qr:<10.4f}\")\n",
    "    print(f\"{'Lasso Selection':<20} {', '.join([feature_names[i] for i in lasso_selected]):<40} {rmse_lasso:<10.4f} {r2_lasso:<10.4f}\")\n",
    "    print(f\"{'True Important':<20} {'Important 1, Important 2, Important 3':<40} {rmse_true:<10.4f} {r2_true:<10.4f}\")\n",
    "    \n",
    "    return X, y, selected_indices, lasso_selected\n",
    "\n",
    "# Demonstrate feature selection using QR decomposition\n",
    "X_fs, y_fs, qr_selected, lasso_selected = demonstrate_feature_selection()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3949629",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored various applications of QR decomposition in computational linear algebra and data science:\n",
    "\n",
    "1. **Solving Linear Systems**: We demonstrated how QR decomposition can be used to solve systems of linear equations, comparing its performance with other methods.\n",
    "\n",
    "2. **Least Squares Problems**: We showed how QR decomposition provides a numerically stable way to solve least squares problems without explicitly forming the normal equations.\n",
    "\n",
    "3. **QR Algorithm for Eigenvalues**: We implemented and visualized the QR algorithm for computing eigenvalues, including an enhanced version with shifts for faster convergence.\n",
    "\n",
    "4. **Singular Value Decomposition (SVD)**: We demonstrated how QR decomposition can be used as a building block for computing the SVD of a matrix.\n",
    "\n",
    "5. **Data Science Applications**: We explored how QR decomposition can be used for dimensionality reduction (PCA) and feature selection.\n",
    "\n",
    "Key advantages of QR decomposition in these applications:\n",
    "\n",
    "- **Numerical stability**: QR decomposition preserves orthogonality, making it numerically stable for a wide range of problems.\n",
    "- **Versatility**: It serves as a building block for many other important algorithms in numerical linear algebra.\n",
    "- **Efficiency**: For certain problems, QR-based methods can be more computationally efficient than alternatives.\n",
    "\n",
    "The main limitation is the computational cost (O(mn²) for an m×n matrix), which can be prohibitive for very large matrices. However, for many practical applications, the numerical stability advantages of QR decomposition outweigh the computational cost.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
