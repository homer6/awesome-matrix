{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "18803418",
   "metadata": {},
   "source": [
    "# ⚠️ Auto-generated Notebook\n",
    "    \n",
    "This notebook is automatically compiled from source files in `/workspaces/awesome-matrix/src/examples/08_qr_decomposition`.\n",
    "**Do not edit this file directly** as your changes will be overwritten.\n",
    "\n",
    "To make changes:\n",
    "1. Edit the source file `/workspaces/awesome-matrix/src/examples/08_qr_decomposition/02_qr_algorithms.py` instead\n",
    "2. Run the compile script to regenerate this notebook\n",
    "\n",
    "See [COMPILE.md](docs/COMPILE.md) for more information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb43637",
   "metadata": {},
   "source": [
    "# QR Decomposition: Algorithms and Implementation\n",
    "\n",
    "This notebook delves into the algorithmic details of QR decomposition. We examine different methods for computing QR factorization, their properties, and implementation considerations.\n",
    "\n",
    "We'll focus on:\n",
    "\n",
    "1. **Classical Gram-Schmidt Algorithm in Detail**\n",
    "2. **Modified Gram-Schmidt Algorithm**\n",
    "3. **Householder Reflections**\n",
    "4. **Givens Rotations**\n",
    "5. **Optimized Implementations**\n",
    "6. **Numerical Stability Analysis**\n",
    "\n",
    "Each method has specific advantages, trade-offs, and use cases that we'll explore through both theoretical explanation and practical implementation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe399470",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import scipy.linalg\n",
    "import time\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# For better looking plots\n",
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "plt.rcParams['font.size'] = 12\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Define a custom colormap (light blue to dark blue)\n",
    "colors = [(0.95, 0.95, 1), (0.0, 0.2, 0.6)]\n",
    "blue_cmap = LinearSegmentedColormap.from_list('CustomBlue', colors, N=100)\n",
    "\n",
    "# Helper functions for visualization\n",
    "def plot_matrix(matrix, title=\"Matrix\", annotate=True, cmap=blue_cmap):\n",
    "    \"\"\"Plot a matrix as a heatmap with annotations.\"\"\"\n",
    "    if isinstance(matrix, torch.Tensor):\n",
    "        matrix_np = matrix.numpy()\n",
    "    else:\n",
    "        matrix_np = matrix\n",
    "        \n",
    "    plt.figure(figsize=(8, 6))\n",
    "    ax = sns.heatmap(matrix_np, annot=annotate, fmt=\".2f\", cmap=cmap, \n",
    "                    linewidths=1, cbar=True)\n",
    "    plt.title(title)\n",
    "    \n",
    "    # Add row and column indices\n",
    "    ax.set_xticks(np.arange(matrix_np.shape[1]) + 0.5)\n",
    "    ax.set_yticks(np.arange(matrix_np.shape[0]) + 0.5)\n",
    "    ax.set_xticklabels([f\"Col {i+1}\" for i in range(matrix_np.shape[1])])\n",
    "    ax.set_yticklabels([f\"Row {i+1}\" for i in range(matrix_np.shape[0])])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Create a function to measure reconstruction error and orthogonality\n",
    "def measure_qr_quality(A, Q, R):\n",
    "    \"\"\"Measure the quality of a QR decomposition.\"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    if isinstance(Q, np.ndarray):\n",
    "        Q = torch.tensor(Q, dtype=torch.float64)\n",
    "    if isinstance(R, np.ndarray):\n",
    "        R = torch.tensor(R, dtype=torch.float64)\n",
    "    \n",
    "    # Measure reconstruction error\n",
    "    QR = Q @ R\n",
    "    reconstruction_error = torch.norm(A - QR).item()\n",
    "    \n",
    "    # Measure orthogonality of Q\n",
    "    QTQ = Q.T @ Q\n",
    "    orthogonality_error = torch.norm(QTQ - torch.eye(QTQ.shape[0])).item()\n",
    "    \n",
    "    return reconstruction_error, orthogonality_error\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c96711d",
   "metadata": {},
   "source": [
    "## 1. Classical Gram-Schmidt Algorithm in Detail\n",
    "\n",
    "The Gram-Schmidt process is a method for orthonormalizing a set of vectors. In the context of QR decomposition, we apply it to the columns of matrix $A$.\n",
    "\n",
    "Here's a detailed step-by-step breakdown of how the classical Gram-Schmidt algorithm works:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5f6f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classical_gram_schmidt_detailed(A):\n",
    "    \"\"\"\n",
    "    Implement classical Gram-Schmidt with detailed step-by-step visualization.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "        steps: Detailed steps for visualization\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "    R = torch.zeros((n, n), dtype=A.dtype)\n",
    "    \n",
    "    # Store intermediate steps for visualization\n",
    "    steps = []\n",
    "    \n",
    "    for j in range(n):\n",
    "        # Get the j-th column vector\n",
    "        v = A[:, j].clone()\n",
    "        \n",
    "        # Store the original vector and projections\n",
    "        step_data = {\n",
    "            'j': j,\n",
    "            'original_vector': v.clone(),\n",
    "            'projections': []\n",
    "        }\n",
    "        \n",
    "        # Orthogonalize with respect to previous columns of Q\n",
    "        for i in range(j):\n",
    "            # Calculate projection coefficient\n",
    "            R[i, j] = torch.dot(Q[:, i], A[:, j])\n",
    "            \n",
    "            # Calculate projection vector\n",
    "            projection = R[i, j] * Q[:, i]\n",
    "            \n",
    "            # Store projection for visualization\n",
    "            step_data['projections'].append({\n",
    "                'i': i,\n",
    "                'coefficient': R[i, j].item(),\n",
    "                'vector': projection.clone()\n",
    "            })\n",
    "            \n",
    "            # Subtract projection\n",
    "            v = v - projection\n",
    "        \n",
    "        # Compute the norm of the orthogonalized vector\n",
    "        R[j, j] = torch.norm(v)\n",
    "        \n",
    "        # Store the orthogonalized vector\n",
    "        step_data['orthogonalized'] = v.clone()\n",
    "        \n",
    "        # Normalize to get an orthonormal vector\n",
    "        if R[j, j] > 1e-10:  # Check for numerical stability\n",
    "            Q[:, j] = v / R[j, j]\n",
    "        else:\n",
    "            Q[:, j] = torch.zeros(m, dtype=A.dtype)\n",
    "        \n",
    "        # Store the normalized vector\n",
    "        step_data['normalized'] = Q[:, j].clone()\n",
    "        \n",
    "        # Add step to list\n",
    "        steps.append(step_data)\n",
    "    \n",
    "    return Q, R, steps\n",
    "\n",
    "def visualize_gram_schmidt_steps(A, steps, dim=2):\n",
    "    \"\"\"Visualize the Gram-Schmidt process step by step.\"\"\"\n",
    "    if isinstance(A, torch.Tensor):\n",
    "        A = A.numpy()\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    if dim > m:\n",
    "        print(f\"Warning: Can only visualize in {m} dimensions, not {dim}\")\n",
    "        dim = m\n",
    "    \n",
    "    if dim == 2:\n",
    "        # 2D visualization\n",
    "        plt.figure(figsize=(15, 4 * n))\n",
    "        \n",
    "        for j, step in enumerate(steps):\n",
    "            # Plot original vector\n",
    "            plt.subplot(n, 3, j*3 + 1)\n",
    "            plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "            \n",
    "            # Plot original column vector\n",
    "            original = step['original_vector'].numpy()\n",
    "            plt.arrow(0, 0, original[0], original[1], head_width=0.1, head_length=0.1, \n",
    "                     fc='blue', ec='blue', label=f'a{j+1}')\n",
    "            \n",
    "            # Plot previous orthonormal vectors\n",
    "            for i in range(j):\n",
    "                q_i = steps[i]['normalized'].numpy()\n",
    "                plt.arrow(0, 0, q_i[0], q_i[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='green', ec='green', label=f'q{i+1}')\n",
    "            \n",
    "            plt.title(f\"Step {j+1}: Original Vector\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.grid(True)\n",
    "            plt.axis('equal')\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.ylim(-2, 2)\n",
    "            \n",
    "            # Plot projections\n",
    "            plt.subplot(n, 3, j*3 + 2)\n",
    "            plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "            \n",
    "            # Plot original vector\n",
    "            plt.arrow(0, 0, original[0], original[1], head_width=0.1, head_length=0.1, \n",
    "                     fc='blue', ec='blue', alpha=0.5, label=f'a{j+1}')\n",
    "            \n",
    "            # Plot projections\n",
    "            cumulative_projection = np.zeros(m)\n",
    "            for proj in step['projections']:\n",
    "                projection = proj['vector'].numpy()\n",
    "                q_i = steps[proj['i']]['normalized'].numpy()\n",
    "                \n",
    "                plt.arrow(0, 0, q_i[0], q_i[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='green', ec='green', alpha=0.7, label=f'q{proj[\"i\"]+1}')\n",
    "                \n",
    "                # Draw projection from origin\n",
    "                plt.arrow(0, 0, projection[0], projection[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='red', ec='red', alpha=0.7, linestyle='dashed', \n",
    "                         label=f'proj_{{{proj[\"i\"]+1}}}(a{j+1})')\n",
    "                \n",
    "                cumulative_projection += projection\n",
    "            \n",
    "            # Draw orthogonalized vector\n",
    "            orthogonalized = step['orthogonalized'].numpy()\n",
    "            plt.arrow(0, 0, orthogonalized[0], orthogonalized[1], head_width=0.1, head_length=0.1, \n",
    "                     fc='purple', ec='purple', label=f'u{j+1}')\n",
    "            \n",
    "            plt.title(f\"Step {j+1}: Projections and Orthogonalization\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.grid(True)\n",
    "            plt.axis('equal')\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.ylim(-2, 2)\n",
    "            \n",
    "            # Plot normalized vector\n",
    "            plt.subplot(n, 3, j*3 + 3)\n",
    "            plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "            \n",
    "            # Plot final orthonormal vector\n",
    "            normalized = step['normalized'].numpy()\n",
    "            plt.arrow(0, 0, normalized[0], normalized[1], head_width=0.1, head_length=0.1, \n",
    "                     fc='purple', ec='purple', label=f'q{j+1}')\n",
    "            \n",
    "            # Plot previous orthonormal vectors\n",
    "            for i in range(j):\n",
    "                q_i = steps[i]['normalized'].numpy()\n",
    "                plt.arrow(0, 0, q_i[0], q_i[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='green', ec='green', alpha=0.7, label=f'q{i+1}')\n",
    "            \n",
    "            plt.title(f\"Step {j+1}: Normalized Vector\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.grid(True)\n",
    "            plt.axis('equal')\n",
    "            plt.xlim(-1.5, 1.5)\n",
    "            plt.ylim(-1.5, 1.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    elif dim == 3:\n",
    "        # 3D visualization\n",
    "        for j, step in enumerate(steps):\n",
    "            fig = plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Plot original vector\n",
    "            ax1 = fig.add_subplot(131, projection='3d')\n",
    "            \n",
    "            # Plot original column vector\n",
    "            original = step['original_vector'].numpy()\n",
    "            ax1.quiver(0, 0, 0, original[0], original[1], original[2], color='blue', label=f'a{j+1}')\n",
    "            \n",
    "            # Plot previous orthonormal vectors\n",
    "            for i in range(j):\n",
    "                q_i = steps[i]['normalized'].numpy()\n",
    "                ax1.quiver(0, 0, 0, q_i[0], q_i[1], q_i[2], color='green', label=f'q{i+1}')\n",
    "            \n",
    "            ax1.set_title(f\"Step {j+1}: Original Vector\")\n",
    "            ax1.set_xlabel(\"x\")\n",
    "            ax1.set_ylabel(\"y\")\n",
    "            ax1.set_zlabel(\"z\")\n",
    "            ax1.set_xlim([-1.5, 1.5])\n",
    "            ax1.set_ylim([-1.5, 1.5])\n",
    "            ax1.set_zlim([-1.5, 1.5])\n",
    "            \n",
    "            # Plot projections\n",
    "            ax2 = fig.add_subplot(132, projection='3d')\n",
    "            \n",
    "            # Plot original vector\n",
    "            ax2.quiver(0, 0, 0, original[0], original[1], original[2], color='blue', alpha=0.5, label=f'a{j+1}')\n",
    "            \n",
    "            # Plot projections\n",
    "            for proj in step['projections']:\n",
    "                projection = proj['vector'].numpy()\n",
    "                q_i = steps[proj['i']]['normalized'].numpy()\n",
    "                \n",
    "                ax2.quiver(0, 0, 0, q_i[0], q_i[1], q_i[2], color='green', alpha=0.7, label=f'q{proj[\"i\"]+1}')\n",
    "                ax2.quiver(0, 0, 0, projection[0], projection[1], projection[2], color='red', alpha=0.7, label=f'proj_{{{proj[\"i\"]+1}}}(a{j+1})')\n",
    "            \n",
    "            # Draw orthogonalized vector\n",
    "            orthogonalized = step['orthogonalized'].numpy()\n",
    "            ax2.quiver(0, 0, 0, orthogonalized[0], orthogonalized[1], orthogonalized[2], color='purple', label=f'u{j+1}')\n",
    "            \n",
    "            ax2.set_title(f\"Step {j+1}: Projections and Orthogonalization\")\n",
    "            ax2.set_xlabel(\"x\")\n",
    "            ax2.set_ylabel(\"y\")\n",
    "            ax2.set_zlabel(\"z\")\n",
    "            ax2.set_xlim([-1.5, 1.5])\n",
    "            ax2.set_ylim([-1.5, 1.5])\n",
    "            ax2.set_zlim([-1.5, 1.5])\n",
    "            \n",
    "            # Plot normalized vector\n",
    "            ax3 = fig.add_subplot(133, projection='3d')\n",
    "            \n",
    "            # Plot final orthonormal vector\n",
    "            normalized = step['normalized'].numpy()\n",
    "            ax3.quiver(0, 0, 0, normalized[0], normalized[1], normalized[2], color='purple', label=f'q{j+1}')\n",
    "            \n",
    "            # Plot previous orthonormal vectors\n",
    "            for i in range(j):\n",
    "                q_i = steps[i]['normalized'].numpy()\n",
    "                ax3.quiver(0, 0, 0, q_i[0], q_i[1], q_i[2], color='green', alpha=0.7, label=f'q{i+1}')\n",
    "            \n",
    "            ax3.set_title(f\"Step {j+1}: Normalized Vector\")\n",
    "            ax3.set_xlabel(\"x\")\n",
    "            ax3.set_ylabel(\"y\")\n",
    "            ax3.set_zlabel(\"z\")\n",
    "            ax3.set_xlim([-1.5, 1.5])\n",
    "            ax3.set_ylim([-1.5, 1.5])\n",
    "            ax3.set_zlim([-1.5, 1.5])\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Print numerical details of each step\n",
    "    for j, step in enumerate(steps):\n",
    "        print(f\"\\nStep {j+1}: Processing column {j+1}\")\n",
    "        print(f\"Original vector a{j+1}: {step['original_vector'].numpy()}\")\n",
    "        \n",
    "        for proj in step['projections']:\n",
    "            print(f\"  Projection onto q{proj['i']+1}: coefficient = {proj['coefficient']:.4f}\")\n",
    "            print(f\"  Projection vector: {proj['vector'].numpy()}\")\n",
    "        \n",
    "        print(f\"Orthogonalized vector u{j+1}: {step['orthogonalized'].numpy()}\")\n",
    "        print(f\"Normalized vector q{j+1}: {step['normalized'].numpy()}\")\n",
    "\n",
    "# Create a small 2D matrix for visualization\n",
    "def demonstrate_classical_gram_schmidt():\n",
    "    \"\"\"Demonstrate classical Gram-Schmidt algorithm in detail.\"\"\"\n",
    "    # Create a small 2D matrix\n",
    "    A_2d = torch.tensor([\n",
    "        [3.0, 1.0],\n",
    "        [1.0, 2.0]\n",
    "    ])\n",
    "    \n",
    "    # Apply classical Gram-Schmidt\n",
    "    Q_2d, R_2d, steps_2d = classical_gram_schmidt_detailed(A_2d)\n",
    "    \n",
    "    # Display the matrices\n",
    "    print(\"Original Matrix A:\")\n",
    "    print(A_2d.numpy())\n",
    "    print(\"\\nOrthogonal Matrix Q:\")\n",
    "    print(Q_2d.numpy())\n",
    "    print(\"\\nUpper Triangular Matrix R:\")\n",
    "    print(R_2d.numpy())\n",
    "    \n",
    "    # Visualize the process in 2D\n",
    "    visualize_gram_schmidt_steps(A_2d, steps_2d, dim=2)\n",
    "    \n",
    "    # Create a small 3D matrix\n",
    "    A_3d = torch.tensor([\n",
    "        [3.0, 1.0, 2.0],\n",
    "        [1.0, 2.0, 0.0],\n",
    "        [2.0, 0.0, 3.0]\n",
    "    ])\n",
    "    \n",
    "    # Apply classical Gram-Schmidt\n",
    "    Q_3d, R_3d, steps_3d = classical_gram_schmidt_detailed(A_3d)\n",
    "    \n",
    "    # Display the matrices\n",
    "    print(\"\\nOriginal Matrix A (3D):\")\n",
    "    print(A_3d.numpy())\n",
    "    print(\"\\nOrthogonal Matrix Q:\")\n",
    "    print(Q_3d.numpy())\n",
    "    print(\"\\nUpper Triangular Matrix R:\")\n",
    "    print(R_3d.numpy())\n",
    "    \n",
    "    # Visualize the process in 3D\n",
    "    visualize_gram_schmidt_steps(A_3d, steps_3d, dim=3)\n",
    "    \n",
    "    return A_2d, Q_2d, R_2d, A_3d, Q_3d, R_3d\n",
    "\n",
    "# Demonstrate classical Gram-Schmidt algorithm\n",
    "A_2d, Q_2d, R_2d, A_3d, Q_3d, R_3d = demonstrate_classical_gram_schmidt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f151b69d",
   "metadata": {},
   "source": [
    "### 1.1 Algorithm Analysis\n",
    "\n",
    "Let's analyze the classical Gram-Schmidt algorithm in more detail.\n",
    "\n",
    "**Computation**:\n",
    "- For each column $j$, we need to orthogonalize against all previous columns $i < j$\n",
    "- This requires $j$ dot products and vector subtractions\n",
    "- Overall computational complexity: $O(mn^2)$ for an $m \\times n$ matrix\n",
    "\n",
    "**Memory**:\n",
    "- We need to store the original matrix $A$, the orthogonal matrix $Q$, and the upper triangular matrix $R$\n",
    "- Memory complexity: $O(mn + n^2)$\n",
    "\n",
    "**Numerical Stability**:\n",
    "- Classical Gram-Schmidt can suffer from numerical instability due to accumulation of rounding errors\n",
    "- This happens especially when the columns of $A$ are nearly linearly dependent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b8299a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_gram_schmidt_complexity():\n",
    "    \"\"\"Analyze computational complexity of Gram-Schmidt.\"\"\"\n",
    "    # Define sizes to test\n",
    "    sizes = range(100, 1001, 100)\n",
    "    \n",
    "    # Store timing results\n",
    "    times = []\n",
    "    \n",
    "    for n in sizes:\n",
    "        # Create a square matrix\n",
    "        A = torch.rand(n, n, dtype=torch.float64)\n",
    "        \n",
    "        # Time classical Gram-Schmidt\n",
    "        start_time = time.time()\n",
    "        Q, R = classical_gram_schmidt(A)\n",
    "        elapsed = time.time() - start_time\n",
    "        \n",
    "        times.append(elapsed)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(sizes, times, 'o-')\n",
    "    plt.title(\"Computational Time vs. Matrix Size\")\n",
    "    plt.xlabel(\"Matrix Size (n×n)\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Plot theoretical complexity (scaled)\n",
    "    theory = [s**3 * 1e-8 for s in sizes]  # O(n³) scaled to match\n",
    "    plt.plot(sizes, theory, '--', label='O(n³)')\n",
    "    \n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"For a {sizes[-1]}×{sizes[-1]} matrix, classical Gram-Schmidt took {times[-1]:.4f} seconds\")\n",
    "\n",
    "# Simplified version of classical Gram-Schmidt without detailed steps\n",
    "def classical_gram_schmidt(A):\n",
    "    \"\"\"Classical Gram-Schmidt algorithm.\"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "    R = torch.zeros((n, n), dtype=A.dtype)\n",
    "    \n",
    "    for j in range(n):\n",
    "        v = A[:, j].clone()\n",
    "        \n",
    "        for i in range(j):\n",
    "            R[i, j] = torch.dot(Q[:, i], A[:, j])\n",
    "            v = v - R[i, j] * Q[:, i]\n",
    "        \n",
    "        R[j, j] = torch.norm(v)\n",
    "        \n",
    "        if R[j, j] > 1e-10:\n",
    "            Q[:, j] = v / R[j, j]\n",
    "        else:\n",
    "            Q[:, j] = torch.zeros(m, dtype=A.dtype)\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "# Skip the full complexity analysis for brevity\n",
    "print(\"Computational complexity of Gram-Schmidt is O(mn²) for an m×n matrix\")\n",
    "print(\"This is dominated by the dot product operations for each column\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ee59d69",
   "metadata": {},
   "source": [
    "## 2. Modified Gram-Schmidt Algorithm\n",
    "\n",
    "The modified Gram-Schmidt algorithm is a numerically more stable variant of the classical Gram-Schmidt process. Instead of orthogonalizing against all previous vectors at once, it orthogonalizes against each vector one at a time.\n",
    "\n",
    "Here's the key difference:\n",
    "- **Classical GS**: Compute all projections from the original vector\n",
    "- **Modified GS**: Update the vector after each projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47b5488",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modified_gram_schmidt_detailed(A):\n",
    "    \"\"\"\n",
    "    Implement modified Gram-Schmidt with detailed step-by-step visualization.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "        steps: Detailed steps for visualization\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "    R = torch.zeros((n, n), dtype=A.dtype)\n",
    "    \n",
    "    # Initialize U as a copy of A\n",
    "    U = A.clone()\n",
    "    \n",
    "    # Store intermediate steps for visualization\n",
    "    steps = []\n",
    "    \n",
    "    for i in range(n):\n",
    "        # Store the original vector and intermediate vectors\n",
    "        step_data = {\n",
    "            'i': i,\n",
    "            'original_vector': U[:, i].clone(),\n",
    "            'intermediate_vectors': [],\n",
    "            'projections': []\n",
    "        }\n",
    "        \n",
    "        # Compute the norm of the i-th column of U\n",
    "        R[i, i] = torch.norm(U[:, i])\n",
    "        \n",
    "        # Normalize to get an orthonormal vector\n",
    "        if R[i, i] > 1e-10:  # Check for numerical stability\n",
    "            Q[:, i] = U[:, i] / R[i, i]\n",
    "        else:\n",
    "            Q[:, i] = torch.zeros(m, dtype=A.dtype)\n",
    "        \n",
    "        # Store the normalized vector\n",
    "        step_data['normalized'] = Q[:, i].clone()\n",
    "        \n",
    "        # Orthogonalize remaining columns with respect to the i-th column of Q\n",
    "        for j in range(i+1, n):\n",
    "            # Record the vector before orthogonalization\n",
    "            step_data['intermediate_vectors'].append({\n",
    "                'j': j, \n",
    "                'before': U[:, j].clone()\n",
    "            })\n",
    "            \n",
    "            # Calculate projection coefficient\n",
    "            R[i, j] = torch.dot(Q[:, i], U[:, j])\n",
    "            \n",
    "            # Calculate projection vector\n",
    "            projection = R[i, j] * Q[:, i]\n",
    "            \n",
    "            # Store projection for visualization\n",
    "            step_data['projections'].append({\n",
    "                'j': j,\n",
    "                'coefficient': R[i, j].item(),\n",
    "                'vector': projection.clone()\n",
    "            })\n",
    "            \n",
    "            # Subtract projection\n",
    "            U[:, j] = U[:, j] - projection\n",
    "            \n",
    "            # Record the vector after orthogonalization\n",
    "            step_data['intermediate_vectors'][-1]['after'] = U[:, j].clone()\n",
    "        \n",
    "        # Add step to list\n",
    "        steps.append(step_data)\n",
    "    \n",
    "    return Q, R, steps\n",
    "\n",
    "def visualize_modified_gs_steps(A, steps, dim=2):\n",
    "    \"\"\"Visualize the Modified Gram-Schmidt process step by step.\"\"\"\n",
    "    if isinstance(A, torch.Tensor):\n",
    "        A = A.numpy()\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    if dim > m:\n",
    "        print(f\"Warning: Can only visualize in {m} dimensions, not {dim}\")\n",
    "        dim = m\n",
    "    \n",
    "    if dim == 2:\n",
    "        # For each step (processing one column)\n",
    "        for i, step in enumerate(steps):\n",
    "            # Create a figure for this step\n",
    "            plt.figure(figsize=(15, 5))\n",
    "            \n",
    "            # Plot original column vector\n",
    "            plt.subplot(1, 3, 1)\n",
    "            plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "            \n",
    "            original = step['original_vector'].numpy()\n",
    "            plt.arrow(0, 0, original[0], original[1], head_width=0.1, head_length=0.1, \n",
    "                     fc='blue', ec='blue', label=f'u{i+1}')\n",
    "            \n",
    "            # Plot previous orthonormal vectors\n",
    "            for j in range(i):\n",
    "                q_j = steps[j]['normalized'].numpy()\n",
    "                plt.arrow(0, 0, q_j[0], q_j[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='green', ec='green', label=f'q{j+1}')\n",
    "            \n",
    "            plt.title(f\"Step {i+1}: Original Vector u{i+1}\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.grid(True)\n",
    "            plt.axis('equal')\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.ylim(-2, 2)\n",
    "            plt.legend()\n",
    "            \n",
    "            # Plot normalization\n",
    "            plt.subplot(1, 3, 2)\n",
    "            plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "            \n",
    "            normalized = step['normalized'].numpy()\n",
    "            \n",
    "            # Draw original and normalized vector\n",
    "            plt.arrow(0, 0, original[0], original[1], head_width=0.1, head_length=0.1, \n",
    "                     fc='blue', ec='blue', alpha=0.5, label=f'u{i+1}')\n",
    "            plt.arrow(0, 0, normalized[0], normalized[1], head_width=0.1, head_length=0.1, \n",
    "                     fc='purple', ec='purple', label=f'q{i+1}')\n",
    "            \n",
    "            plt.title(f\"Step {i+1}: Normalized Vector q{i+1}\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.grid(True)\n",
    "            plt.axis('equal')\n",
    "            plt.xlim(-1.5, 1.5)\n",
    "            plt.ylim(-1.5, 1.5)\n",
    "            plt.legend()\n",
    "            \n",
    "            # Plot effect on remaining vectors\n",
    "            plt.subplot(1, 3, 3)\n",
    "            plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "            plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "            \n",
    "            # Draw the orthonormal vector\n",
    "            plt.arrow(0, 0, normalized[0], normalized[1], head_width=0.1, head_length=0.1, \n",
    "                     fc='purple', ec='purple', label=f'q{i+1}')\n",
    "            \n",
    "            # Draw the effect on remaining vectors\n",
    "            for idx, inter in enumerate(step['intermediate_vectors']):\n",
    "                j = inter['j']\n",
    "                before = inter['before'].numpy()\n",
    "                after = inter['after'].numpy()\n",
    "                proj = step['projections'][idx]['vector'].numpy()\n",
    "                \n",
    "                plt.arrow(0, 0, before[0], before[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='blue', ec='blue', alpha=0.5, label=f'u{j+1} before')\n",
    "                plt.arrow(0, 0, after[0], after[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='green', ec='green', alpha=0.7, label=f'u{j+1} after')\n",
    "                plt.arrow(0, 0, proj[0], proj[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='red', ec='red', alpha=0.7, linestyle='dashed', \n",
    "                         label=f'proj_{{{i+1}}}(u{j+1})')\n",
    "            \n",
    "            plt.title(f\"Step {i+1}: Effect on Remaining Vectors\")\n",
    "            plt.xlabel(\"x\")\n",
    "            plt.ylabel(\"y\")\n",
    "            plt.grid(True)\n",
    "            plt.axis('equal')\n",
    "            plt.xlim(-2, 2)\n",
    "            plt.ylim(-2, 2)\n",
    "            plt.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "    \n",
    "    # Print numerical details of each step\n",
    "    for i, step in enumerate(steps):\n",
    "        print(f\"\\nStep {i+1}: Processing column {i+1}\")\n",
    "        print(f\"Original vector u{i+1}: {step['original_vector'].numpy()}\")\n",
    "        print(f\"Normalized vector q{i+1}: {step['normalized'].numpy()}\")\n",
    "        \n",
    "        for idx, proj in enumerate(step['projections']):\n",
    "            j = proj['j']\n",
    "            print(f\"  Projection onto q{i+1} from u{j+1}: coefficient = {proj['coefficient']:.4f}\")\n",
    "            print(f\"  Vector u{j+1} before: {step['intermediate_vectors'][idx]['before'].numpy()}\")\n",
    "            print(f\"  Vector u{j+1} after: {step['intermediate_vectors'][idx]['after'].numpy()}\")\n",
    "\n",
    "def demonstrate_modified_gram_schmidt():\n",
    "    \"\"\"Demonstrate modified Gram-Schmidt algorithm in detail.\"\"\"\n",
    "    # Create a small 2D matrix\n",
    "    A_2d = torch.tensor([\n",
    "        [3.0, 1.0],\n",
    "        [1.0, 2.0]\n",
    "    ])\n",
    "    \n",
    "    # Apply modified Gram-Schmidt\n",
    "    Q_2d, R_2d, steps_2d = modified_gram_schmidt_detailed(A_2d)\n",
    "    \n",
    "    # Display the matrices\n",
    "    print(\"Original Matrix A:\")\n",
    "    print(A_2d.numpy())\n",
    "    print(\"\\nOrthogonal Matrix Q:\")\n",
    "    print(Q_2d.numpy())\n",
    "    print(\"\\nUpper Triangular Matrix R:\")\n",
    "    print(R_2d.numpy())\n",
    "    \n",
    "    # Visualize the process in 2D\n",
    "    visualize_modified_gs_steps(A_2d, steps_2d, dim=2)\n",
    "    \n",
    "    return A_2d, Q_2d, R_2d, steps_2d\n",
    "\n",
    "# Demonstration of modified Gram-Schmidt\n",
    "A_2d_mgs, Q_2d_mgs, R_2d_mgs, steps_2d_mgs = demonstrate_modified_gram_schmidt()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb959f1b",
   "metadata": {},
   "source": [
    "### 2.1 Comparing Classical and Modified Gram-Schmidt\n",
    "\n",
    "Let's compare the numerical stability of the classical and modified Gram-Schmidt algorithms, especially for ill-conditioned matrices:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8ae6667",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_gs_stability():\n",
    "    \"\"\"Compare the numerical stability of classical and modified Gram-Schmidt.\"\"\"\n",
    "    # Create an ill-conditioned matrix\n",
    "    n = 5\n",
    "    \n",
    "    # Start with orthogonal columns\n",
    "    Q, _ = torch.linalg.qr(torch.randn(n, n))\n",
    "    \n",
    "    # Create a diagonal matrix with a large condition number\n",
    "    kappa = 1e10  # Condition number\n",
    "    S = torch.diag(torch.tensor([1.0] + [1.0/np.sqrt(kappa)] * (n-1)))\n",
    "    \n",
    "    # Combine to get an ill-conditioned matrix\n",
    "    A = Q @ S\n",
    "    \n",
    "    # Apply classical Gram-Schmidt\n",
    "    Q_classical, R_classical = classical_gram_schmidt(A)\n",
    "    \n",
    "    # Apply modified Gram-Schmidt\n",
    "    def modified_gram_schmidt(A):\n",
    "        if isinstance(A, np.ndarray):\n",
    "            A = torch.tensor(A, dtype=torch.float64)\n",
    "        \n",
    "        m, n = A.shape\n",
    "        Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "        R = torch.zeros((n, n), dtype=A.dtype)\n",
    "        \n",
    "        # Initialize U as a copy of A\n",
    "        U = A.clone()\n",
    "        \n",
    "        for i in range(n):\n",
    "            R[i, i] = torch.norm(U[:, i])\n",
    "            \n",
    "            if R[i, i] > 1e-10:\n",
    "                Q[:, i] = U[:, i] / R[i, i]\n",
    "            else:\n",
    "                Q[:, i] = torch.zeros(m, dtype=A.dtype)\n",
    "            \n",
    "            for j in range(i+1, n):\n",
    "                R[i, j] = torch.dot(Q[:, i], U[:, j])\n",
    "                U[:, j] = U[:, j] - R[i, j] * Q[:, i]\n",
    "        \n",
    "        return Q, R\n",
    "    \n",
    "    Q_modified, R_modified = modified_gram_schmidt(A)\n",
    "    \n",
    "    # Also compute QR with PyTorch's built-in function\n",
    "    Q_torch, R_torch = torch.linalg.qr(A)\n",
    "    \n",
    "    # Measure orthogonality\n",
    "    QTQ_classical = Q_classical.T @ Q_classical\n",
    "    QTQ_modified = Q_modified.T @ Q_modified\n",
    "    QTQ_torch = Q_torch.T @ Q_torch\n",
    "    \n",
    "    # Measure reconstruction error\n",
    "    rec_error_classical = torch.norm(A - Q_classical @ R_classical).item()\n",
    "    rec_error_modified = torch.norm(A - Q_modified @ R_modified).item()\n",
    "    rec_error_torch = torch.norm(A - Q_torch @ R_torch).item()\n",
    "    \n",
    "    # Measure orthogonality error\n",
    "    ortho_error_classical = torch.norm(QTQ_classical - torch.eye(n)).item()\n",
    "    ortho_error_modified = torch.norm(QTQ_modified - torch.eye(n)).item()\n",
    "    ortho_error_torch = torch.norm(QTQ_torch - torch.eye(n)).item()\n",
    "    \n",
    "    # Display the orthogonality matrices\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    sns.heatmap(QTQ_classical.numpy(), annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "    plt.title(f\"Q^T Q - Classical GS\\nError: {ortho_error_classical:.2e}\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    sns.heatmap(QTQ_modified.numpy(), annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "    plt.title(f\"Q^T Q - Modified GS\\nError: {ortho_error_modified:.2e}\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    sns.heatmap(QTQ_torch.numpy(), annot=True, fmt=\".2e\", cmap=blue_cmap)\n",
    "    plt.title(f\"Q^T Q - PyTorch QR\\nError: {ortho_error_torch:.2e}\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary\n",
    "    print(f\"Matrix Condition Number: {kappa:.1e}\")\n",
    "    print(\"\\nOrthogonality Error (||Q^T Q - I||):\")\n",
    "    print(f\"Classical Gram-Schmidt: {ortho_error_classical:.2e}\")\n",
    "    print(f\"Modified Gram-Schmidt: {ortho_error_modified:.2e}\")\n",
    "    print(f\"PyTorch QR: {ortho_error_torch:.2e}\")\n",
    "    \n",
    "    print(\"\\nReconstruction Error (||A - QR||):\")\n",
    "    print(f\"Classical Gram-Schmidt: {rec_error_classical:.2e}\")\n",
    "    print(f\"Modified Gram-Schmidt: {rec_error_modified:.2e}\")\n",
    "    print(f\"PyTorch QR: {rec_error_torch:.2e}\")\n",
    "    \n",
    "    # Vary condition number and observe orthogonality error\n",
    "    condition_numbers = [1e4, 1e6, 1e8, 1e10, 1e12]\n",
    "    classical_errors = []\n",
    "    modified_errors = []\n",
    "    torch_errors = []\n",
    "    \n",
    "    for kappa in condition_numbers:\n",
    "        # Create matrix with specified condition number\n",
    "        S = torch.diag(torch.tensor([1.0] + [1.0/np.sqrt(kappa)] * (n-1)))\n",
    "        A = Q @ S\n",
    "        \n",
    "        # Apply methods\n",
    "        Q_classical, R_classical = classical_gram_schmidt(A)\n",
    "        Q_modified, R_modified = modified_gram_schmidt(A)\n",
    "        Q_torch, R_torch = torch.linalg.qr(A)\n",
    "        \n",
    "        # Measure orthogonality error\n",
    "        ortho_error_classical = torch.norm(Q_classical.T @ Q_classical - torch.eye(n)).item()\n",
    "        ortho_error_modified = torch.norm(Q_modified.T @ Q_modified - torch.eye(n)).item()\n",
    "        ortho_error_torch = torch.norm(Q_torch.T @ Q_torch - torch.eye(n)).item()\n",
    "        \n",
    "        classical_errors.append(ortho_error_classical)\n",
    "        modified_errors.append(ortho_error_modified)\n",
    "        torch_errors.append(ortho_error_torch)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.loglog(condition_numbers, classical_errors, 'o-', label='Classical GS')\n",
    "    plt.loglog(condition_numbers, modified_errors, 's-', label='Modified GS')\n",
    "    plt.loglog(condition_numbers, torch_errors, '^-', label='PyTorch QR')\n",
    "    \n",
    "    plt.xlabel(\"Condition Number\")\n",
    "    plt.ylabel(\"Orthogonality Error\")\n",
    "    plt.title(\"Effect of Condition Number on Orthogonality\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    return classical_errors, modified_errors, torch_errors\n",
    "\n",
    "# Compare stability of GS methods\n",
    "classical_errors, modified_errors, torch_errors = compare_gs_stability()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb9c51f9",
   "metadata": {},
   "source": [
    "## 3. Householder Reflections\n",
    "\n",
    "Householder reflections provide another approach to QR decomposition that offers excellent numerical stability. A Householder reflection is a transformation that reflects a vector about a hyperplane.\n",
    "\n",
    "The key idea is to iteratively zero out elements below the diagonal, one column at a time:\n",
    "1. For each column, create a reflection that maps the column to a multiple of the first unit vector\n",
    "2. Apply this reflection to the entire matrix\n",
    "3. The product of these reflections gives $Q$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a724ab6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def householder_detailed(A):\n",
    "    \"\"\"\n",
    "    Implement QR decomposition using Householder reflections with detailed steps.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "        steps: Detailed steps for visualization\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    R = A.clone()\n",
    "    Q = torch.eye(m, dtype=A.dtype)\n",
    "    \n",
    "    # Store steps for visualization\n",
    "    steps = []\n",
    "    \n",
    "    for k in range(min(m-1, n)):\n",
    "        # Extract the column we want to transform\n",
    "        x = R[k:, k]\n",
    "        \n",
    "        # Store the current state before reflection\n",
    "        step_data = {\n",
    "            'k': k,\n",
    "            'column_before': x.clone(),\n",
    "            'R_before': R.clone(),\n",
    "            'Q_before': Q.clone()\n",
    "        }\n",
    "        \n",
    "        # Construct the Householder vector\n",
    "        e1 = torch.zeros_like(x)\n",
    "        e1[0] = 1.0\n",
    "        \n",
    "        # Compute norm of x\n",
    "        alpha = torch.norm(x)\n",
    "        # Ensure proper sign of alpha to avoid cancellation\n",
    "        if x[0] < 0:\n",
    "            alpha = -alpha\n",
    "        \n",
    "        # Construct the Householder vector\n",
    "        u = x - alpha * e1\n",
    "        v = u / torch.norm(u)  # Normalize\n",
    "        \n",
    "        # Store Householder vector\n",
    "        step_data['householder_vector'] = v.clone()\n",
    "        \n",
    "        # Apply the Householder reflection to R\n",
    "        R[k:, k:] = R[k:, k:] - 2.0 * torch.outer(v, torch.matmul(v, R[k:, k:]))\n",
    "        \n",
    "        # Apply the Householder reflection to Q\n",
    "        Q[:, k:] = Q[:, k:] - 2.0 * torch.matmul(Q[:, k:], torch.outer(v, v))\n",
    "        \n",
    "        # Store the state after reflection\n",
    "        step_data['column_after'] = R[k:, k].clone()\n",
    "        step_data['R_after'] = R.clone()\n",
    "        step_data['Q_after'] = Q.clone()\n",
    "        \n",
    "        steps.append(step_data)\n",
    "    \n",
    "    # Final Q and R\n",
    "    Q = Q.T\n",
    "    \n",
    "    # Ensure the diagonal of R is positive\n",
    "    for i in range(min(m, n)):\n",
    "        if R[i, i] < 0:\n",
    "            R[i, i:] = -R[i, i:]\n",
    "            Q[:, i] = -Q[:, i]\n",
    "    \n",
    "    return Q, R, steps\n",
    "\n",
    "def visualize_householder_steps(A, steps):\n",
    "    \"\"\"Visualize the Householder QR decomposition process.\"\"\"\n",
    "    if isinstance(A, torch.Tensor):\n",
    "        A = A.numpy()\n",
    "    \n",
    "    m, n = A.shape\n",
    "    \n",
    "    # For each step\n",
    "    for k, step in enumerate(steps):\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        \n",
    "        # Plot the R matrix before reflection\n",
    "        plt.subplot(2, 3, 1)\n",
    "        sns.heatmap(step['R_before'].numpy(), annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "        plt.title(f\"Step {k+1}: R Before Reflection\")\n",
    "        \n",
    "        # Plot the column before reflection\n",
    "        plt.subplot(2, 3, 2)\n",
    "        column_before = np.zeros(m)\n",
    "        column_before[step['k']:] = step['column_before'].numpy()\n",
    "        sns.heatmap(column_before.reshape(-1, 1), annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "        plt.title(f\"Column {step['k']+1} Before\")\n",
    "        \n",
    "        # Plot the Householder vector\n",
    "        plt.subplot(2, 3, 3)\n",
    "        v_full = np.zeros(m - step['k'])\n",
    "        v_full = step['householder_vector'].numpy()\n",
    "        v_viz = np.zeros(m)\n",
    "        v_viz[step['k']:] = v_full\n",
    "        sns.heatmap(v_viz.reshape(-1, 1), annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "        plt.title(\"Householder Vector v\")\n",
    "        \n",
    "        # Plot the R matrix after reflection\n",
    "        plt.subplot(2, 3, 4)\n",
    "        sns.heatmap(step['R_after'].numpy(), annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "        plt.title(f\"Step {k+1}: R After Reflection\")\n",
    "        \n",
    "        # Plot the column after reflection\n",
    "        plt.subplot(2, 3, 5)\n",
    "        column_after = np.zeros(m)\n",
    "        column_after[step['k']:] = step['column_after'].numpy()\n",
    "        sns.heatmap(column_after.reshape(-1, 1), annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "        plt.title(f\"Column {step['k']+1} After\")\n",
    "        \n",
    "        # Plot the current Q matrix \n",
    "        plt.subplot(2, 3, 6)\n",
    "        sns.heatmap(step['Q_after'].T.numpy(), annot=True, fmt=\".2f\", cmap=blue_cmap)\n",
    "        plt.title(f\"Current Q Matrix\")\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # For 2D or 3D matrices, add a geometric visualization\n",
    "        if m == 2 or m == 3:\n",
    "            # Geometric visualization in 2D\n",
    "            if m == 2:\n",
    "                plt.figure(figsize=(12, 6))\n",
    "                \n",
    "                # Get the original column and Householder vector\n",
    "                col = step['column_before'].numpy()\n",
    "                v = step['householder_vector'].numpy()\n",
    "                \n",
    "                # Compute the reflection plane (line in 2D)\n",
    "                # The reflection plane is perpendicular to v\n",
    "                # In 2D, we can represent it with a line through the origin\n",
    "                if v[1] != 0:\n",
    "                    slope = -v[0] / v[1]\n",
    "                    x = np.linspace(-3, 3, 100)\n",
    "                    y = slope * x\n",
    "                else:\n",
    "                    x = np.zeros(100)\n",
    "                    y = np.linspace(-3, 3, 100)\n",
    "                \n",
    "                plt.subplot(1, 2, 1)\n",
    "                plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "                plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "                \n",
    "                # Draw the original column vector\n",
    "                plt.arrow(0, 0, col[0], col[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='blue', ec='blue', label='Original Column')\n",
    "                \n",
    "                # Draw the Householder vector\n",
    "                plt.arrow(0, 0, v[0], v[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='red', ec='red', label='Householder Vector')\n",
    "                \n",
    "                # Draw the reflection plane\n",
    "                plt.plot(x, y, 'g--', label='Reflection Plane')\n",
    "                \n",
    "                plt.axis('equal')\n",
    "                plt.xlim(-3, 3)\n",
    "                plt.ylim(-3, 3)\n",
    "                plt.grid(True)\n",
    "                plt.title(\"Householder Vector and Reflection Plane\")\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.subplot(1, 2, 2)\n",
    "                plt.axhline(y=0, color='k', linestyle='-', alpha=0.3)\n",
    "                plt.axvline(x=0, color='k', linestyle='-', alpha=0.3)\n",
    "                \n",
    "                # Draw the original column vector\n",
    "                plt.arrow(0, 0, col[0], col[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='blue', ec='blue', alpha=0.5, label='Original Column')\n",
    "                \n",
    "                # Draw the reflected column\n",
    "                col_after = step['column_after'].numpy()\n",
    "                plt.arrow(0, 0, col_after[0], col_after[1], head_width=0.1, head_length=0.1, \n",
    "                         fc='green', ec='green', label='Reflected Column')\n",
    "                \n",
    "                # Draw the reflection plane\n",
    "                plt.plot(x, y, 'g--', label='Reflection Plane')\n",
    "                \n",
    "                plt.axis('equal')\n",
    "                plt.xlim(-3, 3)\n",
    "                plt.ylim(-3, 3)\n",
    "                plt.grid(True)\n",
    "                plt.title(\"Effect of Householder Reflection\")\n",
    "                plt.legend()\n",
    "                \n",
    "                plt.tight_layout()\n",
    "                plt.show()\n",
    "            \n",
    "            # For 3D visualization (not implemented for brevity)\n",
    "            elif m == 3:\n",
    "                pass\n",
    "    \n",
    "    # Print numerical details of the process\n",
    "    for k, step in enumerate(steps):\n",
    "        print(f\"\\nStep {k+1}: Zeroing below diagonal in column {step['k']+1}\")\n",
    "        print(f\"Column before reflection: {step['column_before'].numpy()}\")\n",
    "        print(f\"Householder vector: {step['householder_vector'].numpy()}\")\n",
    "        print(f\"Column after reflection: {step['column_after'].numpy()}\")\n",
    "\n",
    "def demonstrate_householder():\n",
    "    \"\"\"Demonstrate Householder reflections for QR decomposition.\"\"\"\n",
    "    # Create a small 2D matrix for geometric visualization\n",
    "    A_2d = torch.tensor([\n",
    "        [3.0, 1.0],\n",
    "        [1.0, 2.0]\n",
    "    ])\n",
    "    \n",
    "    # Apply Householder QR\n",
    "    Q_2d, R_2d, steps_2d = householder_detailed(A_2d)\n",
    "    \n",
    "    # Display the matrices\n",
    "    print(\"Original Matrix A:\")\n",
    "    print(A_2d.numpy())\n",
    "    print(\"\\nOrthogonal Matrix Q:\")\n",
    "    print(Q_2d.numpy())\n",
    "    print(\"\\nUpper Triangular Matrix R:\")\n",
    "    print(R_2d.numpy())\n",
    "    \n",
    "    # Visualize the process\n",
    "    visualize_householder_steps(A_2d, steps_2d)\n",
    "    \n",
    "    # Create a larger matrix\n",
    "    A_larger = torch.tensor([\n",
    "        [4.0, 1.0, 3.0],\n",
    "        [2.0, 5.0, 2.0],\n",
    "        [1.0, 0.0, 6.0],\n",
    "        [3.0, 2.0, 1.0]\n",
    "    ])\n",
    "    \n",
    "    # Apply Householder QR\n",
    "    Q_larger, R_larger, steps_larger = householder_detailed(A_larger)\n",
    "    \n",
    "    # Display the matrices\n",
    "    print(\"\\nOriginal Matrix A (4×3):\")\n",
    "    print(A_larger.numpy())\n",
    "    print(\"\\nOrthogonal Matrix Q:\")\n",
    "    print(Q_larger.numpy())\n",
    "    print(\"\\nUpper Triangular Matrix R:\")\n",
    "    print(R_larger.numpy())\n",
    "    \n",
    "    # Visualize the process\n",
    "    visualize_householder_steps(A_larger, steps_larger)\n",
    "    \n",
    "    return A_2d, Q_2d, R_2d, A_larger, Q_larger, R_larger\n",
    "\n",
    "# Demonstrate Householder reflections\n",
    "A_2d_hh, Q_2d_hh, R_2d_hh, A_larger_hh, Q_larger_hh, R_larger_hh = demonstrate_householder()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d004f2d2",
   "metadata": {},
   "source": [
    "## 4. Givens Rotations\n",
    "\n",
    "Givens rotations provide an alternative approach to QR decomposition. Instead of using reflections, they use rotations to zero out elements one at a time. This approach is especially useful for sparse matrices, as it can preserve sparsity better than Householder reflections.\n",
    "\n",
    "A Givens rotation rotates a plane by a specific angle to zero out one element:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92abdedc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def givens_rotation(A):\n",
    "    \"\"\"\n",
    "    Implement QR decomposition using Givens rotations.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    R = A.clone()\n",
    "    Q = torch.eye(m, dtype=A.dtype)\n",
    "    \n",
    "    # For each column\n",
    "    for j in range(n):\n",
    "        # For each element below the diagonal in this column\n",
    "        for i in range(m-1, j, -1):\n",
    "            # Skip if the element is already zero\n",
    "            if abs(R[i, j]) < 1e-10:\n",
    "                continue\n",
    "            \n",
    "            # Compute the Givens rotation parameters\n",
    "            a = R[i-1, j]\n",
    "            b = R[i, j]\n",
    "            r = torch.sqrt(a*a + b*b)\n",
    "            c = a / r  # cosine\n",
    "            s = -b / r  # sine\n",
    "            \n",
    "            # Create the Givens rotation matrix (identity with a 2x2 rotation)\n",
    "            G = torch.eye(m, dtype=A.dtype)\n",
    "            G[i-1, i-1] = c\n",
    "            G[i-1, i] = s\n",
    "            G[i, i-1] = -s\n",
    "            G[i, i] = c\n",
    "            \n",
    "            # Apply the rotation to R\n",
    "            R = G @ R\n",
    "            \n",
    "            # Update Q (we use G^T since we're building Q from the left)\n",
    "            Q = Q @ G.T\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "def demonstrate_givens():\n",
    "    \"\"\"Demonstrate Givens rotations for QR decomposition.\"\"\"\n",
    "    # Create a small test matrix\n",
    "    A = torch.tensor([\n",
    "        [3.0, 1.0, 2.0],\n",
    "        [1.0, 2.0, 0.0],\n",
    "        [2.0, 0.0, 3.0]\n",
    "    ])\n",
    "    \n",
    "    # Apply Givens rotations\n",
    "    Q_givens, R_givens = givens_rotation(A)\n",
    "    \n",
    "    # For comparison, use Householder reflections\n",
    "    Q_house, R_house = householder_detailed(A)[0:2]\n",
    "    \n",
    "    # Display the matrices\n",
    "    print(\"Original Matrix A:\")\n",
    "    print(A.numpy())\n",
    "    print(\"\\nOrthogonal Matrix Q (Givens):\")\n",
    "    print(Q_givens.numpy())\n",
    "    print(\"\\nUpper Triangular Matrix R (Givens):\")\n",
    "    print(R_givens.numpy())\n",
    "    \n",
    "    # Verify the decomposition\n",
    "    rec_error_givens = torch.norm(A - Q_givens @ R_givens).item()\n",
    "    ortho_error_givens = torch.norm(Q_givens.T @ Q_givens - torch.eye(3)).item()\n",
    "    \n",
    "    rec_error_house = torch.norm(A - Q_house @ R_house).item()\n",
    "    ortho_error_house = torch.norm(Q_house.T @ Q_house - torch.eye(3)).item()\n",
    "    \n",
    "    print(\"\\nGivens Rotations:\")\n",
    "    print(f\"Reconstruction error: {rec_error_givens:.2e}\")\n",
    "    print(f\"Orthogonality error: {ortho_error_givens:.2e}\")\n",
    "    \n",
    "    print(\"\\nHouseholder Reflections:\")\n",
    "    print(f\"Reconstruction error: {rec_error_house:.2e}\")\n",
    "    print(f\"Orthogonality error: {ortho_error_house:.2e}\")\n",
    "    \n",
    "    # Create a larger sparse matrix\n",
    "    n = 10\n",
    "    sparse_A = torch.zeros((n, n), dtype=torch.float64)\n",
    "    \n",
    "    # Add some non-zero elements (sparse pattern)\n",
    "    for i in range(n):\n",
    "        sparse_A[i, i] = i + 1  # Diagonal\n",
    "        if i < n-1:\n",
    "            sparse_A[i, i+1] = 0.5  # Superdiagonal\n",
    "        if i > 0:\n",
    "            sparse_A[i, i-1] = 0.3  # Subdiagonal\n",
    "    \n",
    "    # Apply both methods\n",
    "    Q_givens_sparse, R_givens_sparse = givens_rotation(sparse_A)\n",
    "    Q_house_sparse, R_house_sparse, _ = householder_detailed(sparse_A)\n",
    "    \n",
    "    # Measure sparsity\n",
    "    def count_zeros(matrix, threshold=1e-10):\n",
    "        return torch.sum(torch.abs(matrix) < threshold).item()\n",
    "    \n",
    "    zeros_givens = count_zeros(R_givens_sparse)\n",
    "    zeros_house = count_zeros(R_house_sparse)\n",
    "    \n",
    "    print(\"\\nSparsity Comparison (number of zero elements in R):\")\n",
    "    print(f\"Givens Rotations: {zeros_givens} zeros out of {n*n} elements\")\n",
    "    print(f\"Householder Reflections: {zeros_house} zeros out of {n*n} elements\")\n",
    "    \n",
    "    # Visualize the sparsity patterns\n",
    "    plt.figure(figsize=(12, 5))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.spy(sparse_A.numpy(), markersize=5)\n",
    "    plt.title(\"Original Sparse Matrix\")\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.spy(R_givens_sparse.numpy(), markersize=5)\n",
    "    plt.title(\"R from Givens Rotations\")\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.spy(R_house_sparse.numpy(), markersize=5)\n",
    "    plt.title(\"R from Householder Reflections\")\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return A, Q_givens, R_givens, sparse_A, R_givens_sparse, R_house_sparse\n",
    "\n",
    "# Demonstrate Givens rotations\n",
    "A_givens, Q_givens, R_givens, sparse_A, R_givens_sparse, R_house_sparse = demonstrate_givens()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8702450",
   "metadata": {},
   "source": [
    "## 5. Performance Comparison of QR Algorithms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d99e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_qr_performance():\n",
    "    \"\"\"Compare the performance of different QR decomposition algorithms.\"\"\"\n",
    "    # Define matrix sizes to test\n",
    "    sizes = range(20, 201, 20)\n",
    "    \n",
    "    # Store timing results\n",
    "    times_classical = []\n",
    "    times_modified = []\n",
    "    times_householder = []\n",
    "    times_givens = []\n",
    "    times_builtin = []\n",
    "    \n",
    "    # Store accuracy results\n",
    "    ortho_classical = []\n",
    "    ortho_modified = []\n",
    "    ortho_householder = []\n",
    "    ortho_givens = []\n",
    "    ortho_builtin = []\n",
    "    \n",
    "    for n in sizes:\n",
    "        # Create a random matrix\n",
    "        A = torch.rand(n, n, dtype=torch.float64)\n",
    "        \n",
    "        # Classical Gram-Schmidt\n",
    "        start_time = time.time()\n",
    "        Q, R = classical_gram_schmidt(A)\n",
    "        times_classical.append(time.time() - start_time)\n",
    "        ortho_classical.append(torch.norm(Q.T @ Q - torch.eye(n)).item())\n",
    "        \n",
    "        # Modified Gram-Schmidt\n",
    "        def modified_gram_schmidt(A):\n",
    "            \"\"\"Modified Gram-Schmidt algorithm.\"\"\"\n",
    "            if isinstance(A, np.ndarray):\n",
    "                A = torch.tensor(A, dtype=torch.float64)\n",
    "            \n",
    "            m, n = A.shape\n",
    "            Q = torch.zeros((m, n), dtype=A.dtype)\n",
    "            R = torch.zeros((n, n), dtype=A.dtype)\n",
    "            \n",
    "            # Initialize U as a copy of A\n",
    "            U = A.clone()\n",
    "            \n",
    "            for i in range(n):\n",
    "                R[i, i] = torch.norm(U[:, i])\n",
    "                \n",
    "                if R[i, i] > 1e-10:\n",
    "                    Q[:, i] = U[:, i] / R[i, i]\n",
    "                else:\n",
    "                    Q[:, i] = torch.zeros(m, dtype=A.dtype)\n",
    "                \n",
    "                for j in range(i+1, n):\n",
    "                    R[i, j] = torch.dot(Q[:, i], U[:, j])\n",
    "                    U[:, j] = U[:, j] - R[i, j] * Q[:, i]\n",
    "            \n",
    "            return Q, R\n",
    "        \n",
    "        start_time = time.time()\n",
    "        Q, R = modified_gram_schmidt(A)\n",
    "        times_modified.append(time.time() - start_time)\n",
    "        ortho_modified.append(torch.norm(Q.T @ Q - torch.eye(n)).item())\n",
    "        \n",
    "        # Householder reflections\n",
    "        def householder(A):\n",
    "            \"\"\"Householder reflections for QR decomposition.\"\"\"\n",
    "            if isinstance(A, np.ndarray):\n",
    "                A = torch.tensor(A, dtype=torch.float64)\n",
    "            \n",
    "            m, n = A.shape\n",
    "            R = A.clone()\n",
    "            Q = torch.eye(m, dtype=A.dtype)\n",
    "            \n",
    "            for k in range(min(m-1, n)):\n",
    "                x = R[k:, k]\n",
    "                \n",
    "                e1 = torch.zeros_like(x)\n",
    "                e1[0] = 1.0\n",
    "                \n",
    "                alpha = torch.norm(x)\n",
    "                if x[0] < 0:\n",
    "                    alpha = -alpha\n",
    "                \n",
    "                u = x - alpha * e1\n",
    "                v = u / torch.norm(u)\n",
    "                \n",
    "                R[k:, k:] = R[k:, k:] - 2.0 * torch.outer(v, torch.matmul(v, R[k:, k:]))\n",
    "                Q[:, k:] = Q[:, k:] - 2.0 * torch.matmul(Q[:, k:], torch.outer(v, v))\n",
    "            \n",
    "            Q = Q.T\n",
    "            \n",
    "            return Q, R\n",
    "        \n",
    "        start_time = time.time()\n",
    "        Q, R = householder(A)\n",
    "        times_householder.append(time.time() - start_time)\n",
    "        ortho_householder.append(torch.norm(Q.T @ Q - torch.eye(n)).item())\n",
    "        \n",
    "        # Givens rotations\n",
    "        start_time = time.time()\n",
    "        Q, R = givens_rotation(A)\n",
    "        times_givens.append(time.time() - start_time)\n",
    "        ortho_givens.append(torch.norm(Q.T @ Q - torch.eye(n)).item())\n",
    "        \n",
    "        # Built-in QR\n",
    "        start_time = time.time()\n",
    "        Q, R = torch.linalg.qr(A)\n",
    "        times_builtin.append(time.time() - start_time)\n",
    "        ortho_builtin.append(torch.norm(Q.T @ Q - torch.eye(n)).item())\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    \n",
    "    # Timing comparison\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(sizes, times_classical, 'o-', label='Classical Gram-Schmidt')\n",
    "    plt.plot(sizes, times_modified, 's-', label='Modified Gram-Schmidt')\n",
    "    plt.plot(sizes, times_householder, '^-', label='Householder Reflections')\n",
    "    plt.plot(sizes, times_givens, 'x-', label='Givens Rotations')\n",
    "    plt.plot(sizes, times_builtin, 'd-', label='PyTorch QR')\n",
    "    \n",
    "    plt.xlabel(\"Matrix Size (n×n)\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(\"QR Decomposition Time vs. Matrix Size\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Orthogonality comparison\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.semilogy(sizes, ortho_classical, 'o-', label='Classical Gram-Schmidt')\n",
    "    plt.semilogy(sizes, ortho_modified, 's-', label='Modified Gram-Schmidt')\n",
    "    plt.semilogy(sizes, ortho_householder, '^-', label='Householder Reflections')\n",
    "    plt.semilogy(sizes, ortho_givens, 'x-', label='Givens Rotations')\n",
    "    plt.semilogy(sizes, ortho_builtin, 'd-', label='PyTorch QR')\n",
    "    \n",
    "    plt.xlabel(\"Matrix Size (n×n)\")\n",
    "    plt.ylabel(\"Orthogonality Error\")\n",
    "    plt.title(\"QR Decomposition Accuracy vs. Matrix Size\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print summary for largest size\n",
    "    largest_size = sizes[-1]\n",
    "    print(f\"Performance Comparison for {largest_size}×{largest_size} Matrix:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(f\"{'Method':<25} {'Time (s)':<15} {'Orthogonality Error':<20}\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    methods = [\"Classical Gram-Schmidt\", \"Modified Gram-Schmidt\", \n",
    "               \"Householder Reflections\", \"Givens Rotations\", \"PyTorch QR\"]\n",
    "    times = [times_classical[-1], times_modified[-1], \n",
    "             times_householder[-1], times_givens[-1], times_builtin[-1]]\n",
    "    errors = [ortho_classical[-1], ortho_modified[-1], \n",
    "              ortho_householder[-1], ortho_givens[-1], ortho_builtin[-1]]\n",
    "    \n",
    "    for method, t, error in zip(methods, times, errors):\n",
    "        print(f\"{method:<25} {t:<15.6f} {error:<20.2e}\")\n",
    "    \n",
    "    # Return the data for further analysis\n",
    "    return {\n",
    "        'sizes': sizes,\n",
    "        'times': {\n",
    "            'classical': times_classical,\n",
    "            'modified': times_modified,\n",
    "            'householder': times_householder,\n",
    "            'givens': times_givens,\n",
    "            'builtin': times_builtin\n",
    "        },\n",
    "        'orthogonality': {\n",
    "            'classical': ortho_classical,\n",
    "            'modified': ortho_modified,\n",
    "            'householder': ortho_householder,\n",
    "            'givens': ortho_givens,\n",
    "            'builtin': ortho_builtin\n",
    "        }\n",
    "    }\n",
    "\n",
    "# Skip the full performance comparison for brevity\n",
    "# Instead, provide a summary\n",
    "print(\"Performance Summary of QR Algorithms:\")\n",
    "print(\"1. Classical Gram-Schmidt: Simple to implement but numerically unstable\")\n",
    "print(\"2. Modified Gram-Schmidt: Improved stability with similar computation cost\")\n",
    "print(\"3. Householder Reflections: Excellent stability, efficient for dense matrices\")\n",
    "print(\"4. Givens Rotations: Good stability, efficient for sparse matrices\")\n",
    "print(\"5. Built-in QR: Optimized implementation, usually fastest and most stable\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191110e",
   "metadata": {},
   "source": [
    "## 6. Block QR Algorithms\n",
    "\n",
    "For large matrices, block algorithms can improve performance by leveraging cache locality and optimized matrix-matrix operations. Let's implement a simple block QR algorithm:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de5dec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def block_qr(A, block_size=32):\n",
    "    \"\"\"\n",
    "    Implement block QR decomposition using Householder reflections.\n",
    "    \n",
    "    Args:\n",
    "        A: Input matrix as a PyTorch tensor\n",
    "        block_size: Size of the blocks\n",
    "        \n",
    "    Returns:\n",
    "        Q: Orthogonal matrix\n",
    "        R: Upper triangular matrix\n",
    "    \"\"\"\n",
    "    if isinstance(A, np.ndarray):\n",
    "        A = torch.tensor(A, dtype=torch.float64)\n",
    "    \n",
    "    m, n = A.shape\n",
    "    R = A.clone()\n",
    "    Q = torch.eye(m, dtype=A.dtype)\n",
    "    \n",
    "    # Process the matrix in block columns\n",
    "    for j in range(0, n, block_size):\n",
    "        # Adjust block size for the last block\n",
    "        current_block_size = min(block_size, n - j)\n",
    "        \n",
    "        # Work on the current block column\n",
    "        for k in range(j, min(j + current_block_size, m-1)):\n",
    "            # Extract the column we want to transform\n",
    "            x = R[k:, k]\n",
    "            \n",
    "            # Skip if the column is already zeroed below the diagonal\n",
    "            if torch.norm(x[1:]) < 1e-10:\n",
    "                continue\n",
    "            \n",
    "            # Construct the Householder vector\n",
    "            e1 = torch.zeros_like(x)\n",
    "            e1[0] = 1.0\n",
    "            \n",
    "            alpha = torch.norm(x)\n",
    "            if x[0] < 0:\n",
    "                alpha = -alpha\n",
    "                \n",
    "            u = x - alpha * e1\n",
    "            v = u / torch.norm(u)\n",
    "            \n",
    "            # Apply the Householder reflection to the remaining part of R\n",
    "            R[k:, k:] = R[k:, k:] - 2.0 * torch.outer(v, torch.matmul(v, R[k:, k:]))\n",
    "            \n",
    "            # Update Q\n",
    "            Q[:, k:] = Q[:, k:] - 2.0 * torch.matmul(Q[:, k:], torch.outer(v, v))\n",
    "    \n",
    "    # Transpose Q to get the final orthogonal matrix\n",
    "    Q = Q.T\n",
    "    \n",
    "    return Q, R\n",
    "\n",
    "def demonstrate_block_qr():\n",
    "    \"\"\"Demonstrate block QR decomposition.\"\"\"\n",
    "    # Create a larger matrix for testing\n",
    "    n = 100\n",
    "    A = torch.rand(n, n, dtype=torch.float64)\n",
    "    \n",
    "    # Compare block QR with standard QR\n",
    "    block_sizes = [5, 10, 20, 50]\n",
    "    times_block = []\n",
    "    times_standard = []\n",
    "    \n",
    "    for bs in block_sizes:\n",
    "        # Block QR\n",
    "        start_time = time.time()\n",
    "        Q_block, R_block = block_qr(A, block_size=bs)\n",
    "        block_time = time.time() - start_time\n",
    "        times_block.append(block_time)\n",
    "        \n",
    "        # Standard QR (just once)\n",
    "        if len(times_standard) == 0:\n",
    "            start_time = time.time()\n",
    "            Q_standard, R_standard = torch.linalg.qr(A)\n",
    "            standard_time = time.time() - start_time\n",
    "            times_standard = [standard_time] * len(block_sizes)\n",
    "    \n",
    "    # Plot the results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(block_sizes, times_block, 'o-', label='Block QR')\n",
    "    plt.axhline(y=times_standard[0], color='r', linestyle='--', label='Standard QR')\n",
    "    \n",
    "    plt.xlabel(\"Block Size\")\n",
    "    plt.ylabel(\"Time (seconds)\")\n",
    "    plt.title(f\"Block QR Performance vs. Block Size (n={n})\")\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Add annotations\n",
    "    for i, bs in enumerate(block_sizes):\n",
    "        plt.annotate(f\"{times_block[i]:.4f}s\", \n",
    "                    (bs, times_block[i]), \n",
    "                    textcoords=\"offset points\",\n",
    "                    xytext=(0, 10), \n",
    "                    ha='center')\n",
    "    \n",
    "    plt.annotate(f\"{times_standard[0]:.4f}s\", \n",
    "                (block_sizes[-1], times_standard[0]), \n",
    "                textcoords=\"offset points\",\n",
    "                xytext=(30, 0), \n",
    "                ha='center')\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Verify the decomposition\n",
    "    error_block = torch.norm(A - Q_block @ R_block).item()\n",
    "    ortho_error_block = torch.norm(Q_block.T @ Q_block - torch.eye(n)).item()\n",
    "    \n",
    "    error_standard = torch.norm(A - Q_standard @ R_standard).item()\n",
    "    ortho_error_standard = torch.norm(Q_standard.T @ Q_standard - torch.eye(n)).item()\n",
    "    \n",
    "    print(f\"Block QR (block size {block_sizes[-1]}):\")\n",
    "    print(f\"Reconstruction error: {error_block:.2e}\")\n",
    "    print(f\"Orthogonality error: {ortho_error_block:.2e}\")\n",
    "    \n",
    "    print(f\"\\nStandard QR:\")\n",
    "    print(f\"Reconstruction error: {error_standard:.2e}\")\n",
    "    print(f\"Orthogonality error: {ortho_error_standard:.2e}\")\n",
    "    \n",
    "    return A, Q_block, R_block, Q_standard, R_standard\n",
    "\n",
    "# Skip the block QR demonstration for brevity\n",
    "print(\"Block QR Algorithms:\")\n",
    "print(\"- Process the matrix in blocks to improve cache locality\")\n",
    "print(\"- Can leverage optimized BLAS routines for matrix-matrix operations\")\n",
    "print(\"- Performance depends on the block size and hardware architecture\")\n",
    "print(\"- Optimal block size typically matches the cache size of the processor\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bd5371d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook, we've explored different algorithms for computing QR decomposition:\n",
    "\n",
    "1. **Classical Gram-Schmidt**: Simple to implement but suffers from numerical instability, especially for ill-conditioned matrices.\n",
    "\n",
    "2. **Modified Gram-Schmidt**: Improves numerical stability by updating the vectors after each projection.\n",
    "\n",
    "3. **Householder Reflections**: Offers excellent numerical stability and efficiency for dense matrices.\n",
    "\n",
    "4. **Givens Rotations**: Particularly useful for sparse matrices as they can preserve sparsity patterns.\n",
    "\n",
    "5. **Block Algorithms**: Improve performance for large matrices by leveraging cache locality.\n",
    "\n",
    "Key takeaways:\n",
    "\n",
    "- For general-purpose use, Householder reflections provide the best balance of stability and efficiency.\n",
    "- For sparse matrices, Givens rotations can be more efficient.\n",
    "- For very large matrices, block algorithms can provide performance improvements.\n",
    "- In practice, optimized libraries typically use a combination of these techniques.\n",
    "\n",
    "The QR decomposition is a fundamental tool in numerical linear algebra with numerous applications, which we'll explore in the next notebook.\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
